{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Containerized development, admin and professional environments. What is workspace Workspace is a docker image with toolset selected for specific tasks, such as software development, database administration or data exploration browser based user interfaces that enable cloud development, such as IDE, file manager, job scheduler Use-cases You are building cloud development platform You need to get started fast without building your own toolset Demo: Ansible-Terraform workspace If you like this project, please support it by simply putting a Github star and sharing with friends on Twitter. Contribution Pull requests welcome. If you feel you need to discuss first, create an issue.","title":"About"},{"location":"#what-is-workspace","text":"Workspace is a docker image with toolset selected for specific tasks, such as software development, database administration or data exploration browser based user interfaces that enable cloud development, such as IDE, file manager, job scheduler","title":"What is workspace"},{"location":"#use-cases","text":"You are building cloud development platform You need to get started fast without building your own toolset Demo: Ansible-Terraform workspace If you like this project, please support it by simply putting a Github star and sharing with friends on Twitter.","title":"Use-cases"},{"location":"#contribution","text":"Pull requests welcome. If you feel you need to discuss first, create an issue.","title":"Contribution"},{"location":"ansible-terraform-workspace/","text":"Ansible-Terraform Workspace Dockerized development environment for Ansible and Terraform. Workspace will help to create and manage infrastructures; visualize planned terraform changes; display ansible hosts plays; schedule and observe executions. Why this images If you don't want to install all of those tools separately. If you need self-hosted environment to create infrastructures and schedule maintenance ansible playbooks. Start docker run --name space-1 -d -p 8020-8040:8020-8040 -p 9000:9000 alnoda/ansible-terraform-workspace open localhost:8020 in browser. Features Ansible tools: Ansible Ansible Ara Ansible-cmdb Ansible inventory grapher Ansible Playbook Grapher Ansible Lint Ansible Doctor Terraform tools: Terrraform Pre-commit-terraform Rover Blast-Radius Terraform Visual Terraform Graph Inframap Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo: Ansible-Terraform workspace","title":"About"},{"location":"ansible-terraform-workspace/#ansible-terraform-workspace","text":"Dockerized development environment for Ansible and Terraform. Workspace will help to create and manage infrastructures; visualize planned terraform changes; display ansible hosts plays; schedule and observe executions.","title":"Ansible-Terraform Workspace"},{"location":"ansible-terraform-workspace/#why-this-images","text":"If you don't want to install all of those tools separately. If you need self-hosted environment to create infrastructures and schedule maintenance ansible playbooks.","title":"Why this images"},{"location":"ansible-terraform-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 -p 9000:9000 alnoda/ansible-terraform-workspace open localhost:8020 in browser.","title":"Start"},{"location":"ansible-terraform-workspace/#features","text":"Ansible tools: Ansible Ansible Ara Ansible-cmdb Ansible inventory grapher Ansible Playbook Grapher Ansible Lint Ansible Doctor Terraform tools: Terrraform Pre-commit-terraform Rover Blast-Radius Terraform Visual Terraform Graph Inframap Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo: Ansible-Terraform workspace","title":"Features"},{"location":"ansible-terraform-workspace/ansible-tools/","text":"Ansible This tutorial is a little demonstration of the included ansible toolset. Workspace has an example ansible play which installs several installs packages. We will use this playbook for some of the examples. For other examples, create your own ansible project. To run an example ansible play, open terminal and execute cd /home/examples/ansible-local ansible-playbook install-packages.yml Ansible Ara Ansible ARA Records Ansible and makes it easier to understand and troubleshoot. Ansible Ara is fully configured in the Ansible-Terraform workspace - it captures any execution (manual or scheduled) of any ansible playbook. Ara server is up and running. Open quickstart page and navigate to Ara WEB UI Ansible Lint Ansible Lint is a command-line tool for linting playbooks, roles and collections aimed towards any Ansible users. Cd to the folder with your Ansible project and execute cd /home/examples/ansible-local ansible-lint --nocolor > /home/static-server/ansible-lint.txt You can view the resulted txt file with static file server, filebrowser. Ansible report Ansible-report is a small utility that generates several reports from your ansible project - visualizes inventory, represents all plays in a format of graphs, generates interactive static website with information about hosts etc. Create your own ansible project, cd and execute ansible-report Ansible-report is a shell script that simply executes several ansible tools in one shot: ansible-lint ansible-cmdb ansible-inventory-grapher ansible-playbook-grapher (for all ansible plays in the folder) You can also use any of the ansible tools separately. Ansible-cmdb Ansible-cmdb takes the output of Ansible's fact gathering and converts it into a static HTML overview page (and other things) containing system configuration information. Cd to the folder with your Ansible project and execute ansible -m setup --tree out/ all ansible-cmdb out/ > overview.html Ansible inventory grapher Ansible inventory grapher creates a dot file suitable for use by graphviz. Cd to the folder with your Ansible project and execute ansible-inventory-grapher all | dot -Tpng > /home/static-server/my.png Ansible Playbook Grapher Ansible Playbook Grapher - is a command line tool that creates a graph representing your Ansible playbook plays, tasks and roles. The aim of this project is to have an overview of your playbook. Cd to the folder with your Ansible project and execute ansible-playbook-grapher --include-role-tasks example.yml -o /home/static-server/example Ansible Doctor Ansible Doctor is a simple annotation like documentation generator based on Jinja2 templates. while ansible-doctor comes with a default template called readme, it is also possible to write your own templates. The first step is to identify if the given folder is an Ansible role. This check is very simple, if the folder contains a sub-directory called tasks is MUST be an Ansible role! :) After the successful check, ansible-doctor will try to read some static files into a dictionary: - defaults/main.yml - meta/main.yml Cd to the folder with your Ansible project and execute ansible-doctor -o /home/static-server/ roles/example_production Schedule Ansible playbook executions Ansible-Terraform workspace has 2 tools (Cronicle and ARA) that make it simple and convenient to use Ansible for periodic tasks and jobs. For example, maintenance jobs for your cloud infrastructure. This is especially handy if you run this workspace on a remote server. Cronicle - allows to schedule tasks and jobs, and lets you observe executions using a nice UI Ansible Ara - tracks all executions of ansible playbooks (manual or scheduled), and has nice UI that provides informationn about every step of every playbook execution You can try scheduling an example ansible playbook with Cronicle NOTE: Scheduling Ansible playbooks is especially useful if you launch Workspace on a remote server rather than on your local laptop.","title":"Ansible tools"},{"location":"ansible-terraform-workspace/ansible-tools/#ansible","text":"This tutorial is a little demonstration of the included ansible toolset. Workspace has an example ansible play which installs several installs packages. We will use this playbook for some of the examples. For other examples, create your own ansible project. To run an example ansible play, open terminal and execute cd /home/examples/ansible-local ansible-playbook install-packages.yml","title":"Ansible"},{"location":"ansible-terraform-workspace/ansible-tools/#ansible-ara","text":"Ansible ARA Records Ansible and makes it easier to understand and troubleshoot. Ansible Ara is fully configured in the Ansible-Terraform workspace - it captures any execution (manual or scheduled) of any ansible playbook. Ara server is up and running. Open quickstart page and navigate to Ara WEB UI","title":"Ansible Ara"},{"location":"ansible-terraform-workspace/ansible-tools/#ansible-lint","text":"Ansible Lint is a command-line tool for linting playbooks, roles and collections aimed towards any Ansible users. Cd to the folder with your Ansible project and execute cd /home/examples/ansible-local ansible-lint --nocolor > /home/static-server/ansible-lint.txt You can view the resulted txt file with static file server, filebrowser.","title":"Ansible Lint"},{"location":"ansible-terraform-workspace/ansible-tools/#ansible-report","text":"Ansible-report is a small utility that generates several reports from your ansible project - visualizes inventory, represents all plays in a format of graphs, generates interactive static website with information about hosts etc. Create your own ansible project, cd and execute ansible-report Ansible-report is a shell script that simply executes several ansible tools in one shot: ansible-lint ansible-cmdb ansible-inventory-grapher ansible-playbook-grapher (for all ansible plays in the folder) You can also use any of the ansible tools separately.","title":"Ansible report"},{"location":"ansible-terraform-workspace/ansible-tools/#ansible-cmdb","text":"Ansible-cmdb takes the output of Ansible's fact gathering and converts it into a static HTML overview page (and other things) containing system configuration information. Cd to the folder with your Ansible project and execute ansible -m setup --tree out/ all ansible-cmdb out/ > overview.html","title":"Ansible-cmdb"},{"location":"ansible-terraform-workspace/ansible-tools/#ansible-inventory-grapher","text":"Ansible inventory grapher creates a dot file suitable for use by graphviz. Cd to the folder with your Ansible project and execute ansible-inventory-grapher all | dot -Tpng > /home/static-server/my.png","title":"Ansible inventory grapher"},{"location":"ansible-terraform-workspace/ansible-tools/#ansible-playbook-grapher","text":"Ansible Playbook Grapher - is a command line tool that creates a graph representing your Ansible playbook plays, tasks and roles. The aim of this project is to have an overview of your playbook. Cd to the folder with your Ansible project and execute ansible-playbook-grapher --include-role-tasks example.yml -o /home/static-server/example","title":"Ansible Playbook Grapher"},{"location":"ansible-terraform-workspace/ansible-tools/#ansible-doctor","text":"Ansible Doctor is a simple annotation like documentation generator based on Jinja2 templates. while ansible-doctor comes with a default template called readme, it is also possible to write your own templates. The first step is to identify if the given folder is an Ansible role. This check is very simple, if the folder contains a sub-directory called tasks is MUST be an Ansible role! :) After the successful check, ansible-doctor will try to read some static files into a dictionary: - defaults/main.yml - meta/main.yml Cd to the folder with your Ansible project and execute ansible-doctor -o /home/static-server/ roles/example_production","title":"Ansible Doctor"},{"location":"ansible-terraform-workspace/ansible-tools/#schedule-ansible-playbook-executions","text":"Ansible-Terraform workspace has 2 tools (Cronicle and ARA) that make it simple and convenient to use Ansible for periodic tasks and jobs. For example, maintenance jobs for your cloud infrastructure. This is especially handy if you run this workspace on a remote server. Cronicle - allows to schedule tasks and jobs, and lets you observe executions using a nice UI Ansible Ara - tracks all executions of ansible playbooks (manual or scheduled), and has nice UI that provides informationn about every step of every playbook execution You can try scheduling an example ansible playbook with Cronicle NOTE: Scheduling Ansible playbooks is especially useful if you launch Workspace on a remote server rather than on your local laptop.","title":"Schedule Ansible playbook executions"},{"location":"ansible-terraform-workspace/terraform-tools/","text":"Terraform Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. Ansible-Terraform workspace contains a small example Terraform project that creates a server on the Scaleway cloud cd /home/examples/terraform-scaleway/ && terraform init If you want to try, set your Scaleway credentials as environment variable in your workspace. Add the following lines to /home/abc/.zshrc: export SCW_DEFAULT_PROJECT_ID=<YOUR_PROJECT_ID> export SCW_ACCESS_KEY=<YOUR_ACCESS_KEY> export SCW_SECRET_KEY=<YOUR_SECRET_KEY> Restart terminal, and execute cd /home/examples/terraform-scaleway/ && terraform plan Create Scaleway infrastructure with cd /home/examples/terraform-scaleway/ && terraform apply Terraform report A small tool that produces several outputs from a terraform project and visualizes terraform plan as an interactive HTML page. Terraform report can be generated from the small example terraform project, included in the Workspace cd /home/examples/terraform-scaleway/ terraform-report Terraform-report outputs artefacts to the folder /home/static-server/terraform-reports/ . This folder is served by the Static File Server that you can use to view the artifacts, that include interactive HTML pages Example with AWS If you want to try Terraform report with your own AWS account, open workspace and configure AWS profile - create file with AWS credentials mkdir -p ~/.aws nano ~/.aws/credentials The file ~/.aws/credentials would look like this [terraform] aws_access_key_id = <YOUR_AWS_KEY> aws_secret_access_key = <YOUR_AWS_SECRET> Clone your terraform project to the workspace, or if you don't have any, you can use this terraform example repository: git clone https://github.com/pvarentsov/terraform-aws-free-tier /home/project/aws-example Open file /home/project/aws-example/src/free-tier/main.tf and comment out the part that configures S3 backend terraform { backend \"s3\" {} } Initialize a working Terraform directory cd /home/project/aws-example/src/free-tier && terraform init Paste public ssh key (for the sake of example you can type anything) nano ./provision/access/free-tier-ec2-key.pub Now you can generate terraform report terraform-report Use Static File Server to review the report Terraform Rover Rover - is an awesome Terraform visualizer with browser-based UI. Rover helps to better understand Terraform state and planned changes. To see how Rover works, you can use a basic Terraform example in folder /home/examples/terraform-scaleway/ . Initialize Terraform project first cd /home/examples/terraform-scaleway/ && terraform init and start Rover to visualize terraform state rover --workingDir /home/examples/terraform-scaleway/ If you have followed hands-on the tutorial from the previous section (terraform report from the terraform-aws-free-tier repo), you can visualize it with Rover: rover --workingDir /home/project/aws-example/src/free-tier Go to the quickstart page, and open Rover WEB UI Blast Radius Blast Radius is a tool for reasoning about Terraform dependency graphs with interactive visualizations. You can try Blast Radius - launch workspace and visualize an example Terraform project. cd /home/examples/terraform-scaleway && terraform init blast-radius --serve --port 8030 Go to the quickstart page, and open Blast Radius WEB UI NOTE: Blast Radius is a great project, but there is lack of updates to the project recently, and it might not work with some Terraform providers. Terraform Inframap Terraform Inframap can visualize terraform state inframap generate terraform.tfstate | dot -Tpng > graph.png","title":"Terraform tools"},{"location":"ansible-terraform-workspace/terraform-tools/#terraform","text":"Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. Ansible-Terraform workspace contains a small example Terraform project that creates a server on the Scaleway cloud cd /home/examples/terraform-scaleway/ && terraform init If you want to try, set your Scaleway credentials as environment variable in your workspace. Add the following lines to /home/abc/.zshrc: export SCW_DEFAULT_PROJECT_ID=<YOUR_PROJECT_ID> export SCW_ACCESS_KEY=<YOUR_ACCESS_KEY> export SCW_SECRET_KEY=<YOUR_SECRET_KEY> Restart terminal, and execute cd /home/examples/terraform-scaleway/ && terraform plan Create Scaleway infrastructure with cd /home/examples/terraform-scaleway/ && terraform apply","title":"Terraform"},{"location":"ansible-terraform-workspace/terraform-tools/#terraform-report","text":"A small tool that produces several outputs from a terraform project and visualizes terraform plan as an interactive HTML page. Terraform report can be generated from the small example terraform project, included in the Workspace cd /home/examples/terraform-scaleway/ terraform-report Terraform-report outputs artefacts to the folder /home/static-server/terraform-reports/ . This folder is served by the Static File Server that you can use to view the artifacts, that include interactive HTML pages Example with AWS If you want to try Terraform report with your own AWS account, open workspace and configure AWS profile - create file with AWS credentials mkdir -p ~/.aws nano ~/.aws/credentials The file ~/.aws/credentials would look like this [terraform] aws_access_key_id = <YOUR_AWS_KEY> aws_secret_access_key = <YOUR_AWS_SECRET> Clone your terraform project to the workspace, or if you don't have any, you can use this terraform example repository: git clone https://github.com/pvarentsov/terraform-aws-free-tier /home/project/aws-example Open file /home/project/aws-example/src/free-tier/main.tf and comment out the part that configures S3 backend terraform { backend \"s3\" {} } Initialize a working Terraform directory cd /home/project/aws-example/src/free-tier && terraform init Paste public ssh key (for the sake of example you can type anything) nano ./provision/access/free-tier-ec2-key.pub Now you can generate terraform report terraform-report Use Static File Server to review the report","title":"Terraform report"},{"location":"ansible-terraform-workspace/terraform-tools/#terraform-rover","text":"Rover - is an awesome Terraform visualizer with browser-based UI. Rover helps to better understand Terraform state and planned changes. To see how Rover works, you can use a basic Terraform example in folder /home/examples/terraform-scaleway/ . Initialize Terraform project first cd /home/examples/terraform-scaleway/ && terraform init and start Rover to visualize terraform state rover --workingDir /home/examples/terraform-scaleway/ If you have followed hands-on the tutorial from the previous section (terraform report from the terraform-aws-free-tier repo), you can visualize it with Rover: rover --workingDir /home/project/aws-example/src/free-tier Go to the quickstart page, and open Rover WEB UI","title":"Terraform Rover"},{"location":"ansible-terraform-workspace/terraform-tools/#blast-radius","text":"Blast Radius is a tool for reasoning about Terraform dependency graphs with interactive visualizations. You can try Blast Radius - launch workspace and visualize an example Terraform project. cd /home/examples/terraform-scaleway && terraform init blast-radius --serve --port 8030 Go to the quickstart page, and open Blast Radius WEB UI NOTE: Blast Radius is a great project, but there is lack of updates to the project recently, and it might not work with some Terraform providers.","title":"Blast Radius"},{"location":"ansible-terraform-workspace/terraform-tools/#terraform-inframap","text":"Terraform Inframap can visualize terraform state inframap generate terraform.tfstate | dot -Tpng > graph.png","title":"Terraform Inframap"},{"location":"base-workspace/","text":"Base-workspace Containerized Linux environment for running jobs on schedule with browser-based scheduler, terminal and file manager. You can host on your favourite Rasberry Pi. Start docker run --name space-1 --user=root -d -p 8020-8040:8020-8040 alnoda/base-workspace and open localhost:8020 in browser. Features FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Terminal - Full-fledged browser-based terminal with Z-shell. Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Why this image If you need to schedule scripts and jobs, monitor executions, upload and download files. Demo: Base workspace","title":"About"},{"location":"base-workspace/#base-workspace","text":"Containerized Linux environment for running jobs on schedule with browser-based scheduler, terminal and file manager. You can host on your favourite Rasberry Pi.","title":"Base-workspace"},{"location":"base-workspace/#start","text":"docker run --name space-1 --user=root -d -p 8020-8040:8020-8040 alnoda/base-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"base-workspace/#features","text":"FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Terminal - Full-fledged browser-based terminal with Z-shell. Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"base-workspace/#why-this-image","text":"If you need to schedule scripts and jobs, monitor executions, upload and download files. Demo: Base workspace","title":"Why this image"},{"location":"codeserver-workspace/","text":"Code-server workspace General-purpose dockerized development environment. Fully isolated inside a docker container. Includes code editor, terminal, scheduler and filebrowser. Why this images If you need isolated dev environment where you can code and install packages and apps without affecting the base operating system. If you want self-hosted remote development environment. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/codeserver-workspace open localhost:8020 in browser. Features Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo: Code-server workspace","title":"About"},{"location":"codeserver-workspace/#code-server-workspace","text":"General-purpose dockerized development environment. Fully isolated inside a docker container. Includes code editor, terminal, scheduler and filebrowser.","title":"Code-server workspace"},{"location":"codeserver-workspace/#why-this-images","text":"If you need isolated dev environment where you can code and install packages and apps without affecting the base operating system. If you want self-hosted remote development environment.","title":"Why this images"},{"location":"codeserver-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/codeserver-workspace open localhost:8020 in browser.","title":"Start"},{"location":"codeserver-workspace/#features","text":"Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo: Code-server workspace","title":"Features"},{"location":"elasticsearch-workspace/","text":"Elasticsearch workspace Several Elasticsearch CLI tools in a containerized dev/admin workspace. Why this images If you need self-hosted tool to interact with Elasticsearch/Opensearch, schedule backups of indexes, migrate to other clusters, export and import Elasticsearch data to S3. To directly access Elasticsearch/Opensearch inside your kubernetes cluster. Start docker run --name elawid-1 -d -p 8020-8040:8020-8040 alnoda/elasticsearch-workspace and open localhost:8020 in browser. Features Elasticsearch CLI tools elasticdump - awesome tool for moving and saving indices. esbulk - fast parallel command line bulk loading utility for Elasticsearch. vulcanizer - cli for interacting with an Elasticsearch cluster. Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"elasticsearch-workspace/#elasticsearch-workspace","text":"Several Elasticsearch CLI tools in a containerized dev/admin workspace.","title":"Elasticsearch workspace"},{"location":"elasticsearch-workspace/#why-this-images","text":"If you need self-hosted tool to interact with Elasticsearch/Opensearch, schedule backups of indexes, migrate to other clusters, export and import Elasticsearch data to S3. To directly access Elasticsearch/Opensearch inside your kubernetes cluster.","title":"Why this images"},{"location":"elasticsearch-workspace/#start","text":"docker run --name elawid-1 -d -p 8020-8040:8020-8040 alnoda/elasticsearch-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"elasticsearch-workspace/#features","text":"Elasticsearch CLI tools elasticdump - awesome tool for moving and saving indices. esbulk - fast parallel command line bulk loading utility for Elasticsearch. vulcanizer - cli for interacting with an Elasticsearch cluster. Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"elasticsearch-workspace/tutorial/","text":"This tutorial demonstrates how to use Elasticsearch workspace to explore cluster and export index to S3. Setup Create docker-compose.yaml file to launch locally 3-node Elasticsearch cluster with Kibana version: '2.2' services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data01:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - elastic es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/usr/share/elasticsearch/data networks: - elastic es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/usr/share/elasticsearch/data networks: - elastic kib01: image: docker.elastic.co/kibana/kibana:7.16.3 container_name: kib01 ports: - 5601:5601 environment: ELASTICSEARCH_URL: http://es01:9200 ELASTICSEARCH_HOSTS: '[\"http://es01:9200\",\"http://es02:9200\",\"http://es03:9200\"]' networks: - elastic workspace: image: alnoda/elasticsearch-workspace container_name: workspace ports: - 8020-8030:8020-8030 networks: - elastic volumes: data01: driver: local data02: driver: local data03: driver: local networks: elastic: driver: bridge and start it with docker-compose up Wait untill the cluster is fully ready, open Kibana on http://localhost:5601 and import sample datasets. Go to the workspace Quickstart page for quick access to all the workspace tools Check cluster Open browser-based terminal and check cluster nodes and shards vulcanizer --host es01 nodes vulcanizer --host es01 shards Create backup Use elasticdump to export index kibana_sample_data_ecommerce (from eCommerce sample dataset) to S3 elasticdump \\ --s3AccessKeyId \"${access_key_id}\" \\ --s3SecretAccessKey \"${access_key_secret}\" \\ --input=http://es01:9200/kibana_sample_data_ecommerce \\ --output \"s3://${bucket_name}/kibana_sample_data_ecommerce.json\" Schedule Open workspace IDE and create file /home/project/export.sh file with the script to export data to S3. Make it executable with chmod +x /home/project/export.sh . Open workspace Scheduler (user/pass: admin/admin), and schedule script, for example weekly. Select category - \"general\", plugin - \"Shell Script\"","title":"Tutorial"},{"location":"elasticsearch-workspace/tutorial/#setup","text":"Create docker-compose.yaml file to launch locally 3-node Elasticsearch cluster with Kibana version: '2.2' services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data01:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - elastic es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/usr/share/elasticsearch/data networks: - elastic es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/usr/share/elasticsearch/data networks: - elastic kib01: image: docker.elastic.co/kibana/kibana:7.16.3 container_name: kib01 ports: - 5601:5601 environment: ELASTICSEARCH_URL: http://es01:9200 ELASTICSEARCH_HOSTS: '[\"http://es01:9200\",\"http://es02:9200\",\"http://es03:9200\"]' networks: - elastic workspace: image: alnoda/elasticsearch-workspace container_name: workspace ports: - 8020-8030:8020-8030 networks: - elastic volumes: data01: driver: local data02: driver: local data03: driver: local networks: elastic: driver: bridge and start it with docker-compose up Wait untill the cluster is fully ready, open Kibana on http://localhost:5601 and import sample datasets. Go to the workspace Quickstart page for quick access to all the workspace tools","title":"Setup"},{"location":"elasticsearch-workspace/tutorial/#check-cluster","text":"Open browser-based terminal and check cluster nodes and shards vulcanizer --host es01 nodes vulcanizer --host es01 shards","title":"Check cluster"},{"location":"elasticsearch-workspace/tutorial/#create-backup","text":"Use elasticdump to export index kibana_sample_data_ecommerce (from eCommerce sample dataset) to S3 elasticdump \\ --s3AccessKeyId \"${access_key_id}\" \\ --s3SecretAccessKey \"${access_key_secret}\" \\ --input=http://es01:9200/kibana_sample_data_ecommerce \\ --output \"s3://${bucket_name}/kibana_sample_data_ecommerce.json\"","title":"Create backup"},{"location":"elasticsearch-workspace/tutorial/#schedule","text":"Open workspace IDE and create file /home/project/export.sh file with the script to export data to S3. Make it executable with chmod +x /home/project/export.sh . Open workspace Scheduler (user/pass: admin/admin), and schedule script, for example weekly. Select category - \"general\", plugin - \"Shell Script\"","title":"Schedule"},{"location":"erlang-elixir-workspace/","text":"Erlang-Elixir workspace Docker image with Erlang, Elixir and browser-based VS-Code version. Why this images If you need self-hosted remote development environment. If you want to be one command away from coding in Erlang, Elixir. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/erlang-elixir-workspace and open localhost:8020 in browser. Features Erlang/OTP Elixir Kerl - easy building and installing of Erlang/OTP instances. Kiex - allows you to easily build and switch between different Elixir versions. Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"erlang-elixir-workspace/#erlang-elixir-workspace","text":"Docker image with Erlang, Elixir and browser-based VS-Code version.","title":"Erlang-Elixir workspace"},{"location":"erlang-elixir-workspace/#why-this-images","text":"If you need self-hosted remote development environment. If you want to be one command away from coding in Erlang, Elixir.","title":"Why this images"},{"location":"erlang-elixir-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/erlang-elixir-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"erlang-elixir-workspace/#features","text":"Erlang/OTP Elixir Kerl - easy building and installing of Erlang/OTP instances. Kiex - allows you to easily build and switch between different Elixir versions. Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"erlang-elixir-workspace/tutorial/","text":"This is a basic tutorial showing how to work with Erlang and Elixir in the workspace. Erlang Shell The Erlang shell is used for testing of expressions. Start the Erlang shell from a command prompt with the command erl having started shell, evaluate the following expressions 2 + 5. (42 + 77) * 66 / 3. Str = \"abcd\". L = length(Str). Hello world Go to the Quickstart page, open VS-Code and create file hello.erl with the following code -module(hello). -export([hello_world/0]). hello_world() -> io:format(\"Hello, World!~n\", []). Now open Terminal, start Erlang shell with erl and execute c(hello). Elixir Shell iex is a command which stands for Interactive Elixir. Openn terminal, start interactive Elixir shell with iex and evaluate 40 + 8 \"hello\" <> \" world\" Scripts Open IDE and create file hello.exs with the following code IO.puts(\"Hello world from Elixir\") In terminal execute elixir hello.exs Manage Elixir versions with Kiex Kiex allows you to easily build and switch between different Elixir versions. List installed versions kiex list List known releases kiex list releases Install a known release kiex install 1.13.0 Use specific elixir version kiex use 1.13.0","title":"Tutorial"},{"location":"erlang-elixir-workspace/tutorial/#erlang","text":"","title":"Erlang"},{"location":"erlang-elixir-workspace/tutorial/#shell","text":"The Erlang shell is used for testing of expressions. Start the Erlang shell from a command prompt with the command erl having started shell, evaluate the following expressions 2 + 5. (42 + 77) * 66 / 3. Str = \"abcd\". L = length(Str).","title":"Shell"},{"location":"erlang-elixir-workspace/tutorial/#hello-world","text":"Go to the Quickstart page, open VS-Code and create file hello.erl with the following code -module(hello). -export([hello_world/0]). hello_world() -> io:format(\"Hello, World!~n\", []). Now open Terminal, start Erlang shell with erl and execute c(hello).","title":"Hello world"},{"location":"erlang-elixir-workspace/tutorial/#elixir","text":"","title":"Elixir"},{"location":"erlang-elixir-workspace/tutorial/#shell_1","text":"iex is a command which stands for Interactive Elixir. Openn terminal, start interactive Elixir shell with iex and evaluate 40 + 8 \"hello\" <> \" world\"","title":"Shell"},{"location":"erlang-elixir-workspace/tutorial/#scripts","text":"Open IDE and create file hello.exs with the following code IO.puts(\"Hello world from Elixir\") In terminal execute elixir hello.exs","title":"Scripts"},{"location":"erlang-elixir-workspace/tutorial/#manage-elixir-versions-with-kiex","text":"Kiex allows you to easily build and switch between different Elixir versions. List installed versions kiex list List known releases kiex list releases Install a known release kiex install 1.13.0 Use specific elixir version kiex use 1.13.0","title":"Manage Elixir versions with Kiex"},{"location":"get-started/common-features/","text":"Every workspace has browser-based VS-Code version, full-screen terminal, file manager, and task scheduler. You can code, upload and dowload files and schedule periodic executios of scripts and jobs. Quickstart From the quickstart page you can open workspace apps, such as code editor or terminal Demo: Quickstart page Code Editor Code editor is a browser-based open-source Visual Studio Code. It is fast, responsive, and full-featured. It features code highlighting, autocompletion, a great number of pre-installed color themes. You can install any extension from open-vsx.org that has hundreeds of extensions for VS Code compatible editors. Terminal Workspace has full-size browser-base terminal File Browser File browser helps to manage files and folders within the workspace, as well as upload and download files and folders to and from workspace. Scheduler Cronicle can execute on schedule scripts, jobs and tasks. It has nice UI to monitor executions and failures","title":"Common features"},{"location":"get-started/common-features/#quickstart","text":"From the quickstart page you can open workspace apps, such as code editor or terminal Demo: Quickstart page","title":"Quickstart"},{"location":"get-started/common-features/#code-editor","text":"Code editor is a browser-based open-source Visual Studio Code. It is fast, responsive, and full-featured. It features code highlighting, autocompletion, a great number of pre-installed color themes. You can install any extension from open-vsx.org that has hundreeds of extensions for VS Code compatible editors.","title":"Code Editor"},{"location":"get-started/common-features/#terminal","text":"Workspace has full-size browser-base terminal","title":"Terminal"},{"location":"get-started/common-features/#file-browser","text":"File browser helps to manage files and folders within the workspace, as well as upload and download files and folders to and from workspace.","title":"File Browser"},{"location":"get-started/common-features/#scheduler","text":"Cronicle can execute on schedule scripts, jobs and tasks. It has nice UI to monitor executions and failures","title":"Scheduler"},{"location":"get-started/create-workspace/","text":"Workspace is a docker image. To create workspace you need to have Docker on your computer. If you have docker execute docker run command, providing the port mapping for the range of ports from 8020 to 8040. For example, to start Codeserver Workspace open terminal, and execute docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/codeserver-workspace Open Quickstart page on localhost:8020 Demo: Quickstart page","title":"Create workspace"},{"location":"get-started/schedule-script/","text":"This tutorial shows how to scrap Google news on schedule. We use Python for the demonstration purpose. To start, open workspace terminal and install Python package GNews pip install gnews Create python script nano scraper.py Paste the following code snippet. This is scrapper, that will output results to a file from gnews import GNews import json google_news = GNews() usa_news = google_news.get_news('USA') with open('usa_news.json', 'w') as f: json.dump(usa_news, f) Save the file with Ctrl+s and return to the terminal with Ctrl+x . Execute the script python3 scraper.py Navigate to Quickstart and Open file browser. View and download the output file Schedule daily executions with Cronicle","title":"Schedule script executions"},{"location":"get-started/working-in-terminal/","text":"For convenient use of CLI applications, workspaces have full-screen browser-based terminal. Workspace is based on Ubuntu 20.04 with many typical CLI applications pre-installed, such as Git, Vim, Nano and Curl. Install new applications Open workspace terminal to install new applications. Simply execute apt install with sudo . For example, install emacs sudo apt-get install emacs If you want to install PHP, execute sudo add-apt-repository ppa:ondrej/php sudo apt-get update sudo apt-get install php8.1 Python Python and Pip are installed. Execute python3 in terminal. To install python packages use PIP pip install pandas Node.js Use Nodeenv to create Node.js environments. For example, open workspace terminal, create folder npmgui, and activate environment with node v.12.18.3 and npm v.6.0.0 cd /home mkdir npmgui; cd npmgui nodeenv --node=12.18.3 --npm=6.0.0 env Let's install package and start node application, explicitly on port 8040 . env/bin/activate && npm i -g npm-gui npm-gui 0.0.0.0:8040 In the Quicklaunch go to the tab 'My apps' and open app on the port 8040. NOTE: If you close terminal, the application will stop. If you want application to keep running after workspace terminal is closed start it with \"&!\" at the end. Keep services runnning Any application started in the terminal will run as long as your terminal session is alive. If you want any application or service runing after terminal session is closed, start service with \"&!\" at the end of the command. For example, to start npm-gui and keep it running after terminal is closed, run npm-gui 0.0.0.0:8040 &!","title":"Working in terminal"},{"location":"get-started/working-in-terminal/#install-new-applications","text":"Open workspace terminal to install new applications. Simply execute apt install with sudo . For example, install emacs sudo apt-get install emacs If you want to install PHP, execute sudo add-apt-repository ppa:ondrej/php sudo apt-get update sudo apt-get install php8.1","title":"Install new applications"},{"location":"get-started/working-in-terminal/#python","text":"Python and Pip are installed. Execute python3 in terminal. To install python packages use PIP pip install pandas","title":"Python"},{"location":"get-started/working-in-terminal/#nodejs","text":"Use Nodeenv to create Node.js environments. For example, open workspace terminal, create folder npmgui, and activate environment with node v.12.18.3 and npm v.6.0.0 cd /home mkdir npmgui; cd npmgui nodeenv --node=12.18.3 --npm=6.0.0 env Let's install package and start node application, explicitly on port 8040 . env/bin/activate && npm i -g npm-gui npm-gui 0.0.0.0:8040 In the Quicklaunch go to the tab 'My apps' and open app on the port 8040. NOTE: If you close terminal, the application will stop. If you want application to keep running after workspace terminal is closed start it with \"&!\" at the end.","title":"Node.js"},{"location":"get-started/working-in-terminal/#keep-services-runnning","text":"Any application started in the terminal will run as long as your terminal session is alive. If you want any application or service runing after terminal session is closed, start service with \"&!\" at the end of the command. For example, to start npm-gui and keep it running after terminal is closed, run npm-gui 0.0.0.0:8040 &!","title":"Keep services runnning"},{"location":"go-workspace/","text":"Go workspace Docker image with Go and browser-based VS-Code version. Why this images If you need self-hosted remote development environment. If you want to be one terminal command away from coding in Go. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/go-workspace open localhost:8020 in browser. Features Go Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"go-workspace/#go-workspace","text":"Docker image with Go and browser-based VS-Code version.","title":"Go workspace"},{"location":"go-workspace/#why-this-images","text":"If you need self-hosted remote development environment. If you want to be one terminal command away from coding in Go.","title":"Why this images"},{"location":"go-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/go-workspace open localhost:8020 in browser.","title":"Start"},{"location":"go-workspace/#features","text":"Go Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"go-workspace/tutorial/","text":"This simple tutorial demonstrates how to create Go Hello-world application, install dependencies with Go Modules, build, run, and install Go applications. Example: hello world Check Go version go version Create new Go project mkdir myProject/ cd myProject go mod init myProject Create file main.go package main import \"fmt\" func main() { fmt.Println(\"Hello Go\") } Then test it using the go run command go run main.go Dependencies Go Modules - Go\u2019s dependency management system that makes dependency version information explicit and easier to manage. Create new Go project mkdir simpleserver/ cd simpleserver go mod init simpleserver Adding a remote module as a dependency manually: go get github.com/spf13/cobra@latest Check go.mod file cat go.mod Create file main.go package main import \"github.com/gin-gonic/gin\" func main() { r := gin.Default() r.GET(\"/\", func(c *gin.Context) { c.JSON(200, gin.H{ \"message\": \"pong\", }) }) r.Run() } To add module requirements and sums execute go mod tidy Run, build and install go run - to quickly test your go code and to check the output. But internally it compiles your code and builds an executable binary in a temporary location, launches that temp exe-file and finally cleans it when your app exits. go build - compile and builds executable in current directory. go install - will compile and move the executable to executable directory included in $PATH, so that you can run this executable from any path on the terminal. Run the simple server with export PORT=8040 go run main.go Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app Build executable locally go build This will create an executable simpleserver in the same folder. Build and move to executable folder go install Now you can execute anywhere in terminal export PORT=8040 simpleserver and the server will start","title":"Tutorial"},{"location":"go-workspace/tutorial/#example-hello-world","text":"Check Go version go version Create new Go project mkdir myProject/ cd myProject go mod init myProject Create file main.go package main import \"fmt\" func main() { fmt.Println(\"Hello Go\") } Then test it using the go run command go run main.go","title":"Example: hello world"},{"location":"go-workspace/tutorial/#dependencies","text":"Go Modules - Go\u2019s dependency management system that makes dependency version information explicit and easier to manage. Create new Go project mkdir simpleserver/ cd simpleserver go mod init simpleserver Adding a remote module as a dependency manually: go get github.com/spf13/cobra@latest Check go.mod file cat go.mod Create file main.go package main import \"github.com/gin-gonic/gin\" func main() { r := gin.Default() r.GET(\"/\", func(c *gin.Context) { c.JSON(200, gin.H{ \"message\": \"pong\", }) }) r.Run() } To add module requirements and sums execute go mod tidy","title":"Dependencies"},{"location":"go-workspace/tutorial/#run-build-and-install","text":"go run - to quickly test your go code and to check the output. But internally it compiles your code and builds an executable binary in a temporary location, launches that temp exe-file and finally cleans it when your app exits. go build - compile and builds executable in current directory. go install - will compile and move the executable to executable directory included in $PATH, so that you can run this executable from any path on the terminal. Run the simple server with export PORT=8040 go run main.go Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app Build executable locally go build This will create an executable simpleserver in the same folder. Build and move to executable folder go install Now you can execute anywhere in terminal export PORT=8040 simpleserver and the server will start","title":"Run, build and install"},{"location":"ide-workspace/","text":"IDE-workspace General-purpose dockerized development environment. Fully isolated inside a docker container. Includes code editor, terminal, scheduler and filebrowser. Why this images If you need isolated dev environment where you can code and install packages and apps without affecting the base operating system. If you want self-hosted remote development environment. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/ide-workspace and open localhost:8020 in browser. Features Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo: IDE workspace","title":"About"},{"location":"ide-workspace/#ide-workspace","text":"General-purpose dockerized development environment. Fully isolated inside a docker container. Includes code editor, terminal, scheduler and filebrowser.","title":"IDE-workspace"},{"location":"ide-workspace/#why-this-images","text":"If you need isolated dev environment where you can code and install packages and apps without affecting the base operating system. If you want self-hosted remote development environment.","title":"Why this images"},{"location":"ide-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/ide-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"ide-workspace/#features","text":"Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo: IDE workspace","title":"Features"},{"location":"java-workspace/","text":"Java workspace Docker image with Java and browser-based VS-Code version. Why this images If you need self-hosted remote development environment. If you want to be one command away from coding in Java. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/java-workspace and open localhost:8020 in browser. Features Java Maven Gradle Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"java-workspace/#java-workspace","text":"Docker image with Java and browser-based VS-Code version.","title":"Java workspace"},{"location":"java-workspace/#why-this-images","text":"If you need self-hosted remote development environment. If you want to be one command away from coding in Java.","title":"Why this images"},{"location":"java-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/java-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"java-workspace/#features","text":"Java Maven Gradle Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"java-workspace/tutorial/","text":"This basic tutorial demonstrates how to create Java Hello-world application, and use Maven or Gradle to install dependnecies and build applications. Hello world java -version Open IDE and create file Main.java with the following content public class Main { public static void main(String[] args) { System.out.println(\"Hello World\"); } } Use terminal to compile and execute cd /home/project javac Main.java java Main Maven Maven helps with: Making the build process easy Providing a uniform build system Providing quality project information Encouraging better development practices Check Maven version in terminal mvn -v Build Java code cp -r /home/abc/example /home/project/example cd /home/project/example mvn compile This will run Maven, telling it to execute the compile goal. When it\u2019s finished, you should find the compiled .class files in the target/classes directory. Run the package goal mvn package To execute the JAR file run java -jar target/gs-maven-0.1.0.jar (taken from https://spring.io/guides/gs/maven/) Gradle Copy example project cp -r /home/abc/example /home/project/example cd /home/project/example Check Gradle installation, run Gradle from the command-line cd /home/project/example gradle Initialize Gradle gradle init Now that Gradle is installed, see what it can do gradle tasks Build project with Gradle gradle build","title":"Tutorial"},{"location":"java-workspace/tutorial/#hello-world","text":"java -version Open IDE and create file Main.java with the following content public class Main { public static void main(String[] args) { System.out.println(\"Hello World\"); } } Use terminal to compile and execute cd /home/project javac Main.java java Main","title":"Hello world"},{"location":"java-workspace/tutorial/#maven","text":"Maven helps with: Making the build process easy Providing a uniform build system Providing quality project information Encouraging better development practices Check Maven version in terminal mvn -v Build Java code cp -r /home/abc/example /home/project/example cd /home/project/example mvn compile This will run Maven, telling it to execute the compile goal. When it\u2019s finished, you should find the compiled .class files in the target/classes directory. Run the package goal mvn package To execute the JAR file run java -jar target/gs-maven-0.1.0.jar (taken from https://spring.io/guides/gs/maven/)","title":"Maven"},{"location":"java-workspace/tutorial/#gradle","text":"Copy example project cp -r /home/abc/example /home/project/example cd /home/project/example Check Gradle installation, run Gradle from the command-line cd /home/project/example gradle Initialize Gradle gradle init Now that Gradle is installed, see what it can do gradle tasks Build project with Gradle gradle build","title":"Gradle"},{"location":"kafka-workspace/","text":"Kafka workspace Single-node Kafka cluster together with several Kafka CLI tools in containerized dev/admin environment. Why this images If you need a tool to interact with Kakfa, such as produce and consume events, explore, manage, query and troubleshoot your Kafka clusters To directly access Kafka inside your kubernetes cluster. A better Kafka docker image for local dev environment. You get single-nnode Kafka together with the toolset to work with it. If you want to get started with Kafka easy and fast. Motivation explained in this Medium article Start docker run --name rwid-1 -d -p 8020-8040:8020-8040 alnoda/kafka-workspace and open localhost:8020 in browser. Features Single-node Kafka cluster Kafka CLI tools kcat - generic non-JVM producer and consumer for Apache Kafka. kafkactl - command-line interface for interaction with Apache Kafka. trubka - Kafka CLI tool built in Go which gives you everything you need. kt - Kafka tool that likes JSON. kcli - Kafka read only command line browser. Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"kafka-workspace/#kafka-workspace","text":"Single-node Kafka cluster together with several Kafka CLI tools in containerized dev/admin environment.","title":"Kafka workspace"},{"location":"kafka-workspace/#why-this-images","text":"If you need a tool to interact with Kakfa, such as produce and consume events, explore, manage, query and troubleshoot your Kafka clusters To directly access Kafka inside your kubernetes cluster. A better Kafka docker image for local dev environment. You get single-nnode Kafka together with the toolset to work with it. If you want to get started with Kafka easy and fast. Motivation explained in this Medium article","title":"Why this images"},{"location":"kafka-workspace/#start","text":"docker run --name rwid-1 -d -p 8020-8040:8020-8040 alnoda/kafka-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"kafka-workspace/#features","text":"Single-node Kafka cluster Kafka CLI tools kcat - generic non-JVM producer and consumer for Apache Kafka. kafkactl - command-line interface for interaction with Apache Kafka. trubka - Kafka CLI tool built in Go which gives you everything you need. kt - Kafka tool that likes JSON. kcli - Kafka read only command line browser. Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"kafka-workspace/tutorial/","text":"This tutorial demonstrates how to use Kafka workspace to explore and interact with Kafka. Singl-node Kafka cluster is running within the workspace, but all the tools can be used with the external Kafka clusters. To start, open a Quickstart page, and go to the VS-code. We will connect to the local Kafka cluster using VS-code Kafka extension. You only need to provide the name for the cluster, which can be any. Using VS-code Kafka extension create topic \"quickstart-events\", and cosume events from this topic directly in VS-code using Kafka extension Kafka native tools Kafka distribution itself contains command line tools that allow to create topics, send and consume events, etc. Open workspace terminal http://localhost:8026/ and go to Kafka directory cd /opt/kafka create topic bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --topic quickstart-events --bootstrap-server localhost:9092 send some messages bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092 consume messages bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092 kt Kafka tool that likes JSON. Configure brokers, topic, Kafka version and authentication via environment variables KT_BROKERS, KT_TOPIC, KT_KAFKA_VERSION and KT_AUTH. Set topic to \"quickstart-events\" (local Kafka instance by default) export KT_TOPIC=\"quickstart-events\" Get information about topics, brockers and consumer groups kt topic kt group consume messages kt consume produce messages echo 'Bob wins Oscar' | kt produce -topic quickstart-events -literal kafkactl A command-line interface for interaction with Apache Kafka. Consume from topic \"quickstart-events\" kafkactl consume quickstart-events --from-beginning kafkactl consume quickstart-events --from-beginning --print-keys --print-timestamps -o yaml kcat Generic non-JVM producer and consumer for Apache Kafka. Consume topic \"quickstart-events\" kafkacat -b localhost -t quickstart-events Produce events to the topic echo \"Hello World\" | kafkacat -b localhost -t quickstart-events kcli Kafka read only command line browser Launch kcli in the Workspace terminal kcli trubka Kafka CLI tool built in Go which gives you everything you need. Consume from the topic \"quickstart-events\" trubka consume plain quickstart-events --brokers localhost:9092 Produce message to the topic trubka produce plain quickstart-events \"Random Data\" --brokers localhost:9092","title":"Tutorial"},{"location":"kafka-workspace/tutorial/#kafka-native-tools","text":"Kafka distribution itself contains command line tools that allow to create topics, send and consume events, etc. Open workspace terminal http://localhost:8026/ and go to Kafka directory cd /opt/kafka create topic bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --topic quickstart-events --bootstrap-server localhost:9092 send some messages bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092 consume messages bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092","title":"Kafka native tools"},{"location":"kafka-workspace/tutorial/#kt","text":"Kafka tool that likes JSON. Configure brokers, topic, Kafka version and authentication via environment variables KT_BROKERS, KT_TOPIC, KT_KAFKA_VERSION and KT_AUTH. Set topic to \"quickstart-events\" (local Kafka instance by default) export KT_TOPIC=\"quickstart-events\" Get information about topics, brockers and consumer groups kt topic kt group consume messages kt consume produce messages echo 'Bob wins Oscar' | kt produce -topic quickstart-events -literal","title":"kt"},{"location":"kafka-workspace/tutorial/#kafkactl","text":"A command-line interface for interaction with Apache Kafka. Consume from topic \"quickstart-events\" kafkactl consume quickstart-events --from-beginning kafkactl consume quickstart-events --from-beginning --print-keys --print-timestamps -o yaml","title":"kafkactl"},{"location":"kafka-workspace/tutorial/#kcat","text":"Generic non-JVM producer and consumer for Apache Kafka. Consume topic \"quickstart-events\" kafkacat -b localhost -t quickstart-events Produce events to the topic echo \"Hello World\" | kafkacat -b localhost -t quickstart-events","title":"kcat"},{"location":"kafka-workspace/tutorial/#kcli","text":"Kafka read only command line browser Launch kcli in the Workspace terminal kcli","title":"kcli"},{"location":"kafka-workspace/tutorial/#trubka","text":"Kafka CLI tool built in Go which gives you everything you need. Consume from the topic \"quickstart-events\" trubka consume plain quickstart-events --brokers localhost:9092 Produce message to the topic trubka produce plain quickstart-events \"Random Data\" --brokers localhost:9092","title":"trubka"},{"location":"kubespray-workspace/","text":"Kubespray workspace Collection of tools to install, explore, develop, manage and maintain Kubernetes cluster. Why this images You need to set up and manage Kubernetes cluster with Kubespray and Ansible. You need a convenient tool that has everything needed to create infrastructure, install and manage Kubernetes cluster. All you need is servers with SSH access (via SSH key) set up. Start workspace, follow brief istructions, and you will have Kubernetes cluster up and running. Moreover, with this workspace you will have a broad toolset to interact, explore, monitor and develop this cluster. Workspace will also be useful for declarative infrastructure creation and visualisation (Terraform, Rover, Blast-radius and other) create, schedule and monitor maintenance tasks for the k8s cluster and servers (Cronicle, Ansible, Ansible Ara) work entirely inside a private network (Browser-based VS-Code, browser-based terminal, browser-based Filebrowser) Start docker run --name rwid-1 -d -p 8020-8040:8020-8040 -p 9000:9000 alnoda/kubespray-workspace and open localhost:8020 in browser. Features Kubernetes tools: Kubespray with required dependencies. Octant - highly extensible platform for developers to better understand the complexity of Kubernetes clusters. Kubectl - Kubernetes command-line tool, allows you to run commands against Kubernetes clusters. Helm - package manager for Kubernetes. K9s - Kubernetes CLI To Manage Your Clusters In Style. Kube-shell - integrated shell for working with the Kubernetes CLI. Krew - plugin manager for kubectl command-line tool. kubectl-aliases - hundreds of convenient shell aliases for kubectl. Ansible tools: Ansible Ansible Ara Ansible-cmdb Ansible inventory grapher Ansible Playbook Grapher Ansible Lint Ansible Doctor Terraform tools: Terrraform Pre-commit-terraform Rover Blast-Radius Terraform Visual Terraform Graph Inframap Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip, Pipx Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Index"},{"location":"kubespray-workspace/#kubespray-workspace","text":"Collection of tools to install, explore, develop, manage and maintain Kubernetes cluster.","title":"Kubespray workspace"},{"location":"kubespray-workspace/#why-this-images","text":"You need to set up and manage Kubernetes cluster with Kubespray and Ansible. You need a convenient tool that has everything needed to create infrastructure, install and manage Kubernetes cluster. All you need is servers with SSH access (via SSH key) set up. Start workspace, follow brief istructions, and you will have Kubernetes cluster up and running. Moreover, with this workspace you will have a broad toolset to interact, explore, monitor and develop this cluster. Workspace will also be useful for declarative infrastructure creation and visualisation (Terraform, Rover, Blast-radius and other) create, schedule and monitor maintenance tasks for the k8s cluster and servers (Cronicle, Ansible, Ansible Ara) work entirely inside a private network (Browser-based VS-Code, browser-based terminal, browser-based Filebrowser)","title":"Why this images"},{"location":"kubespray-workspace/#start","text":"docker run --name rwid-1 -d -p 8020-8040:8020-8040 -p 9000:9000 alnoda/kubespray-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"kubespray-workspace/#features","text":"Kubernetes tools: Kubespray with required dependencies. Octant - highly extensible platform for developers to better understand the complexity of Kubernetes clusters. Kubectl - Kubernetes command-line tool, allows you to run commands against Kubernetes clusters. Helm - package manager for Kubernetes. K9s - Kubernetes CLI To Manage Your Clusters In Style. Kube-shell - integrated shell for working with the Kubernetes CLI. Krew - plugin manager for kubectl command-line tool. kubectl-aliases - hundreds of convenient shell aliases for kubectl. Ansible tools: Ansible Ansible Ara Ansible-cmdb Ansible inventory grapher Ansible Playbook Grapher Ansible Lint Ansible Doctor Terraform tools: Terrraform Pre-commit-terraform Rover Blast-Radius Terraform Visual Terraform Graph Inframap Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip, Pipx Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"kubespray-workspace/tutorial/","text":"This is a brief tutorial on how to use Kubespray workspace to create kubernetes cluster with Kubespray and manage it with Octant, kubectl, k9s, and other tools included in the workspace. All of the required tools, applications and dependencies are already set up and configured inside the Workspace, so you don't need to make any additional configuration, apart of configuring SSH access to your servers. Preparation Upload to the workspace SSH keys for your servers, on which you want to install K8s. For this you can use Filebrowser, VS-Code or paste the contains of the key in a Terminal. If you want to use terminal, open it from the Quickstart page, and execute nano ~/.ssh/id_rsa # paste ssh key, save and exeit(Ctrl+x) # change permission of your ssh key: chmod 400 ~/.ssh/id_rsa In order to make the next preparation steps, execute the following commands in the workspace terminal. cd to the Kubespray project folder cd /home/project/kubespray Update ansible config to use the ssh key you copied. nano /home/project/kubespray/ansible.cfg Copy inventory/sample as inventory/mycluster cp -rfp inventory/sample inventory/mycluster Update Ansible inventory file with inventory builder. Set your servers IPS instead! declare -a IPS=(10.10.1.3 10.10.1.4 10.10.1.5) CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]} Test you can connect to all the servers ansible -m ping -i inventory/mycluster/hosts.yaml all --user=root Installation If all the preparation steps are done properly, the installation of k8s cluster is a single step. Cd to the Kubespray folder and execute ansible-playbook -i inventory/mycluster/hosts.yaml --user=root --become --become-user=root cluster.yml Post-installation It remains to copy kubeconfig and configure kubectl. SSH to k8s master node and show the entire kubeconfig cat ~/.kube/config Copy the content, and change server IP from https://127.0.0.1:6443 to your master node IP. Go back to the Workspace terminal, and paste this to the '~/.kube/config' file mkdir -p ~/.kube/ nano ~/.kube/config Test kubectl kubectl get nodes You should be able to see your kubernetes nodes. Now you can interact with your kubernetes cluster from the workspace using any of these tools CLI tools kubectl helm k9s kube-shell UI tools Octant","title":"Tutorial"},{"location":"kubespray-workspace/tutorial/#preparation","text":"Upload to the workspace SSH keys for your servers, on which you want to install K8s. For this you can use Filebrowser, VS-Code or paste the contains of the key in a Terminal. If you want to use terminal, open it from the Quickstart page, and execute nano ~/.ssh/id_rsa # paste ssh key, save and exeit(Ctrl+x) # change permission of your ssh key: chmod 400 ~/.ssh/id_rsa In order to make the next preparation steps, execute the following commands in the workspace terminal. cd to the Kubespray project folder cd /home/project/kubespray Update ansible config to use the ssh key you copied. nano /home/project/kubespray/ansible.cfg Copy inventory/sample as inventory/mycluster cp -rfp inventory/sample inventory/mycluster Update Ansible inventory file with inventory builder. Set your servers IPS instead! declare -a IPS=(10.10.1.3 10.10.1.4 10.10.1.5) CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]} Test you can connect to all the servers ansible -m ping -i inventory/mycluster/hosts.yaml all --user=root","title":"Preparation"},{"location":"kubespray-workspace/tutorial/#installation","text":"If all the preparation steps are done properly, the installation of k8s cluster is a single step. Cd to the Kubespray folder and execute ansible-playbook -i inventory/mycluster/hosts.yaml --user=root --become --become-user=root cluster.yml","title":"Installation"},{"location":"kubespray-workspace/tutorial/#post-installation","text":"It remains to copy kubeconfig and configure kubectl. SSH to k8s master node and show the entire kubeconfig cat ~/.kube/config Copy the content, and change server IP from https://127.0.0.1:6443 to your master node IP. Go back to the Workspace terminal, and paste this to the '~/.kube/config' file mkdir -p ~/.kube/ nano ~/.kube/config Test kubectl kubectl get nodes You should be able to see your kubernetes nodes. Now you can interact with your kubernetes cluster from the workspace using any of these tools CLI tools kubectl helm k9s kube-shell UI tools Octant","title":"Post-installation"},{"location":"mkdocs-magicspace/","text":"MkDocs-MagicSpace MkDocs-MagicSpace is an all-in-one tool, carefully crafted to make the development of gorgeous documentation websites like this one as easy as possible. Why this images If want a ready tool to develop and build beautiful doccumentation websites with only Markdown. Why documentation websites with MkDocs Why create separate documentation websites? And if so, why MkDocs? MkDocs website with beautiful themes looks much better than any readme file. The resulting documentation website looks professional and awesome. MkDocs adds text search to your documentation website. In the case of closed-source software, sharing readme files from the git repository with external users is not an option. Github does not render beautiful extended markdown features like admonitions, tabs, etc. Neither renders diagrams, formulas, swagger docs, or notebooks. Using MkDocs-MagicSpace you create documentation from the same markdown readme files you have in your repo together with the code. And you can create a unified documentation website from multiple repositories in Github, GitLab, Bitbucket. You can add such features as Google Analytics, multi-language localization. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/mkdocs-magicspace and open localhost:8020 in browser. Features MkDocs: MkDocs - a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Material for MkDocs - gorgeous theme for MkDocs. PyMdown Extensions - add even more cool features of the extended markdown: sub- and superscripts, keys, magic links, sane headers etc. Mkdocs-macro plugin - add variables and macros written in Python! Mkdocs-multirepo-plugin - import docs directly from git repositories. Mkdocs-monorepo plugin - build multiple documentation folders in a single Mkdocs. Designed for large codebases. MkDocs Newsletter - show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. Mkdocs-mermaid2-plugin - renders textual graph descriptions into Mermaid graphs (flow charts, sequence diagrams, pie charts, etc.). Pygments - a generic syntax highlighter suitable for use in code hosting, forums, wikis or other applications that need to prettify source code, with over 500 languages and other text formats. Mkdocs-include-markdown-plugin - include Markdown files completely or partially, and include files of any type. Mkdocs-table-reader-plugin - directly insert CSV files as tables in your website. Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Docs See our guides on getting started , extended Markdown tutorials and advanced features .","title":"About"},{"location":"mkdocs-magicspace/#mkdocs-magicspace","text":"MkDocs-MagicSpace is an all-in-one tool, carefully crafted to make the development of gorgeous documentation websites like this one as easy as possible.","title":"MkDocs-MagicSpace"},{"location":"mkdocs-magicspace/#why-this-images","text":"If want a ready tool to develop and build beautiful doccumentation websites with only Markdown.","title":"Why this images"},{"location":"mkdocs-magicspace/#why-documentation-websites-with-mkdocs","text":"Why create separate documentation websites? And if so, why MkDocs? MkDocs website with beautiful themes looks much better than any readme file. The resulting documentation website looks professional and awesome. MkDocs adds text search to your documentation website. In the case of closed-source software, sharing readme files from the git repository with external users is not an option. Github does not render beautiful extended markdown features like admonitions, tabs, etc. Neither renders diagrams, formulas, swagger docs, or notebooks. Using MkDocs-MagicSpace you create documentation from the same markdown readme files you have in your repo together with the code. And you can create a unified documentation website from multiple repositories in Github, GitLab, Bitbucket. You can add such features as Google Analytics, multi-language localization.","title":"Why documentation websites with MkDocs"},{"location":"mkdocs-magicspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/mkdocs-magicspace and open localhost:8020 in browser.","title":"Start"},{"location":"mkdocs-magicspace/#features","text":"MkDocs: MkDocs - a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Material for MkDocs - gorgeous theme for MkDocs. PyMdown Extensions - add even more cool features of the extended markdown: sub- and superscripts, keys, magic links, sane headers etc. Mkdocs-macro plugin - add variables and macros written in Python! Mkdocs-multirepo-plugin - import docs directly from git repositories. Mkdocs-monorepo plugin - build multiple documentation folders in a single Mkdocs. Designed for large codebases. MkDocs Newsletter - show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. Mkdocs-mermaid2-plugin - renders textual graph descriptions into Mermaid graphs (flow charts, sequence diagrams, pie charts, etc.). Pygments - a generic syntax highlighter suitable for use in code hosting, forums, wikis or other applications that need to prettify source code, with over 500 languages and other text formats. Mkdocs-include-markdown-plugin - include Markdown files completely or partially, and include files of any type. Mkdocs-table-reader-plugin - directly insert CSV files as tables in your website. Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"mkdocs-magicspace/#docs","text":"See our guides on getting started , extended Markdown tutorials and advanced features .","title":"Docs"},{"location":"nodejs-workspace/","text":"Node.js workspace Docker image with Node.js and browser-based VS-Code version. Why this images If you need self-hosted development environment. If you want to be one terminal command away from coding in JavaScript. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/nodejs-workspace and open localhost:8020 in browser. Features Node Npm Yarn Nvm Nodeenv Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"nodejs-workspace/#nodejs-workspace","text":"Docker image with Node.js and browser-based VS-Code version.","title":"Node.js workspace"},{"location":"nodejs-workspace/#why-this-images","text":"If you need self-hosted development environment. If you want to be one terminal command away from coding in JavaScript.","title":"Why this images"},{"location":"nodejs-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/nodejs-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"nodejs-workspace/#features","text":"Node Npm Yarn Nvm Nodeenv Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"nodejs-workspace/tutorial/","text":"This tutorial shows how to install other versions of Node.js, install packages with Npm and Yarn, create simple web server Node.js Open workspace terminal and check Node.js or npm versions node -v npm -v To start Node.js REPL session simply execute node Nvm NOTE: nvm does not work from the Codeserver embedded terminal. Use another version of Node.js nvm install 16.0.0 nvm use 16.0.0 Npm Check npm version npm -v Install latest version of npm npm install -g npm@latest Yarn Yarn is a package manager for Node. js that focuses on speed, security, and consistency. It was originally created to address some issues with the popular NPM package manager. npm install --global yarn yarn --version Nodeenv Node.js virtual environment - a tool to create isolated node.js environments. It creates an environment that has its own installation directories, that doesn\u2019t share libraries with other node.js virtual environments. Create folder and vrtual ennvironment in it mkdir /home/project/venv-test cd /home/project/venv-test nodeenv --node=12.18.3 env && . env/bin/activate Check Nnode.js version node -v npm -v Simple example Clone example project cd /home/project git clone https://github.com/contentful/the-example-app.nodejs.git cd the-example-app.nodejs Install the dependencies npm install Start app export PORT=8040 npm run start:dev Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app","title":"Tutorial"},{"location":"nodejs-workspace/tutorial/#nodejs","text":"Open workspace terminal and check Node.js or npm versions node -v npm -v To start Node.js REPL session simply execute node","title":"Node.js"},{"location":"nodejs-workspace/tutorial/#nvm","text":"NOTE: nvm does not work from the Codeserver embedded terminal. Use another version of Node.js nvm install 16.0.0 nvm use 16.0.0","title":"Nvm"},{"location":"nodejs-workspace/tutorial/#npm","text":"Check npm version npm -v Install latest version of npm npm install -g npm@latest","title":"Npm"},{"location":"nodejs-workspace/tutorial/#yarn","text":"Yarn is a package manager for Node. js that focuses on speed, security, and consistency. It was originally created to address some issues with the popular NPM package manager. npm install --global yarn yarn --version","title":"Yarn"},{"location":"nodejs-workspace/tutorial/#nodeenv","text":"Node.js virtual environment - a tool to create isolated node.js environments. It creates an environment that has its own installation directories, that doesn\u2019t share libraries with other node.js virtual environments. Create folder and vrtual ennvironment in it mkdir /home/project/venv-test cd /home/project/venv-test nodeenv --node=12.18.3 env && . env/bin/activate Check Nnode.js version node -v npm -v","title":"Nodeenv"},{"location":"nodejs-workspace/tutorial/#simple-example","text":"Clone example project cd /home/project git clone https://github.com/contentful/the-example-app.nodejs.git cd the-example-app.nodejs Install the dependencies npm install Start app export PORT=8040 npm run start:dev Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app","title":"Simple example"},{"location":"notebook-workspace/","text":"Jupyter notebook workspace Opinionated Jupyter notebook & Jupyter Lab instsllation, together with the tolset which helps to get the most out of Jupyter notebooks. Explore and analyze data, build reports, presentations and documentations. Convert notebooks to various formats. Serve notebooks for reporting. Create data pipelines from notebooks. Schedule executions with UI and monitoring tool. Why this images You need a ready-to-go Jupyter environment with batteries included, such as pandas dataframes vizualisation and exploration, code autocompletion etc. And you don't want to install all the extensions and struggle with dependencies resolution. You need to schedule executions of your notebooks, whether it is on local laptop or on the server You need to make use of your notebooks - create slides, PDFs, HTML, reports, docs You want to turn notebooks into analytical reporting apps or interactive web applications You want to productionalize notebooks - create pipelines of batch jobs and schedule their executions There are short tutorials on how to use most of the mentioned features, with example notebooks. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/notebook-workspace and open localhost:8020 in browser. Features Notebooks Jupyter Notebook - Classic web-based notebook environment for interactive computing. JupyterLab - An extensible environment for interactive and reproducible computing, based on the Jupyter Notebook. VS-Code notebooks extension - Jupyter Notebooks in Visual Studio Code Notebook execution NBClient - Run Jupyter Notebooks in different execution contexts, including the command line. Nbterm - Lets you view, edit and execute Jupyter Notebooks in the terminal, and produce the new notebook as a result of execution. Papermill - A tool for parameterizing, executing, and analyzing Jupyter Notebooks. Convert Notebooks to other formats Jupytext - Can save Jupyter notebooks as python or markdown. Nbconvert - Tool that allows to convert an .ipynb notebook file into various static formats including executable scripts. nbless - Conversion of python notebooks into various commands, including .py executable scripts. Notebook as reports Voila - Voil\u00e0 turns Jupyter notebooks into standalone web applications. NBViewer - A simple way to view and share Jupyter Notebooks. Datapane -The easiest way to create data science reports from Python. Interactive web applications Mercury - Mercury is a perfect tool to convert any Jupyter notebook to interactive web app. Jupyter Dash - Develop Plotly Dash apps interactively from within Jupyter environments. Data visualisation and exploration D-Tale - is a comprehensive solution that allows to explore and analyze raw data files, as well as pandas dataframes. D-Tale is extremely powerful, is great toolset to visualise, explore and study the datasets. Lux - Recommends a set of visualizations highlighting interesting trends and patterns in the pandas data frame. Qgrid - Jupyter notebook widget which uses SlickGrid to render pandas data frames. Data Preview - VS-Code extension which provides tabular view for csv files and allows interactive exploration of data. Pipelines: scheduling and execution Ploomber - The fastest way to build data pipelines with Jupyter notebooks. Luigi - Build complex pipelines of batch jobs. Luigi handles dependency resolution, workflow management, visualization etc. Cronicle - Task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Documentation and slideshow Jupyter book - Create beautiful, publication-quality books and documents from computational content. MkDocs - Create awesome documentation for your project with only markdown. RISE - Allows you to instantly turn your Jupyter Notebooks into a slideshow. Jupyter extensions Jupyterlab-lsp - Coding assistance for JupyterLab (code navigation + hover suggestions + linters + autocompletion + rename. Jupyterlab-theme-toggle - Toggle light/dark themes in the JupyterLab Top Bar area. Python data and viz packages Pandas . Fast, powerful, flexible and easy to use open source data analysis and manipulation tool. Fugue SQL - SQL for Pandas. Numpy - Fundamental package for scientific computing with Python. SciPy - Software for mathematics, science, and engineering. Matplotlib - Comprehensive library for creating static, animated, and interactive visualizations in Python Seaborn - Data visualization library which provides a high-level interface for drawing attractive and informative statistical graphics. Plotline - An implementation of a grammar of graphics in Python, it is based on ggplot2. Plotly - Interactive, open-source plotting library that supports over 40 unique chart types. Altair - Altair is a declarative statistical visualization library for Python. Altair's API is simple, friendly and consistent. Python tools: IPython and Notebooks Pdoc3 Pytest-html-reporter SnakeViz Vprof Pyinstrument Flameprof Pylint-json2html Pre-commit Flake8 Poetry Black Dev tools: VS-Code - browser-based open source version of popular Visual Studio Code IDE. It has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo Demo: Notebook workspace","title":"About"},{"location":"notebook-workspace/#jupyter-notebook-workspace","text":"Opinionated Jupyter notebook & Jupyter Lab instsllation, together with the tolset which helps to get the most out of Jupyter notebooks. Explore and analyze data, build reports, presentations and documentations. Convert notebooks to various formats. Serve notebooks for reporting. Create data pipelines from notebooks. Schedule executions with UI and monitoring tool.","title":"Jupyter notebook workspace"},{"location":"notebook-workspace/#why-this-images","text":"You need a ready-to-go Jupyter environment with batteries included, such as pandas dataframes vizualisation and exploration, code autocompletion etc. And you don't want to install all the extensions and struggle with dependencies resolution. You need to schedule executions of your notebooks, whether it is on local laptop or on the server You need to make use of your notebooks - create slides, PDFs, HTML, reports, docs You want to turn notebooks into analytical reporting apps or interactive web applications You want to productionalize notebooks - create pipelines of batch jobs and schedule their executions There are short tutorials on how to use most of the mentioned features, with example notebooks.","title":"Why this images"},{"location":"notebook-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/notebook-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"notebook-workspace/#features","text":"Notebooks Jupyter Notebook - Classic web-based notebook environment for interactive computing. JupyterLab - An extensible environment for interactive and reproducible computing, based on the Jupyter Notebook. VS-Code notebooks extension - Jupyter Notebooks in Visual Studio Code Notebook execution NBClient - Run Jupyter Notebooks in different execution contexts, including the command line. Nbterm - Lets you view, edit and execute Jupyter Notebooks in the terminal, and produce the new notebook as a result of execution. Papermill - A tool for parameterizing, executing, and analyzing Jupyter Notebooks. Convert Notebooks to other formats Jupytext - Can save Jupyter notebooks as python or markdown. Nbconvert - Tool that allows to convert an .ipynb notebook file into various static formats including executable scripts. nbless - Conversion of python notebooks into various commands, including .py executable scripts. Notebook as reports Voila - Voil\u00e0 turns Jupyter notebooks into standalone web applications. NBViewer - A simple way to view and share Jupyter Notebooks. Datapane -The easiest way to create data science reports from Python. Interactive web applications Mercury - Mercury is a perfect tool to convert any Jupyter notebook to interactive web app. Jupyter Dash - Develop Plotly Dash apps interactively from within Jupyter environments. Data visualisation and exploration D-Tale - is a comprehensive solution that allows to explore and analyze raw data files, as well as pandas dataframes. D-Tale is extremely powerful, is great toolset to visualise, explore and study the datasets. Lux - Recommends a set of visualizations highlighting interesting trends and patterns in the pandas data frame. Qgrid - Jupyter notebook widget which uses SlickGrid to render pandas data frames. Data Preview - VS-Code extension which provides tabular view for csv files and allows interactive exploration of data. Pipelines: scheduling and execution Ploomber - The fastest way to build data pipelines with Jupyter notebooks. Luigi - Build complex pipelines of batch jobs. Luigi handles dependency resolution, workflow management, visualization etc. Cronicle - Task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Documentation and slideshow Jupyter book - Create beautiful, publication-quality books and documents from computational content. MkDocs - Create awesome documentation for your project with only markdown. RISE - Allows you to instantly turn your Jupyter Notebooks into a slideshow. Jupyter extensions Jupyterlab-lsp - Coding assistance for JupyterLab (code navigation + hover suggestions + linters + autocompletion + rename. Jupyterlab-theme-toggle - Toggle light/dark themes in the JupyterLab Top Bar area. Python data and viz packages Pandas . Fast, powerful, flexible and easy to use open source data analysis and manipulation tool. Fugue SQL - SQL for Pandas. Numpy - Fundamental package for scientific computing with Python. SciPy - Software for mathematics, science, and engineering. Matplotlib - Comprehensive library for creating static, animated, and interactive visualizations in Python Seaborn - Data visualization library which provides a high-level interface for drawing attractive and informative statistical graphics. Plotline - An implementation of a grammar of graphics in Python, it is based on ggplot2. Plotly - Interactive, open-source plotting library that supports over 40 unique chart types. Altair - Altair is a declarative statistical visualization library for Python. Altair's API is simple, friendly and consistent. Python tools: IPython and Notebooks Pdoc3 Pytest-html-reporter SnakeViz Vprof Pyinstrument Flameprof Pylint-json2html Pre-commit Flake8 Poetry Black Dev tools: VS-Code - browser-based open source version of popular Visual Studio Code IDE. It has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"notebook-workspace/#demo","text":"Demo: Notebook workspace","title":"Demo"},{"location":"notebook-workspace/data-exploration/","text":"Data Exploration To work with data sucessfully, it is important to be able to explore data, look on the raw table, plot charts and plot such as scatterplots, correlation tables, outlier heatmaps, etc. Workspace has tools to explore raw datasets. It also has configured extensions and widgets for Jupyter n otebooks and Jupyterlab for data exploration and discovery. To start with, copy tutorial notebooks into the project forlder. cp -r /home/examples/tutorials /home/project Jupyter Lab You can view data files, such as .csv in a nice tabular form directly in Jupyter lab. Go to Jupyter Lab from the Quickstart page, and open file 'cars.csv' in folder 'tutorials' from Jupyter Lab You can explore notebook 'cars-jupyter.ipynb' to see how to import and export data from .csv files. Data Preview Data Preview - is a VS-Code extension which provides tabular view for csv files and allows interactive exploration of such files. It is very convenient for initial exploration of the csv files before loading them to pandas dataframes. Lux By simply printing out a dataframe in a Jupyter notebook, Lux recommends a set of visualizations highlighting interesting trends and patterns in the dataset. Visualizations are displayed via an interactive widget that enables users to quickly browse through large collections of visualizations and make sense of their data. To try Lux open in Jupyter, Jupyterlab or VS-Code small example notebook tutorials/lux.ipynb . Follow the instruction and execute notes QGrid Qgrid is a Jupyter notebook widget which uses SlickGrid to render pandas DataFrames within a Jupyter notebook. This allows you to explore your DataFrames with intuitive scrolling, sorting, and filtering controls, as well as edit your DataFrames by double clicking cells. NOTE: Qgrid works only i Jupyter notebooks, and is not working with current version of Jupyterlab. Open in Jupyter notebook example notebook tutorials/qgrid.ipynb and run cells one-by-one QGrid is extremely convenient if you need to work with multiple data frames, or need to apply complex transformations to the same dataframe. You can vizualise every step of thransformation, look on the whole dataset, filter and order columns. D-Tale D-Tale - is a comprehensive solution that allows to explore and analyze raw data files, as well as pandas dataframes. D-Tale is extremely powerful, is great toolset to visualise, explore and study the datasets. Open D-Tale from the Quickstart page. Upload your data file, or use example dataset to start with Load the Movies Sample dataset. Dataset will be displayed as an iteractive table. All the cells of this table can be edited, and you can make direct changes to the values just like in excel. Whenever you click on the column header, you will get a list of options depending upon the type of data the column contains. Among other actions you can sort, order and filter table Highlighters are used to highlight some sections of the dataset. Like we use stylers in pandas to bring out the odd values, highlighters do the same job. You can highlight missing values, Data types, Outliers, and range. The example below shows how the missing values and outliers have been highlighted Among data exploration features you can detect outliers, find patterns and correlations, detect missinng values, generate comprehensive column reports With D-Tale you can perform certain data transformation steps, such as change types, clean data, remove duplicates, creation of new columns Select \"Dataframe Functions\" from the main D-Tale menu, in order to create new column D-Tale Charts D-Tale Charts is somewhat hidden among other D-Tale options, but it is an awesome feature for analytical exploration and visualisation of the dataset or pandas dataframe. You can build complex charts interactively. This can be used for reporting or visual patterns and trends discovery To open D-Tale Charts, select \"Charts\" from the main D-Tale menu Pandas data frames in D-Tale D-Tale can visualize pandas dataframes too. Open tutorial notebook tutorials/d-tale.ipynb , follow the instructions and execute cells. After you execute cell which containe the line dtale.show(df, host=\"0.0.0.0\", port=8038, force=True) # host and port are important! open Quickstart page, go to \"My Apps\" and open app on port 8038. You will see the D-Tale, loaded with your pandas dataframe","title":"Data exploration"},{"location":"notebook-workspace/data-exploration/#data-exploration","text":"To work with data sucessfully, it is important to be able to explore data, look on the raw table, plot charts and plot such as scatterplots, correlation tables, outlier heatmaps, etc. Workspace has tools to explore raw datasets. It also has configured extensions and widgets for Jupyter n otebooks and Jupyterlab for data exploration and discovery. To start with, copy tutorial notebooks into the project forlder. cp -r /home/examples/tutorials /home/project","title":"Data Exploration"},{"location":"notebook-workspace/data-exploration/#jupyter-lab","text":"You can view data files, such as .csv in a nice tabular form directly in Jupyter lab. Go to Jupyter Lab from the Quickstart page, and open file 'cars.csv' in folder 'tutorials' from Jupyter Lab You can explore notebook 'cars-jupyter.ipynb' to see how to import and export data from .csv files.","title":"Jupyter Lab"},{"location":"notebook-workspace/data-exploration/#data-preview","text":"Data Preview - is a VS-Code extension which provides tabular view for csv files and allows interactive exploration of such files. It is very convenient for initial exploration of the csv files before loading them to pandas dataframes.","title":"Data Preview"},{"location":"notebook-workspace/data-exploration/#lux","text":"By simply printing out a dataframe in a Jupyter notebook, Lux recommends a set of visualizations highlighting interesting trends and patterns in the dataset. Visualizations are displayed via an interactive widget that enables users to quickly browse through large collections of visualizations and make sense of their data. To try Lux open in Jupyter, Jupyterlab or VS-Code small example notebook tutorials/lux.ipynb . Follow the instruction and execute notes","title":"Lux"},{"location":"notebook-workspace/data-exploration/#qgrid","text":"Qgrid is a Jupyter notebook widget which uses SlickGrid to render pandas DataFrames within a Jupyter notebook. This allows you to explore your DataFrames with intuitive scrolling, sorting, and filtering controls, as well as edit your DataFrames by double clicking cells. NOTE: Qgrid works only i Jupyter notebooks, and is not working with current version of Jupyterlab. Open in Jupyter notebook example notebook tutorials/qgrid.ipynb and run cells one-by-one QGrid is extremely convenient if you need to work with multiple data frames, or need to apply complex transformations to the same dataframe. You can vizualise every step of thransformation, look on the whole dataset, filter and order columns.","title":"QGrid"},{"location":"notebook-workspace/data-exploration/#d-tale","text":"D-Tale - is a comprehensive solution that allows to explore and analyze raw data files, as well as pandas dataframes. D-Tale is extremely powerful, is great toolset to visualise, explore and study the datasets. Open D-Tale from the Quickstart page. Upload your data file, or use example dataset to start with Load the Movies Sample dataset. Dataset will be displayed as an iteractive table. All the cells of this table can be edited, and you can make direct changes to the values just like in excel. Whenever you click on the column header, you will get a list of options depending upon the type of data the column contains. Among other actions you can sort, order and filter table Highlighters are used to highlight some sections of the dataset. Like we use stylers in pandas to bring out the odd values, highlighters do the same job. You can highlight missing values, Data types, Outliers, and range. The example below shows how the missing values and outliers have been highlighted Among data exploration features you can detect outliers, find patterns and correlations, detect missinng values, generate comprehensive column reports With D-Tale you can perform certain data transformation steps, such as change types, clean data, remove duplicates, creation of new columns Select \"Dataframe Functions\" from the main D-Tale menu, in order to create new column","title":"D-Tale"},{"location":"notebook-workspace/data-exploration/#d-tale-charts","text":"D-Tale Charts is somewhat hidden among other D-Tale options, but it is an awesome feature for analytical exploration and visualisation of the dataset or pandas dataframe. You can build complex charts interactively. This can be used for reporting or visual patterns and trends discovery To open D-Tale Charts, select \"Charts\" from the main D-Tale menu","title":"D-Tale Charts"},{"location":"notebook-workspace/data-exploration/#pandas-data-frames-in-d-tale","text":"D-Tale can visualize pandas dataframes too. Open tutorial notebook tutorials/d-tale.ipynb , follow the instructions and execute cells. After you execute cell which containe the line dtale.show(df, host=\"0.0.0.0\", port=8038, force=True) # host and port are important! open Quickstart page, go to \"My Apps\" and open app on port 8038. You will see the D-Tale, loaded with your pandas dataframe","title":"Pandas data frames in D-Tale"},{"location":"notebook-workspace/documentation/","text":"Documentation an materials With the help of the Workspace you can create awesome static documentation websites and pdf documents from Markdown and Jupyter notebooks. Below is an example of MkDocs scientific document created from the notebooks Mkdocs With MkDocs you can use Jupyter Notebooks in MkDocs. This means you can create MkDocs pages not only from Markdown files, but also from Jupyter notes, where you can mix Markdown, code, outputs and visualisations such as maps, charts and plots. Workspace contains an example MkDocs project to demonstrate this functionality. Although, it is not necessary, separate conda environment will be used to isolate MkDocs project dependencies. To start, cd to the example project folder cd /home/examples/mkdocs Create new conda environment conda create --name mkdocs python=3.8 Activate conda environment conda activate mkdocs Install dependencies conda install pip python -m pip install -r requirements.txt Start mkdocs dev server conda activate mkdocs python -m mkdocs serve -a 0.0.0.0:8040 MkDocs dev server is being served on port 8040, and we have shortcut on the Quickstart page to open it. There are 3 pages, one demonnstrates the extended Markdown features, two other - are Jupyter notebooks as MkDocs pages. This running app - is a development server with live updates, all changes to the notebooks or .md files will be applied without the need to restart. NOTE: You don't need to execute notebook when rendering with MkDocs. Notebook that was run on any other environment, if it contains all required output, can be included to MkDocs. This example bolierplate MkDocs project lots lots of other extra-Markdown features. When dependencies are installed, it has the features of the MkDocs MagicSpace. Check out **documentation and tutorials Jupyter book You can also create awesome static websites with Jupyter Book - an open source project for building beautiful, publication-quality books and documents from computational material. Here is a basic hands-on example of how to get started with the Jupyter Book. Clone the repository containing the demo book source files cd /home/project git clone https://github.com/executablebooks/quantecon-mini-example cd quantecon-mini-example Create conda environment & install libraries needed to run the code in this particular example. This includes the latest version of Jupyter Book: conda env create -f environment.yml conda activate qe-mini-example Build the Jupyter Book jupyter-book build ./mini_book Move the resulting static website into the folder that you can view with Static Server, and delete the root index.html that prevents Static Server to render site properly mv mini_book/_build/html /home/static-server rm /home/static-server/html/index.html From the Quickstart page open Static Server, and go to html/docs folder","title":"Documentation"},{"location":"notebook-workspace/documentation/#documentation-an-materials","text":"With the help of the Workspace you can create awesome static documentation websites and pdf documents from Markdown and Jupyter notebooks. Below is an example of MkDocs scientific document created from the notebooks","title":"Documentation an materials"},{"location":"notebook-workspace/documentation/#mkdocs","text":"With MkDocs you can use Jupyter Notebooks in MkDocs. This means you can create MkDocs pages not only from Markdown files, but also from Jupyter notes, where you can mix Markdown, code, outputs and visualisations such as maps, charts and plots. Workspace contains an example MkDocs project to demonstrate this functionality. Although, it is not necessary, separate conda environment will be used to isolate MkDocs project dependencies. To start, cd to the example project folder cd /home/examples/mkdocs Create new conda environment conda create --name mkdocs python=3.8 Activate conda environment conda activate mkdocs Install dependencies conda install pip python -m pip install -r requirements.txt Start mkdocs dev server conda activate mkdocs python -m mkdocs serve -a 0.0.0.0:8040 MkDocs dev server is being served on port 8040, and we have shortcut on the Quickstart page to open it. There are 3 pages, one demonnstrates the extended Markdown features, two other - are Jupyter notebooks as MkDocs pages. This running app - is a development server with live updates, all changes to the notebooks or .md files will be applied without the need to restart. NOTE: You don't need to execute notebook when rendering with MkDocs. Notebook that was run on any other environment, if it contains all required output, can be included to MkDocs. This example bolierplate MkDocs project lots lots of other extra-Markdown features. When dependencies are installed, it has the features of the MkDocs MagicSpace. Check out **documentation and tutorials","title":"Mkdocs"},{"location":"notebook-workspace/documentation/#jupyter-book","text":"You can also create awesome static websites with Jupyter Book - an open source project for building beautiful, publication-quality books and documents from computational material. Here is a basic hands-on example of how to get started with the Jupyter Book. Clone the repository containing the demo book source files cd /home/project git clone https://github.com/executablebooks/quantecon-mini-example cd quantecon-mini-example Create conda environment & install libraries needed to run the code in this particular example. This includes the latest version of Jupyter Book: conda env create -f environment.yml conda activate qe-mini-example Build the Jupyter Book jupyter-book build ./mini_book Move the resulting static website into the folder that you can view with Static Server, and delete the root index.html that prevents Static Server to render site properly mv mini_book/_build/html /home/static-server rm /home/static-server/html/index.html From the Quickstart page open Static Server, and go to html/docs folder","title":"Jupyter book"},{"location":"notebook-workspace/interactive/","text":"Interactive web applications Jypyter notebooks can be converted to interactive web applications, where users can interactively chose parameters that would become inputs to the notebooks, forcing notebooks to re-run entirely or partially. Mercury Mercury is a perfect tool to convert Python notebook to interactive web app, slides, dashboard, report, REST API and share with non-programmers. Compared to Voila (which runs notebook only once at start) Mercury allows multiple parameterized executions. Notebook has a set of parameters which can be varied by the end user, triggering re-running of the notebook. It is very easy to turn any notebook into an interactive web application. It does not require serious changes to the notebook to become mercury app. You would only need to add a raw cell with the params yaml that would define input widgets for your variables. Definition of the widget (in params) starts with the widget name. It should correspond to the variable in the code (in the notebook). Check Mercury documentation for more details. Mercury is not started by default. If you want to use it, start it from the terminal. Serve example folder with Mercury cd /home/examples/mercury; mercury run 0.0.0.0:8035 Open Quickstart page, go to \"Results\" tab and open Mercury. You can explore different Mercury examples . To try them on your own, downnload examples to the workspace cd /home/project git clone https://github.com/pplonski/mercury-demo-notebooks Serve these examples with Mercury cd mercury-demo-notebooks mercury run 0.0.0.0:8035 Open any of the example notebooks in the JupytterLab to see how Mercury yaml definitions turn them into the interactive web applications. Running Mercury permanently If you want to run Mercury permanently, add &! to the end of the mercury run command. For example, to serve cloned example mercury folder, execute cd mercury-demo-notebooks; mercury run 0.0.0.0:8035 &! Jupyter-Dash Dash is the original low-code framework for rapidly building data apps in Python. Workspace has Dash installed. You can try running Dash hello-world example from the /home/project/tutorials folder. cd /home/project/tutorials python dash-helloworld.py If you look in the .py file, you would see that Dash application runs on host \"0.0.0.0\" (!important) and port 8038. Now you can go to the workspace Quickstart page, navigate to \"My apps\" tab, and open Application on port 8038 Jupyter-Dash library makes it easy to develop Plotly Dash apps interactively from within Jupyter environments. Workspace includes notebook with an example Dash application that you can start directly from the notebook. Open JypyterLab, go to the folder 'tutorials' and open notebook 'jupyterdash.ipynb' Execute cells to run the dash application. You need to run a cell app.run_server(host=\"0.0.0.0\",port=8040) to start Dash application Now you only need to open the Application on port 8040 from the Quickstart page (\"My apps\" tab). Even better is to create a .py notebook with Jupytext. Then you can develop Dash applications with JupyterLab, and run them as a python applications at the same time! Workspace includes such an example - file \"jupyterdash.py\" in the \"tutorials\" folder. You can open this python script with JupyterLab, and develop it like a notebook. And at any moment you can also run it in the terminal cd /home/project/tutorials python jupyterdash.py","title":"Interactive web app"},{"location":"notebook-workspace/interactive/#interactive-web-applications","text":"Jypyter notebooks can be converted to interactive web applications, where users can interactively chose parameters that would become inputs to the notebooks, forcing notebooks to re-run entirely or partially.","title":"Interactive web applications"},{"location":"notebook-workspace/interactive/#mercury","text":"Mercury is a perfect tool to convert Python notebook to interactive web app, slides, dashboard, report, REST API and share with non-programmers. Compared to Voila (which runs notebook only once at start) Mercury allows multiple parameterized executions. Notebook has a set of parameters which can be varied by the end user, triggering re-running of the notebook. It is very easy to turn any notebook into an interactive web application. It does not require serious changes to the notebook to become mercury app. You would only need to add a raw cell with the params yaml that would define input widgets for your variables. Definition of the widget (in params) starts with the widget name. It should correspond to the variable in the code (in the notebook). Check Mercury documentation for more details. Mercury is not started by default. If you want to use it, start it from the terminal. Serve example folder with Mercury cd /home/examples/mercury; mercury run 0.0.0.0:8035 Open Quickstart page, go to \"Results\" tab and open Mercury. You can explore different Mercury examples . To try them on your own, downnload examples to the workspace cd /home/project git clone https://github.com/pplonski/mercury-demo-notebooks Serve these examples with Mercury cd mercury-demo-notebooks mercury run 0.0.0.0:8035 Open any of the example notebooks in the JupytterLab to see how Mercury yaml definitions turn them into the interactive web applications.","title":"Mercury"},{"location":"notebook-workspace/interactive/#running-mercury-permanently","text":"If you want to run Mercury permanently, add &! to the end of the mercury run command. For example, to serve cloned example mercury folder, execute cd mercury-demo-notebooks; mercury run 0.0.0.0:8035 &!","title":"Running Mercury permanently"},{"location":"notebook-workspace/interactive/#jupyter-dash","text":"Dash is the original low-code framework for rapidly building data apps in Python. Workspace has Dash installed. You can try running Dash hello-world example from the /home/project/tutorials folder. cd /home/project/tutorials python dash-helloworld.py If you look in the .py file, you would see that Dash application runs on host \"0.0.0.0\" (!important) and port 8038. Now you can go to the workspace Quickstart page, navigate to \"My apps\" tab, and open Application on port 8038 Jupyter-Dash library makes it easy to develop Plotly Dash apps interactively from within Jupyter environments. Workspace includes notebook with an example Dash application that you can start directly from the notebook. Open JypyterLab, go to the folder 'tutorials' and open notebook 'jupyterdash.ipynb' Execute cells to run the dash application. You need to run a cell app.run_server(host=\"0.0.0.0\",port=8040) to start Dash application Now you only need to open the Application on port 8040 from the Quickstart page (\"My apps\" tab). Even better is to create a .py notebook with Jupytext. Then you can develop Dash applications with JupyterLab, and run them as a python applications at the same time! Workspace includes such an example - file \"jupyterdash.py\" in the \"tutorials\" folder. You can open this python script with JupyterLab, and develop it like a notebook. And at any moment you can also run it in the terminal cd /home/project/tutorials python jupyterdash.py","title":"Jupyter-Dash"},{"location":"notebook-workspace/pipelines/","text":"This basic example shows how to create pipelines in the workspace with Luigi or Ploomber. Luigi Luigi is a Python package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more. Luigi workflows consists more or less of tasks and targets. Targets . Broadly speaking, the Target class corresponds to a file on a disk. Or a file on S3. Or some kind of a checkpoint, like an entry in a database. You might use the LocalTarget, S3Target, or FTPTarget classes that are available out of the box. These directly map to a file on the local drive, on S3, or in FTP, respectively. Tasks are defined as python classes that subclass the luigi.Task superclass. Each task has methods that the workflow designer is supposed to implement: requires(): should return one or more instantiated tasks that the current task depends on. You can have as many upstream dependencies as you like, or you can indicate zero dependencies by returning an empty array (return []), which you'll often do for the Task that kicks off a pipeline. Returning None also works. output(): return one or more targets objects, typically representing files, the the current task will produce when run. When you run a pipeline, Luigi first checks whether its output Targets already exist, and if not, schedules the Task to run. Otherwise, Luigi assumes that the Task is complete, and does not rerun it. run(): Here goes all the code that the task should run as its job. Example 1 Workspace includes couple of Luigi examples in the folder /home/exemples/luigi . From the Quickstart page go to IDE and explore file helloworld.py in the VS-Code editor. You can see the most basic Luigi workflow, which consists of just one task that creates textual file with one line - \"Hello World!\". You can see in this example one task - class 'HelloWorld' which extends 'luigi.Task'. This class implements alle the abovementioned methods: requires(), output() and run(). This task has no depenndencies, thus requires() returns None. Output uses 'LocalTarget', which is a .txt file. The run() method outputs to the Target. You can execute this Luigi workflow, open terminal, cd to the folder and run cd /home/examples/luigi python helloworld.py --local-scheduler HelloWorld You can see that file 'helloworld.txt' has appeared in the same folder as expected. Example 2 The first example was executed with the '--local-scheduler' flag. While the --local-scheduler flag is useful for development purposes, it\u2019s not recommended for production usage. The centralized scheduler serves two purposes: Make sure two instances of the same task are not running simultaneously Provide visualization of everything that\u2019s going on. Workspace has centralized Luigi Scheduler up and running, any Luigi workflow executed without the'--local-scheduler' flag, will be tracked by the Luigi scheduler. For example, cd /home/examples/luigi python luigitutorial-2.py NameSubstituter --name Alfred Go to the Quickstart page, and open Luigi UI. You can see that the workflow execution was captured, you can explore tasks and history of executions. Useful links: Luigi Github Repo Luigi on PyPi A Tutorial on Luigi, the Spotify\u2019s Pipeline Create your first ETL in Luigi Building Data Science Pipelines with Luigi and Jupyter Notebooks Ploomber Ploomber is the fastest way to build data pipelines with Jupyter notebooks. Ploomber helps you build modular pipelines. A pipeline (or DAG) is a group of tasks with a particular execution order, where subsequent (or downstream tasks) use previous (or upstream) tasks as inputs. Create file pipeline.yaml with the following content To demonstrate Ploomber, chose and download a Ploomber examples # list examples ploomber examples # choose an example, and download with name ploomber examples --name {name} # download the following example ploomber examples --name guides/first-pipeline This pipeline is an introductory tutorial to learn the basics of Ploomber. Pipeline contains five tasks, 1-get.py, 2-profile-raw.py, 3-clean.py, 4-profile-clean.py and 5-plot.py. The pipeline.yaml file defines the execution order Read more abou this pipeline in the pipeline documentation Explore the tasks with the Jupyter Lab. Notice, that tasks are .py files, but they are not mere scripts - they are jupytext notebooks. Open them as notebooks In order to execute, cd to the downloaded pipeline folder, and run cd guides/first-pipeline/ ploomber build The execution produces an output in the ./output folder. Copy it to the folder served by a static file serer. cp -r output /home/static-server Now you can open Static File Server from the Quickstart page, and explore the output.","title":"Pipelines"},{"location":"notebook-workspace/pipelines/#luigi","text":"Luigi is a Python package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more. Luigi workflows consists more or less of tasks and targets. Targets . Broadly speaking, the Target class corresponds to a file on a disk. Or a file on S3. Or some kind of a checkpoint, like an entry in a database. You might use the LocalTarget, S3Target, or FTPTarget classes that are available out of the box. These directly map to a file on the local drive, on S3, or in FTP, respectively. Tasks are defined as python classes that subclass the luigi.Task superclass. Each task has methods that the workflow designer is supposed to implement: requires(): should return one or more instantiated tasks that the current task depends on. You can have as many upstream dependencies as you like, or you can indicate zero dependencies by returning an empty array (return []), which you'll often do for the Task that kicks off a pipeline. Returning None also works. output(): return one or more targets objects, typically representing files, the the current task will produce when run. When you run a pipeline, Luigi first checks whether its output Targets already exist, and if not, schedules the Task to run. Otherwise, Luigi assumes that the Task is complete, and does not rerun it. run(): Here goes all the code that the task should run as its job. Example 1 Workspace includes couple of Luigi examples in the folder /home/exemples/luigi . From the Quickstart page go to IDE and explore file helloworld.py in the VS-Code editor. You can see the most basic Luigi workflow, which consists of just one task that creates textual file with one line - \"Hello World!\". You can see in this example one task - class 'HelloWorld' which extends 'luigi.Task'. This class implements alle the abovementioned methods: requires(), output() and run(). This task has no depenndencies, thus requires() returns None. Output uses 'LocalTarget', which is a .txt file. The run() method outputs to the Target. You can execute this Luigi workflow, open terminal, cd to the folder and run cd /home/examples/luigi python helloworld.py --local-scheduler HelloWorld You can see that file 'helloworld.txt' has appeared in the same folder as expected. Example 2 The first example was executed with the '--local-scheduler' flag. While the --local-scheduler flag is useful for development purposes, it\u2019s not recommended for production usage. The centralized scheduler serves two purposes: Make sure two instances of the same task are not running simultaneously Provide visualization of everything that\u2019s going on. Workspace has centralized Luigi Scheduler up and running, any Luigi workflow executed without the'--local-scheduler' flag, will be tracked by the Luigi scheduler. For example, cd /home/examples/luigi python luigitutorial-2.py NameSubstituter --name Alfred Go to the Quickstart page, and open Luigi UI. You can see that the workflow execution was captured, you can explore tasks and history of executions.","title":"Luigi"},{"location":"notebook-workspace/pipelines/#useful-links","text":"Luigi Github Repo Luigi on PyPi A Tutorial on Luigi, the Spotify\u2019s Pipeline Create your first ETL in Luigi Building Data Science Pipelines with Luigi and Jupyter Notebooks","title":"Useful links:"},{"location":"notebook-workspace/pipelines/#ploomber","text":"Ploomber is the fastest way to build data pipelines with Jupyter notebooks. Ploomber helps you build modular pipelines. A pipeline (or DAG) is a group of tasks with a particular execution order, where subsequent (or downstream tasks) use previous (or upstream) tasks as inputs. Create file pipeline.yaml with the following content To demonstrate Ploomber, chose and download a Ploomber examples # list examples ploomber examples # choose an example, and download with name ploomber examples --name {name} # download the following example ploomber examples --name guides/first-pipeline This pipeline is an introductory tutorial to learn the basics of Ploomber. Pipeline contains five tasks, 1-get.py, 2-profile-raw.py, 3-clean.py, 4-profile-clean.py and 5-plot.py. The pipeline.yaml file defines the execution order Read more abou this pipeline in the pipeline documentation Explore the tasks with the Jupyter Lab. Notice, that tasks are .py files, but they are not mere scripts - they are jupytext notebooks. Open them as notebooks In order to execute, cd to the downloaded pipeline folder, and run cd guides/first-pipeline/ ploomber build The execution produces an output in the ./output folder. Copy it to the folder served by a static file serer. cp -r output /home/static-server Now you can open Static File Server from the Quickstart page, and explore the output.","title":"Ploomber"},{"location":"notebook-workspace/presentations/","text":"Presentations from Jupyter notebooks Jupyter notebooks is an awesome environment for creating slides and presentations. This is especially useful, if you need to make a presentation as a result of the analysis and experiments done in the notebook itself. First let's copy an example notebook to the project forlder. Open Quickstart, and then Terminal, execute cp /home/examples/jupyter/magpye.ipynb /home/project/ Now open Jupyter notebook, and open the notebook we have just copied. First thing to do - is to enable cell Slideshow toolbar. Click 'View' -> 'Cell toolbar' -> 'Slideshow' Assign different types of slide to notebook cells: Slide: Presents each cell in a different slide. Good for containing heading, sub-heading, image and text notes. Use right arrow to navigate. Sub-slide : A continuation from the previous slide. Use down arrow to go to a sub-slide from a slide. Fragment : Fragment is interesting because it presents multiple cells in one slide one by one. Skip/Notes : Skip skips the marked cells. Cells that are unused for the presentation can also be marked as the notes. Click on the toolbar pannel 'enter RISE slideshow' Navigate through the presentation nbless If you don't want to bother making slides yourself, nbless can do it for you, creating reasonable slides from your notebook. The nbless Python package includes nbdeck shell command which prepares a notebook to be viewed as or converted into slides. Try it out with the example notebook nbdeck /home/examples/jupyter/magpye.ipynb -o /home/project/nbless-slides.ipynb Open Jupyter notebook, go to notebook 'nbless-slides.ipynb' and click on the toolbar pannel 'enter RISE slideshow'. Now you can convert notebook into HTML presentation with the nbconv shell command. nbconv /home/project/nbless-slides.ipynb -e slides -o /home/static-server/nbless-slides.html Folder '/home/static-server' can be viewed with the Static File Server. Open Static File Server from the Quickstart page, and view the generated HTML slides.","title":"Presentations"},{"location":"notebook-workspace/presentations/#presentations-from-jupyter-notebooks","text":"Jupyter notebooks is an awesome environment for creating slides and presentations. This is especially useful, if you need to make a presentation as a result of the analysis and experiments done in the notebook itself. First let's copy an example notebook to the project forlder. Open Quickstart, and then Terminal, execute cp /home/examples/jupyter/magpye.ipynb /home/project/ Now open Jupyter notebook, and open the notebook we have just copied. First thing to do - is to enable cell Slideshow toolbar. Click 'View' -> 'Cell toolbar' -> 'Slideshow' Assign different types of slide to notebook cells: Slide: Presents each cell in a different slide. Good for containing heading, sub-heading, image and text notes. Use right arrow to navigate. Sub-slide : A continuation from the previous slide. Use down arrow to go to a sub-slide from a slide. Fragment : Fragment is interesting because it presents multiple cells in one slide one by one. Skip/Notes : Skip skips the marked cells. Cells that are unused for the presentation can also be marked as the notes. Click on the toolbar pannel 'enter RISE slideshow' Navigate through the presentation","title":"Presentations from Jupyter notebooks"},{"location":"notebook-workspace/presentations/#nbless","text":"If you don't want to bother making slides yourself, nbless can do it for you, creating reasonable slides from your notebook. The nbless Python package includes nbdeck shell command which prepares a notebook to be viewed as or converted into slides. Try it out with the example notebook nbdeck /home/examples/jupyter/magpye.ipynb -o /home/project/nbless-slides.ipynb Open Jupyter notebook, go to notebook 'nbless-slides.ipynb' and click on the toolbar pannel 'enter RISE slideshow'. Now you can convert notebook into HTML presentation with the nbconv shell command. nbconv /home/project/nbless-slides.ipynb -e slides -o /home/static-server/nbless-slides.html Folder '/home/static-server' can be viewed with the Static File Server. Open Static File Server from the Quickstart page, and view the generated HTML slides.","title":"nbless"},{"location":"notebook-workspace/productionalize/","text":"Productionizing notebooks Notebooks are usually used for exploration, analysis, experiments and visualisations. Notebook that were created as a result of exploration, experimentation and as POC can be \"productionalized\". \"Productionalizing\" notebooks means tutning them into executable notebooks which can be executed automatically without manual triggering, run on schedule and produce arftifacts, such as reports, data, machine learning models, etc. There are several alternative ways to productionalize notebooks: manual conversion: create executable python scripts by copying code from notebooks into python files by hand parameterize and execute notebooks without conversion programmatic conversion of notebooks to python scripts, and schedule executions In addition you might want to schedule automatic executions of notebooks or converted notebooks. Manual conversion This is the most widely used approach. The code is copied from the notebooks into the new script or application, providing great flexibility. The structure of the resulting application might differ significantly from the notebook, which allows better structuring of the code, applying abstractions, such as classes (which are raraly used in the notebooks), creation of unit and integration tests, etc. This approach suits better when the resulting application is complex, consists of many classes, files and modules. But when the resulting application or script does not differ much from the initial notebook, this approach creates extra overhead, requires more timeand slows down the iteration of experimentation-development-productionalization cycles. Notebooks without conversion In many situations it is enough to simply execute the notebook. It might be enough if the notebook is straightforward and result is generated dataset or report, transformation of data in the database, retrained machine learning model. The typical use-case is for example, scheduling of data extraction from the database and providing the result as csv file for end-users, scheduling of SQL stored procedures in the database, web-scrapping tasks, fetching Google Analytics statistics, fetching cryptocurrency daily prices, etc. Execution of notebooks without any transformation removes extra overhead, allows move faster from the experiment to the end result. Auto conversion NBClient NBClient is a tool for running Jupyter Notebooks in different execution contexts, including the command line. NBClient makes it possible to run command jupyter execute which runs Notebook cells one-by-one. For example, to execute notebook 'basic-notebook.ipynb' in folder /home/examples/notebooks' you would execute command jupyter execute /home/examples/notebooks/basic-notebook.ipynb This will execute each note in the notebook. NBClient might handy, if you need to automate such tasks as web scrapping, data analysis or preparation, retraining of machine learning models. Nbterm Nbterm lets you view, edit and execute Jupyter Notebooks in the terminal, and produce the new notebook as a result of execution. One of the differences between NBClient and Nbterm, is that the latter produces new notebook as a result. This is especially useful if you plan to use notebooks as reports. An example use-case would be querying datasase, computing statistics from new orders or client activity, and providing read-only executed notebook via NBviewer. Such notebook will contain both code, SQL as well as results of execution - statistics, chartss and plots. For example, to execute notebook 'basic-notebook.ipynb' in folder /home/examples/notebooks' you would execute command cd /home/examples/notebooks; nbterm --run basic-notebook.ipynb --save-path /home/project/voila/basic-notebook.ipynb Because flag --save-path was defined, executed notebook has been saved to 'basic-notebook_run.ipynb' in folder '/home/project/voila/'. Workspace serves notebooks in this folder with Voila. From the Quickstart page open Voila, and view the notebook in browser. Papermill Papermill is a tool for parameterizing, executing, and analyzing Jupyter Notebooks. Papermill solves the problem of parameterizing notebooks in a simple and efficient way. Jupyter notebooks allow users to tag cells. Papermill lets you to tag a cell, which contains parameters with parameters tag. To parameterize a notebook, designate a cell with the tag parameters. As an example, open Jupyter notebook from the Quickstart page, and create a new note. First thing to do - is to enable tags. Click View -> Cell Toolbar -> Tags . This will enable the tags UI. Create first cell and add tag 'parameters' - simply a variable with default value SOMETEXT=\"HELLO WORLD!\" ADDITIONAL=\"HELLO WORLD!\" Create another cell where variables are used file = \"/home/project/papermill-test.txt\" with open(file, 'a+') as f: f.write(SOMETEXT + \" - \" + ADDITIONAL + \"\\n\") print(SOMETEXT + \" - \" + ADDITIONAL + \"\\n\") Give your notebook a name, for example 'HelloWorld' and save. Open terminal and execute with different parameters papermill /home/project/HelloWorld.ipynb /home/project/nbviewer/HelloWorld.ipynb -p SOMETEXT \"My name\" -p ADDITIONAL \"Slim Shady\" The result notebook will be produced to the folder served by Nbviewer. From the Quickstart page open Nbviewer, and view the notebook in browser. This notebook also creates a file /home/project/papermill-test.txt . Read in this article which explains how to use Jupyter Notebooks (using Jupytext and Papermill) to automatically generate reports. You can schedule generation of automatic reports, which will output to folder served by Nbviewer or Voila. Programmatic conversion Nbconvert Nbconvert tool allows you to convert an .ipynb notebook file into various static formats including executable scripts. For example, if you want to convert Jupyter notebook /home/examples/jupyter/strings.ipynb to a Python script, you would execute jupyter nbconvert --to script /home/examples/jupyter/strings.ipynb --output /home/project/strings Now you can execute this newly-generated python script python /home/project/strings.py Nbconvert can convert to other formats too, such as HTML, PDF, LaTeX. Check tutorial about reports. nbless The nbless Python package includes nbconv shell command, which makes conversion of python notebooks into various commands, including .py executable scripts. To try it with the example notebook, execute in terminal nbconv /home/examples/jupyter/strings.ipynb --exporter python --out_file /home/project/nbless-strings.py You will see that file 'nbless-strings.py' has appeared in the '/home/project' folder, Execute it python /home/project/nbless-strings.py Jupytext Jupytext can save Jupyter notebooks as python or markdown. Try converting example notebooks to python script jupytext --to py /home/examples/jupyter/strings.ipynb --output /home/project/jupytext-strings.py In addition, Jupytext has a plugin for Jupyter that can save Jupyter notebooks as either Markdown files or scripts. Scripts created with Jupytext can be edited and modified just like standard notebooks. Moreover, Jupytext allows to create notebooks paired with the scripts, with any changes made to the notebook automatically applied to the scripts. To get a feeling how this works, copy example notebooks to the project folder cp -r /home/examples/jupyter /home/project From the Quickstart page open Jupyter notebook, and then open note 'strings.ipynb' in the copyed folder. In Jupyter Notebook, pair your notebook to the script format with the Jupytext menu If you refresh the tree folder, you will see that file strings.py appeared You can open this file in VS-Code and see that this is indeed a python file with some meta information on the top, as a comment. You can execute this file from the terminal python /home/project/jupyter/strings.py Awesome! Now if you change the original notebook, you will find out that this script is automatically changed too. Moreover, you can open this strings.py file in Jupyter as a notebook! You can modify and execute cells. As soon as you save changes, refresh the original .ipnb notebook, and it will be automatically get your latest updates. You can also work with this script as notebook in the Jupyter Lab! You just need to explicitly open this file as notebook. Scheduling automatic executions You can schedule automatic executions of your notebooks or converted notebooks. If workspace is in cloud, it becomes a simple reporting solution. The resulted executions can generate data, reports or other artifacts that can be downloaded with Filebrowser or viewed with Voila or NBviewer. If you run workspace on your local machine, make sure you enable \"Catch Up\" for scheduled events. This will automatically run all missed events after you turn on your laptop. Hence, you can have generated reports every day without the need to manually trigger every notebook. Cronicle With Cronicle we can easily schedule executions of shell commands and scripts. Cronicle has WEB UI, monitor of executions, collect logs and advanced features such as limiting resources for each execution. From the Quickstart page open Cronicle, (Cronicle user/pass: admin/admin), go to 'Schedule' and add new Event. Chose category 'General' and plugin 'Shell script'. Now we can configure the schedule, and provide any of the command to be executed. To execute notebook, parameterized with Papermil daily, simply paste the command to the 'Script Source' box, and configure time when you want Jupyter notebook to run","title":"Productionalize notebooks"},{"location":"notebook-workspace/productionalize/#productionizing-notebooks","text":"Notebooks are usually used for exploration, analysis, experiments and visualisations. Notebook that were created as a result of exploration, experimentation and as POC can be \"productionalized\". \"Productionalizing\" notebooks means tutning them into executable notebooks which can be executed automatically without manual triggering, run on schedule and produce arftifacts, such as reports, data, machine learning models, etc. There are several alternative ways to productionalize notebooks: manual conversion: create executable python scripts by copying code from notebooks into python files by hand parameterize and execute notebooks without conversion programmatic conversion of notebooks to python scripts, and schedule executions In addition you might want to schedule automatic executions of notebooks or converted notebooks.","title":"Productionizing notebooks"},{"location":"notebook-workspace/productionalize/#manual-conversion","text":"This is the most widely used approach. The code is copied from the notebooks into the new script or application, providing great flexibility. The structure of the resulting application might differ significantly from the notebook, which allows better structuring of the code, applying abstractions, such as classes (which are raraly used in the notebooks), creation of unit and integration tests, etc. This approach suits better when the resulting application is complex, consists of many classes, files and modules. But when the resulting application or script does not differ much from the initial notebook, this approach creates extra overhead, requires more timeand slows down the iteration of experimentation-development-productionalization cycles.","title":"Manual conversion"},{"location":"notebook-workspace/productionalize/#notebooks-without-conversion","text":"In many situations it is enough to simply execute the notebook. It might be enough if the notebook is straightforward and result is generated dataset or report, transformation of data in the database, retrained machine learning model. The typical use-case is for example, scheduling of data extraction from the database and providing the result as csv file for end-users, scheduling of SQL stored procedures in the database, web-scrapping tasks, fetching Google Analytics statistics, fetching cryptocurrency daily prices, etc. Execution of notebooks without any transformation removes extra overhead, allows move faster from the experiment to the end result.","title":"Notebooks without conversion"},{"location":"notebook-workspace/productionalize/#auto-conversion","text":"","title":"Auto conversion"},{"location":"notebook-workspace/productionalize/#nbclient","text":"NBClient is a tool for running Jupyter Notebooks in different execution contexts, including the command line. NBClient makes it possible to run command jupyter execute which runs Notebook cells one-by-one. For example, to execute notebook 'basic-notebook.ipynb' in folder /home/examples/notebooks' you would execute command jupyter execute /home/examples/notebooks/basic-notebook.ipynb This will execute each note in the notebook. NBClient might handy, if you need to automate such tasks as web scrapping, data analysis or preparation, retraining of machine learning models.","title":"NBClient"},{"location":"notebook-workspace/productionalize/#nbterm","text":"Nbterm lets you view, edit and execute Jupyter Notebooks in the terminal, and produce the new notebook as a result of execution. One of the differences between NBClient and Nbterm, is that the latter produces new notebook as a result. This is especially useful if you plan to use notebooks as reports. An example use-case would be querying datasase, computing statistics from new orders or client activity, and providing read-only executed notebook via NBviewer. Such notebook will contain both code, SQL as well as results of execution - statistics, chartss and plots. For example, to execute notebook 'basic-notebook.ipynb' in folder /home/examples/notebooks' you would execute command cd /home/examples/notebooks; nbterm --run basic-notebook.ipynb --save-path /home/project/voila/basic-notebook.ipynb Because flag --save-path was defined, executed notebook has been saved to 'basic-notebook_run.ipynb' in folder '/home/project/voila/'. Workspace serves notebooks in this folder with Voila. From the Quickstart page open Voila, and view the notebook in browser.","title":"Nbterm"},{"location":"notebook-workspace/productionalize/#papermill","text":"Papermill is a tool for parameterizing, executing, and analyzing Jupyter Notebooks. Papermill solves the problem of parameterizing notebooks in a simple and efficient way. Jupyter notebooks allow users to tag cells. Papermill lets you to tag a cell, which contains parameters with parameters tag. To parameterize a notebook, designate a cell with the tag parameters. As an example, open Jupyter notebook from the Quickstart page, and create a new note. First thing to do - is to enable tags. Click View -> Cell Toolbar -> Tags . This will enable the tags UI. Create first cell and add tag 'parameters' - simply a variable with default value SOMETEXT=\"HELLO WORLD!\" ADDITIONAL=\"HELLO WORLD!\" Create another cell where variables are used file = \"/home/project/papermill-test.txt\" with open(file, 'a+') as f: f.write(SOMETEXT + \" - \" + ADDITIONAL + \"\\n\") print(SOMETEXT + \" - \" + ADDITIONAL + \"\\n\") Give your notebook a name, for example 'HelloWorld' and save. Open terminal and execute with different parameters papermill /home/project/HelloWorld.ipynb /home/project/nbviewer/HelloWorld.ipynb -p SOMETEXT \"My name\" -p ADDITIONAL \"Slim Shady\" The result notebook will be produced to the folder served by Nbviewer. From the Quickstart page open Nbviewer, and view the notebook in browser. This notebook also creates a file /home/project/papermill-test.txt . Read in this article which explains how to use Jupyter Notebooks (using Jupytext and Papermill) to automatically generate reports. You can schedule generation of automatic reports, which will output to folder served by Nbviewer or Voila.","title":"Papermill"},{"location":"notebook-workspace/productionalize/#programmatic-conversion","text":"","title":"Programmatic conversion"},{"location":"notebook-workspace/productionalize/#nbconvert","text":"Nbconvert tool allows you to convert an .ipynb notebook file into various static formats including executable scripts. For example, if you want to convert Jupyter notebook /home/examples/jupyter/strings.ipynb to a Python script, you would execute jupyter nbconvert --to script /home/examples/jupyter/strings.ipynb --output /home/project/strings Now you can execute this newly-generated python script python /home/project/strings.py Nbconvert can convert to other formats too, such as HTML, PDF, LaTeX. Check tutorial about reports.","title":"Nbconvert"},{"location":"notebook-workspace/productionalize/#nbless","text":"The nbless Python package includes nbconv shell command, which makes conversion of python notebooks into various commands, including .py executable scripts. To try it with the example notebook, execute in terminal nbconv /home/examples/jupyter/strings.ipynb --exporter python --out_file /home/project/nbless-strings.py You will see that file 'nbless-strings.py' has appeared in the '/home/project' folder, Execute it python /home/project/nbless-strings.py","title":"nbless"},{"location":"notebook-workspace/productionalize/#jupytext","text":"Jupytext can save Jupyter notebooks as python or markdown. Try converting example notebooks to python script jupytext --to py /home/examples/jupyter/strings.ipynb --output /home/project/jupytext-strings.py In addition, Jupytext has a plugin for Jupyter that can save Jupyter notebooks as either Markdown files or scripts. Scripts created with Jupytext can be edited and modified just like standard notebooks. Moreover, Jupytext allows to create notebooks paired with the scripts, with any changes made to the notebook automatically applied to the scripts. To get a feeling how this works, copy example notebooks to the project folder cp -r /home/examples/jupyter /home/project From the Quickstart page open Jupyter notebook, and then open note 'strings.ipynb' in the copyed folder. In Jupyter Notebook, pair your notebook to the script format with the Jupytext menu If you refresh the tree folder, you will see that file strings.py appeared You can open this file in VS-Code and see that this is indeed a python file with some meta information on the top, as a comment. You can execute this file from the terminal python /home/project/jupyter/strings.py Awesome! Now if you change the original notebook, you will find out that this script is automatically changed too. Moreover, you can open this strings.py file in Jupyter as a notebook! You can modify and execute cells. As soon as you save changes, refresh the original .ipnb notebook, and it will be automatically get your latest updates. You can also work with this script as notebook in the Jupyter Lab! You just need to explicitly open this file as notebook.","title":"Jupytext"},{"location":"notebook-workspace/productionalize/#scheduling-automatic-executions","text":"You can schedule automatic executions of your notebooks or converted notebooks. If workspace is in cloud, it becomes a simple reporting solution. The resulted executions can generate data, reports or other artifacts that can be downloaded with Filebrowser or viewed with Voila or NBviewer. If you run workspace on your local machine, make sure you enable \"Catch Up\" for scheduled events. This will automatically run all missed events after you turn on your laptop. Hence, you can have generated reports every day without the need to manually trigger every notebook.","title":"Scheduling automatic executions"},{"location":"notebook-workspace/productionalize/#cronicle","text":"With Cronicle we can easily schedule executions of shell commands and scripts. Cronicle has WEB UI, monitor of executions, collect logs and advanced features such as limiting resources for each execution. From the Quickstart page open Cronicle, (Cronicle user/pass: admin/admin), go to 'Schedule' and add new Event. Chose category 'General' and plugin 'Shell script'. Now we can configure the schedule, and provide any of the command to be executed. To execute notebook, parameterized with Papermil daily, simply paste the command to the 'Script Source' box, and configure time when you want Jupyter notebook to run","title":"Cronicle"},{"location":"notebook-workspace/reports/","text":"Reports from notebooks Jupyter notebook - is a famous exploration and visualisation tool, which in turn makes it a good option to present the results or becoming a reporting solution. There are couple options how to create reports from notebooks: Notebooks in view-only mode Notebooks converted to different formats that can be downloaded This workspace has necessary tools to enable all of these options. Jupyter notebook and Jupyter Lab The easiest way to create a report from the Jupyter notebook is export and download it directly from the Jupyter notebook or Jupyter Lab. Oopen any notebook go to 'File' -> 'Download as' and coose the format This is an easy and fast option, but the tools listed below provide more functionality. They are executed in terminal, and hence can be scheduled to be executed automatically. You can get a daily report of your stock price technical analysis ready for you without the need to execute notebook yourself. You can view the report whenever yopu want. NBViewer NBViewer is a web service that displays Jupyter notebooks in view-only mode. NBViewer does not execute notebooks. NBViewer suits the purpos to visualize executed notebooks. It is also a hub for sharing notebooks, as users can download notebooks from NBViewer. Workspace has NBViewer up and running, it serves notebooks from the folder /home/project/nbviewer . It is enough to simply move or copy your Jupyter notebook to this folder to make it displayed by the NBViewer. Copy an example notebook to the NBviever folder cp /home/examples/jupyter/magpye.ipynb /home/project/nbviewer Open Notebook Viewer from the Quickstart page. HINT: if you need to view freshly executed notebook every day, schedule periodic executions with Cronicle. Nbconvert With Nbconvert we can convert an .ipynb notebook to various static formats including HTML, PDF, LaTeX, Markdown. Workspace has a Static File Server, which serves HTML pages from folder /home/static-server . Let's convert Jupyter notebook to the HTML page in this folder jupyter nbconvert --to html /home/examples/jupyter/magpye.ipynb --output /home/static-server/magpye From the Quickstart page open Static File Server, and then open 'magpye.html'. If you want to hide code in the output, add --no-input flag jupyter nbconvert --to html --no-input /home/examples/jupyter/magpye.ipynb --output /home/static-server/magpyeout Nbconvert can convert to other formats too, such as PDF, jupyter nbconvert --to pdf /home/examples/jupyter/magpye.ipynb --output /home/project/magpye From the Quickstart page open Filebrowser, go to folder '/home/project', preview and download pdf file If you want to convert notebook to LaTeX, run jupyter nbconvert --to latex /home/examples/jupyter/magpye.ipynb --output /home/project/magpyetex Voila Voil\u00e0 allows you to convert a Jupyter Notebook into an interactive dashboard that allows you to share your work with others. Voila is up and running as a standalone application inside the workspace. Shortcut for Voila can be found on the Quickstart page. To serve notebok with Voila it is enough to move this notebook to the folder '/home/project/voila'. Voila can serve any notebook. Upon opening, notebook is executed. This is a differece with NBViewer that does not run notebooks. To demonstrate how Voila works, copy an example notebook to the served folder cp /home/examples/notebooks/basic-notebook.ipynb /home/project/voila/ Open Voila dashboard and check the notebook. Voila can also serve notebooks with interactive widgets, as well as notebook with Slides. Workspace has several examples of such interactive notebooks. Copy examples to the folder served cp /home/examples/voila/* /home/project/voila/ Datapane Datapane is a Python library for building interactive reports for your end-users in seconds. Workspace has everything needed to get started with the Datapane. You can see how datapane works by following instructions and running cells in the tutorial notebook 'datapane.ipynb'. Save resulted reports in the folder '/home/static-server/', open Static Server from the Quickstart page, and explore the generated reports. Cronicle With Cronicle we can easily schedule executions of shell commands and scripts. Cronicle has WEB UI, monitor of executions, collect logs and advanced features such as limiting resources for each execution. From the Quickstart page open Cronicle, (Cronicle user/pass: admin/admin), go to 'Schedule' and add new Event. Chose category 'General' and plugin 'Shell script'. Now we can configure the schedule, and provide any of the command to be executed.","title":"Reports"},{"location":"notebook-workspace/reports/#reports-from-notebooks","text":"Jupyter notebook - is a famous exploration and visualisation tool, which in turn makes it a good option to present the results or becoming a reporting solution. There are couple options how to create reports from notebooks: Notebooks in view-only mode Notebooks converted to different formats that can be downloaded This workspace has necessary tools to enable all of these options.","title":"Reports from notebooks"},{"location":"notebook-workspace/reports/#jupyter-notebook-and-jupyter-lab","text":"The easiest way to create a report from the Jupyter notebook is export and download it directly from the Jupyter notebook or Jupyter Lab. Oopen any notebook go to 'File' -> 'Download as' and coose the format This is an easy and fast option, but the tools listed below provide more functionality. They are executed in terminal, and hence can be scheduled to be executed automatically. You can get a daily report of your stock price technical analysis ready for you without the need to execute notebook yourself. You can view the report whenever yopu want.","title":"Jupyter notebook and Jupyter Lab"},{"location":"notebook-workspace/reports/#nbviewer","text":"NBViewer is a web service that displays Jupyter notebooks in view-only mode. NBViewer does not execute notebooks. NBViewer suits the purpos to visualize executed notebooks. It is also a hub for sharing notebooks, as users can download notebooks from NBViewer. Workspace has NBViewer up and running, it serves notebooks from the folder /home/project/nbviewer . It is enough to simply move or copy your Jupyter notebook to this folder to make it displayed by the NBViewer. Copy an example notebook to the NBviever folder cp /home/examples/jupyter/magpye.ipynb /home/project/nbviewer Open Notebook Viewer from the Quickstart page. HINT: if you need to view freshly executed notebook every day, schedule periodic executions with Cronicle.","title":"NBViewer"},{"location":"notebook-workspace/reports/#nbconvert","text":"With Nbconvert we can convert an .ipynb notebook to various static formats including HTML, PDF, LaTeX, Markdown. Workspace has a Static File Server, which serves HTML pages from folder /home/static-server . Let's convert Jupyter notebook to the HTML page in this folder jupyter nbconvert --to html /home/examples/jupyter/magpye.ipynb --output /home/static-server/magpye From the Quickstart page open Static File Server, and then open 'magpye.html'. If you want to hide code in the output, add --no-input flag jupyter nbconvert --to html --no-input /home/examples/jupyter/magpye.ipynb --output /home/static-server/magpyeout Nbconvert can convert to other formats too, such as PDF, jupyter nbconvert --to pdf /home/examples/jupyter/magpye.ipynb --output /home/project/magpye From the Quickstart page open Filebrowser, go to folder '/home/project', preview and download pdf file If you want to convert notebook to LaTeX, run jupyter nbconvert --to latex /home/examples/jupyter/magpye.ipynb --output /home/project/magpyetex","title":"Nbconvert"},{"location":"notebook-workspace/reports/#voila","text":"Voil\u00e0 allows you to convert a Jupyter Notebook into an interactive dashboard that allows you to share your work with others. Voila is up and running as a standalone application inside the workspace. Shortcut for Voila can be found on the Quickstart page. To serve notebok with Voila it is enough to move this notebook to the folder '/home/project/voila'. Voila can serve any notebook. Upon opening, notebook is executed. This is a differece with NBViewer that does not run notebooks. To demonstrate how Voila works, copy an example notebook to the served folder cp /home/examples/notebooks/basic-notebook.ipynb /home/project/voila/ Open Voila dashboard and check the notebook. Voila can also serve notebooks with interactive widgets, as well as notebook with Slides. Workspace has several examples of such interactive notebooks. Copy examples to the folder served cp /home/examples/voila/* /home/project/voila/","title":"Voila"},{"location":"notebook-workspace/reports/#datapane","text":"Datapane is a Python library for building interactive reports for your end-users in seconds. Workspace has everything needed to get started with the Datapane. You can see how datapane works by following instructions and running cells in the tutorial notebook 'datapane.ipynb'. Save resulted reports in the folder '/home/static-server/', open Static Server from the Quickstart page, and explore the generated reports.","title":"Datapane"},{"location":"notebook-workspace/reports/#cronicle","text":"With Cronicle we can easily schedule executions of shell commands and scripts. Cronicle has WEB UI, monitor of executions, collect logs and advanced features such as limiting resources for each execution. From the Quickstart page open Cronicle, (Cronicle user/pass: admin/admin), go to 'Schedule' and add new Event. Chose category 'General' and plugin 'Shell script'. Now we can configure the schedule, and provide any of the command to be executed.","title":"Cronicle"},{"location":"notebook-workspace/working-with-data/","text":"Working with Data Jupyter notebooks are famous for their convenience to explore, experiment and visualize. All these steps include working with data, including data manipulations and looking on data, tables and charts. Workspace has several popular python data packages including numpy, pandas, and matplotlib. To start with, copy tutorial notebooks into the project forlder. cp -r /home/examples/tutorials /home/project Pandas Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Workspace contains basic pandas tutorial notebook. Open in Jupyter tutorials/pandas.ipynb and follow the steps Explore entire pandas documentation Pandas SQL Pandas is great for complex data transformations, but some queries on data frames can be much easier with the standard SQL. Workspace has Fugue SQL which allows querying and transforming pandas data frames. FugueSQL integrates with Python by creating extensions and applying them in the %%fsql cells. Open in Jupyter tutorials/sql-pandas.ipynb and run with examples cells one-byone, following the instructions Charts Plotting charts - is one of the most typical steps in experiments, analytics and data science. Workspace has several popular plotting packages installed, which you can use to visualise trends and patterns in your data: Matplotlib Seaborn Plotline Plotly Workspace includes tutorial notebook tutorials/charts.ipynb , open and follow the instructions","title":"Working with data"},{"location":"notebook-workspace/working-with-data/#working-with-data","text":"Jupyter notebooks are famous for their convenience to explore, experiment and visualize. All these steps include working with data, including data manipulations and looking on data, tables and charts. Workspace has several popular python data packages including numpy, pandas, and matplotlib. To start with, copy tutorial notebooks into the project forlder. cp -r /home/examples/tutorials /home/project","title":"Working with Data"},{"location":"notebook-workspace/working-with-data/#pandas","text":"Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Workspace contains basic pandas tutorial notebook. Open in Jupyter tutorials/pandas.ipynb and follow the steps Explore entire pandas documentation","title":"Pandas"},{"location":"notebook-workspace/working-with-data/#pandas-sql","text":"Pandas is great for complex data transformations, but some queries on data frames can be much easier with the standard SQL. Workspace has Fugue SQL which allows querying and transforming pandas data frames. FugueSQL integrates with Python by creating extensions and applying them in the %%fsql cells. Open in Jupyter tutorials/sql-pandas.ipynb and run with examples cells one-byone, following the instructions","title":"Pandas SQL"},{"location":"notebook-workspace/working-with-data/#charts","text":"Plotting charts - is one of the most typical steps in experiments, analytics and data science. Workspace has several popular plotting packages installed, which you can use to visualise trends and patterns in your data: Matplotlib Seaborn Plotline Plotly Workspace includes tutorial notebook tutorials/charts.ipynb , open and follow the instructions","title":"Charts"},{"location":"php-workspace/","text":"PHP workspace Docker image with PHP, Composer and browser-based VS-Code version. Why this images If you need self-hosted remote development environment. If you want to be one command away from coding in PHP. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/php-workspace and open localhost:8020 in browser. Features PHP Composer Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"php-workspace/#php-workspace","text":"Docker image with PHP, Composer and browser-based VS-Code version.","title":"PHP workspace"},{"location":"php-workspace/#why-this-images","text":"If you need self-hosted remote development environment. If you want to be one command away from coding in PHP.","title":"Why this images"},{"location":"php-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/php-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"php-workspace/#features","text":"PHP Composer Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"php-workspace/tutorial/","text":"This basic tutorial shows how to create a Hello-world PHP app, serve example websites and how to install packages with Composer. Hello World Check PHP version php -v Open IDE and create file hello.php with the following content <html> <head> <title>PHP Test</title> </head> <body> <?php echo '<p>Hello World</p>'; ?> </body> </html> Start server in terminal cd /home/project php -S 127.0.0.1:8040 Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app Website example Clone GitHub repo with a PHP website, for example git clone https://github.com/banago/simple-php-website.git Server with PHP development server cd simple-php-website php -S 0.0.0.0:8040 Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app Composer Install package with Composer composer require phpunit/php-timer","title":"Tutorial"},{"location":"php-workspace/tutorial/#hello-world","text":"Check PHP version php -v Open IDE and create file hello.php with the following content <html> <head> <title>PHP Test</title> </head> <body> <?php echo '<p>Hello World</p>'; ?> </body> </html> Start server in terminal cd /home/project php -S 127.0.0.1:8040 Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app","title":"Hello World"},{"location":"php-workspace/tutorial/#website-example","text":"Clone GitHub repo with a PHP website, for example git clone https://github.com/banago/simple-php-website.git Server with PHP development server cd simple-php-website php -S 0.0.0.0:8040 Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app","title":"Website example"},{"location":"php-workspace/tutorial/#composer","text":"Install package with Composer composer require phpunit/php-timer","title":"Composer"},{"location":"postgres-workspace/","text":"Postgres workspace Collection of tools to interact with PostgreSQL. Query, explore, manage, develop, test performance, import annd export data, generate mock data, create backups, manage migrations, generate reports, schedule tasks. Why this images If you need a toolset to interact with PostgreSQL and you don't have time to build it on your own. To directly access and manage PostgreSQL inside your kubernetes cluster. A better PostgreSQL docker image for local dev environment. This is PostgreSQL with toolset in a single image. You want to learn PostgreSQL features. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/postgres-workspace open localhost:8020 in browser. Features PostgreSQL 14 database PostgreSQL tools: Postgres CLI tools: psql - standard PostgreSQL interactive terminal. pgcli - command line interface for Postgres with auto-completion and syntax highlighting. pspg - data visualizer (pager). Performance and load testing tools: pgmetrics - collect 350+ metrics from a running PostgreSQL server. pgCenter - admin tool for observing and troubleshooting Postgres. pgbench - run benchmark tests on PostgreSQL. Backups and restore tools: pg_dump - generate a file with SQL commands that, when fed back to the server, will recreate the database. pg_dumpall - back up each database in a given cluster, and also preserve cluster-wide data such as role and tablespace definitions. pg_restore - utility for restoring a PostgreSQL database from an archive created by pg_dump. Tools to import/export data to/from Postgres: pgclimb - export data into different data formats, including CSV, JSON, templated HTML & Markdown. pgfutter - import CSV and line delimited JSON into PostgreSQL the easy way. pgloader - very versatile data loading tool for PostgreSQL. Fake data generators and data mocking tools: synth - a tool for generating realistic data. mock-data - generate mock data, tables and databases easily. Database change management toolset: sqitch - database change management application. yuniql - schema versioning and database migration engine. pgmigrate - database migration tool developed by Yandex. migra - find differences in database schemas easily. pg_sample - create database sample while maintaining referential integrity. Postgres explain visualizer - understand Postgres execution plans. DBdesigner - draw ERD diagrams and use it to generate DDL code. Schemaspy - generate extensive HTML database documentation that looks great. tbls - generate documentation (essentially data catalog) from the database pg_flame - flamegraph generator for Postgres explain output. Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Rclone - save backups to S3 Demo: Postgres workspace","title":"About"},{"location":"postgres-workspace/#postgres-workspace","text":"Collection of tools to interact with PostgreSQL. Query, explore, manage, develop, test performance, import annd export data, generate mock data, create backups, manage migrations, generate reports, schedule tasks.","title":"Postgres workspace"},{"location":"postgres-workspace/#why-this-images","text":"If you need a toolset to interact with PostgreSQL and you don't have time to build it on your own. To directly access and manage PostgreSQL inside your kubernetes cluster. A better PostgreSQL docker image for local dev environment. This is PostgreSQL with toolset in a single image. You want to learn PostgreSQL features.","title":"Why this images"},{"location":"postgres-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/postgres-workspace open localhost:8020 in browser.","title":"Start"},{"location":"postgres-workspace/#features","text":"PostgreSQL 14 database PostgreSQL tools: Postgres CLI tools: psql - standard PostgreSQL interactive terminal. pgcli - command line interface for Postgres with auto-completion and syntax highlighting. pspg - data visualizer (pager). Performance and load testing tools: pgmetrics - collect 350+ metrics from a running PostgreSQL server. pgCenter - admin tool for observing and troubleshooting Postgres. pgbench - run benchmark tests on PostgreSQL. Backups and restore tools: pg_dump - generate a file with SQL commands that, when fed back to the server, will recreate the database. pg_dumpall - back up each database in a given cluster, and also preserve cluster-wide data such as role and tablespace definitions. pg_restore - utility for restoring a PostgreSQL database from an archive created by pg_dump. Tools to import/export data to/from Postgres: pgclimb - export data into different data formats, including CSV, JSON, templated HTML & Markdown. pgfutter - import CSV and line delimited JSON into PostgreSQL the easy way. pgloader - very versatile data loading tool for PostgreSQL. Fake data generators and data mocking tools: synth - a tool for generating realistic data. mock-data - generate mock data, tables and databases easily. Database change management toolset: sqitch - database change management application. yuniql - schema versioning and database migration engine. pgmigrate - database migration tool developed by Yandex. migra - find differences in database schemas easily. pg_sample - create database sample while maintaining referential integrity. Postgres explain visualizer - understand Postgres execution plans. DBdesigner - draw ERD diagrams and use it to generate DDL code. Schemaspy - generate extensive HTML database documentation that looks great. tbls - generate documentation (essentially data catalog) from the database pg_flame - flamegraph generator for Postgres explain output. Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Rclone - save backups to S3 Demo: Postgres workspace","title":"Features"},{"location":"postgres-workspace/tutorial/","text":"These tutorials demonstrates how to use Postgres workspace to explore and interact with Postgres. To start, open Quickstart page and go to the terminal to execute commands from this tutorial. Postgres The workspace includes Postgres 14 up and running. You can use it to experiment and try things before trying on the application database. Connection parameters: host - localhost , user - abc (no password), database - abc . You can load sample database to check out the workspace features and applications. For example, load \"usda\" database git clone https://github.com/morenoh149/postgresDBSamples /home/pg_dbs cd /home/pg_dbs/usda-r18-1.0 createdb usda psql -f usda.sql usda #Explore data pgcli usda SELECT * FROM data_src load Chinook database cd /home/pg_dbs/chinook-1.4 createdb -E UTF8 chinook psql -f Chinook_PostgreSql_utf8.sql -d chinook # Explore data pgcli chinook SELECT * FROM \"Track\" load Adventureworks database cd /home/pg_dbs/adventureworks/ unzip data.zip # this will unzip all csv to the current directory psql -c \"CREATE DATABASE \\\"adventureworks\\\";\" psql -d adventureworks < install.sql # Explore data pgcli adventureworks SELECT * FROM sales.salesorderdetail Further examples will use this local Postgres instance, but can easily applied to another Postgres database. VS-code & plugins Apache Theia , a browser-based version of the VS-code, is part of the workspace. It has extensions to query Postgres If you loaded sample databases to the local Postgres, connect & explore data directly in the IDE Postgres CLI tools Querying from command line can be awesome! psql psql is a PostgreSQL interactive terminal # connect to local Postgres psql # connect to database chinook (local Postgres) psql chinook # general psql connection pattern psql postgresql://dbmaster:5432/mydb # with password prompt psql -U postgres -W postgresql://my-postgres-host:5432/mydb $ without password prompt PGPASSWORD=xxxxxxxxxxx psql -U postgres postgresql://my-postgres-host:5432/mydb pgcli pgcli - is a postgres client that does auto-completion and syntax highlighting. # pgcli to the local Postgres pgcli # connect to database usda (local Postgres) pgcli usda # pgcli to the remote Postgres pgcli -h my-postgres-host -p 5432 -U postgres -W -d mydb pspg Pspg is a unix pager (with very rich functionality) designed for work with tables. Pspg is enabled by default, and will automatically applied to psql and pgcli if the result of SELECT statement is large. You can disable pspg by deleting the (last) line PAGER='pspg -s 4 --interactive --no-mouse' from file /home/abc/.zshrc. (or you can modify configuration). Performance and load testing Monitor live activity, collect and analyse metrics, perform load tests. pgmetrics pgmetrics is an open-source, zero-dependency, single-binary tool that can collect 350+ metrics from a running PostgreSQL server and display it in easy-to-read text format or export it as JSON and CSV for scripting. # Try on local Postgres database \"usda\" (upon password prompt just hit Enter) pgmetrics --no-pager -h localhost -U abc usda # Remote Postgres pgmetrics --no-pager -h my-postgres-host -p 5432 -U postgres mydb Hint: Output to txt file pgmetrics -h localhost -U abc usda >> /home/static-server/pgmetrics-usda.txt and view with static server. Alternatively, preview with file browser. pgCenter pgCenter is a command-line admin tool for observing and troubleshooting Postgres. # for local Postgres pgcenter top -h localhost -p 5432 -U abc -d usda # for remote Postgres (password prompt) pgcenter top -h my-postgres-host -p 5432 -U postgres -d mydb # for remote Postgres (no password prompt) PGPASSWORD=xxxxxxxxxxx pgcenter top -h my-postgres-host -p 5432 -U postgres -d mydb pgbench pgbench is a simple program for running benchmark tests on PostgreSQL. # initialize pgbench -h localhost -U abc -i -s 50 abc # test pgbench -h localhost -U abc -c 10 -j 2 -t 10000 abc Example use on remote Postgres: # initialize pgbench -h my-postgres-host -U postgres -i -s 50 mydb # test pgbench -h my-postgres-host -U postgres -c 10 -j 2 -t 10000 mydb Learn more: - Pgbench tutorial . Backups and restore Create backup dumps with pg_dump or pg_dumpall , push to S3 with rclone and schedule with Cronicle . pgdump The purpose of pg_dump is to generate a file with SQL commands that, when fed back to the server, will recreate the database in the same state as it was at the time of the dump. mkdir /home/dumps/ # Dump from local Postgres pg_dump -h localhost -p 5432 -U abc usda > /home/dumps/local_postgres_dump # Dump from remote Postgres PGPASSWORD=xxxxxxxxxxx pg_dump -h my-postgres-host -p 5432 -U postgres mydb > /home/dumps/remote_postgres_dump # Restore from dump to local Postgres psql dbname < /home/dumps/remote_postgres_dump Other versions of pg_dump The default pg_dump version is for Postgres 14. If you see error error: aborting because of server version mismatch , try pg_dump for your postgres version (currently Postgres versions 12, 13 and 14 are supported) pg_dump_12 -h localhost -p 5432 -U abc abc > /home/dumps/local_postgres_dump_12 pg_dump_13 -h localhost -p 5432 -U abc abc > /home/dumps/local_postgres_dump_13 pg_dumpall pg_dumpall is a utility for writing out (\"dumping\") all PostgreSQL databases of a cluster into one script file. pg_dumpall -h localhost -p 5432 -U abc > /home/dumps/local_postgres_dumpall # Dump from remote Postgres PGPASSWORD=xxxxxxxxxxx pg_dump -h my-postgres-host -p 5432 -U postgres mydb > /home/dumps/remote_postgres_dump # Restore from dump to local Postgres psql dbname < /home/dumps/remote_postgres_dump The default is pg_dumpall for Postgres 14, if you need earlier version, use pg_dumpall_12 , pg_dumpall_13 . rclone rclone is a command line program to manage files on cloud storage. It is a feature rich alternative to cloud vendors' web storage interfaces. Copy dump to S3: Create file ~/.config/rclone/rclone.conf with the following content [remote] type = s3 provider = AWS access_key_id = XXXXXXXXXXXXXXXXXXXXXX secret_access_key = XXxxXXxXXXxxxXXxXXXxxXXXxxxXXXxXXXX region = xx-xxxx-x Use Rclone to copy to S3 and delete from local rclone move /home/dumps/ remote:my-s3-bucket/dumps/ Restore from S3 to local: When there is a need to restore database from a dump, copy dupm from S3 or mount S3 bucket to local folder. In case of mounting, file will not be physically copied. Database restoration will happen directly from dump on S3. # copy & restore rclone copy remote:dwh-1/dumps/remote_postgres_dump /home/s3-dumps/ psql dbname < /home/s3-dumps/remote_postgres_dump # mount & restore rclone sync remote:dwh-1/dumps /home/s3-dumps psql dbname < /home/s3-dumps/remote_postgres_dump scheduling backups Schedule with Cronicle : open Cronicle WEB UI from the workspace Quickstart page, use user/pass - admin/admin, and create scheduled event (plugin - shell script). Provide commands to be executed. Database sample Who hasn't faced a need to make a sample database for testing/development purposes? :-) # Create sample from database \"usda\" in local Postgres pg_sample -h localhost -U abc --limit=500 --file=/home/project/sample_db.sql usda # create new database (local Postgres) createdb usda_sample # restore sample to the new database psql usda_sample < /home/project/sample_db.sql Data import/export Import data from various files, export to csv, json, html. Download with Filebrowser , upload to S3 with Rclone , schedule with Cronicle . pgclimb pgclimb is a PostgreSQL utility to export data into different data formats with support for templates. # From local Postgres instance pgclimb --host localhost --port 5432 --dbname abc --username abc --query \"SELECT * FROM distributors\" -o /home/project/distributors.csv csv # From remote Postgres instance pgclimb --host my-postgres-host --port 5432 --dbname mydb --username postgres --password xxxxxxxxxxx --query \"SELECT * FROM phonebook\" -o /home/project/phone.csv csv pgclimb --host my-postgres-host --port 5432 --dbname mydb --username postgres --password xxxxxxxxxxx --query \"SELECT * FROM phonebook\" -o /home/static-server/phone.html html pgfutter pgfutter helps to import CSV and line delimited JSON into PostgreSQL the easy way. For example, create file /home/project/friends.json with the following content {\"name\": \"Jacob\", \"age\": 26, \"friends\": [\"Anthony\"]} {\"name\": \"Anthony\", \"age\": 25, \"friends\": []} {\"name\": \"Emma\", \"age\": 28, \"friends\": [\"Jacob\", \"Anthony\"]} Import data to Postgres # to the local Postgres pgfutter --host localhost --port 5432 --dbname abc --username abc --table friends json /home/project/friends.json # to the remote Postgres pgfutter --host my-postgres-host --port 5432 --dbname mydb --username postgres --pass admin --schema imports --table friends json /home/project/friends.json Another example: create file /home/project/friends2.csv with the following content name,age,friends Jacob,26,\"Anthony\" Anthony,25,\"\" Emma,28,\"Jacob,Anthony\" Import data to Postgres pgfutter --host my-postgres-host --port 5432 --dbname mydb --username postgres --pass admin --schema public --table friends2 csv /home/project/friends2.csv PGLoader PGLoader - a very versatile data loading tool for PostgreSQL. Load data from files, or migrate entire databases to Postgres. Example of use: create test load file /home/project/test.csv with the following content Header, with a \u00a9 sign \"2.6.190.56\",\"2.6.190.63\",\"33996344\",\"33996351\",\"GB\",\"United Kingdom\" \"3.0.0.0\",\"4.17.135.31\",\"50331648\",\"68257567\",\"US\",\"United States\" \"4.17.135.32\",\"4.17.135.63\",\"68257568\",\"68257599\",\"CA\",\"Canada\" \"4.17.135.64\",\"4.17.142.255\",\"68257600\",\"68259583\",\"US\",\"United States\" \"4.17.143.0\",\"4.17.143.15\",\"68259584\",\"68259599\",\"CA\",\"Canada\" \"4.17.143.16\",\"4.18.32.71\",\"68259600\",\"68296775\",\"US\",\"United States\" Create PGLoader file /home/project/test.load with the following content (use your postgres connection arguments) LOAD CSV FROM '/home/project/test.csv' (x, y, a, b, c, d) -- local Postgres INTO postgresql://abc@localhost:5432/abc?csv (a, b, d, c) -- remote Postgres -- INTO postgresql://user:password@my-postgres-host:5432/mydb?csv (a, b, d, c) WITH truncate, skip header = 1, fields optionally enclosed by '\"', fields escaped by double-quote, fields terminated by ',' SET client_encoding to 'latin1', work_mem to '12MB', standard_conforming_strings to 'on' BEFORE LOAD DO $$ drop table if exists csv; $$, $$ create table csv ( a bigint, b bigint, c char(2), d text ); $$; Load data into new table called \"csv\" pgloader /home/project/test.load Data generators/mocking Generate realistic data for existing tables, create new mock tables or databases. Synth Synth is a tool for generating realistic data using a declarative data model. Synth is database agnostic and can scale to millions of rows of data. Create dir mkdir /home/project/synth and create file /home/project/synth/companies.json with the following contents { \"type\": \"array\", \"length\": { \"type\": \"number\", \"constant\": 1 }, \"content\": { \"type\": \"object\", \"company_id\": { \"type\": \"number\", \"id\": {} }, \"company_name\": { \"type\": \"string\", \"faker\": { \"generator\": \"company_name\" } } } } Generate data (2 entries only) as json - synth generate /home/project/synth/ --size 2 | jq . Generate data into (local) Postgres # Create table users in local Postgres database echo \"CREATE TABLE companies(company_id SERIAL PRIMARY KEY, company_name VARCHAR(255) NOT NULL);\" | psql # generate random data to table users synth generate /home/project/synth/ --size 200 --to postgres://abc@localhost:5432/abc # check 10 records echo \"SELECT * FROM companies LIMIT 10\" | psql Learn more: - Synth docs - How to Create PostgreSQL Test Data mock-data The idea behind mock-data is to allow users to test database queries with sets of fake data in any pre-defined table. # Create table in (local) Postgres echo \"CREATE TABLE distributors (did integer, name varchar(40));\" | psql # Populate (local) Postgres with fake data (2000 rows) mock tables -t \"distributors\" -r 2000 --uri=\"postgres://abc@localhost:5432/abc?sslmode=disable\" Learn more: - Mock-data documentation Database change management Implement database change management in your team and drive higher software delivery and organizational performance. sqitch sqitch - is a database change management application. To demonstrate (very briefly) how Sqitch works, create a new database called 'flipr_test' in the local Postgres: createdb flipr_test Initialize and configure a Sqitch project, called 'flipr' mkdir /home/project/flipr && cd /home/project/flipr sqitch init flipr --engine pg Open file /home/project/flipr/sqitch.conf and update configuration for the \"pg\" engine to conect to local Postgres as following [core] engine = pg [engine \"pg\"] target = db:pg: client = psql Create database and changes to be managed by Sqitch # create new Postgres database createdb flipr_test # create sqitch change sqitch add appschema -n 'Add schema for all flipr objects.' # add SQL command to change file echo \"CREATE SCHEMA flipr;\" >> /home/project/flipr/deploy/appschema.sql # add SQL command to change-revert file echo \"DROP SCHEMA flipr;\" >> /home/project/flipr/revert/appschema.sql Apply db change with Squitch sqitch deploy db:pg:flipr_test Connet to local Postgres, database 'flipr_test' and check schema 'flipr' was created pgcli flipr_test SELECT schema_name FROM information_schema.schemata Now you can open terminal back, and revert this change with the command sqitch revert db:pg:flipr_test . Learn more : - Sqitch Postgres tutorial yuniql Yuniql is a data platform devops tool using migration-based and database-first delivery model. Migration-based as each changeset to the schema and seed data is a set of carefully prepared scripts controlled with a version number. To briefly demonstrate how Yuniql works, clone example Yuniql project git clone https://github.com/rdagumampan/yuniql.git /home/project/yuniql Create new database in the local Postgres instance createdb helloyuniql Apply to the local Postgres a Yuniql project in /home/project/yuniql/samples/basic-postgresql-sample export YUNIQL_PLATFORM=\"postgresql\" export YUNIQL_WORKSPACE=\"/home/project/yuniql/samples/basic-postgresql-sample\" export YUNIQL_CONNECTION_STRING=\"Host=localhost;Port=5432;Username=abc;Password=;Database=helloyuniql\" Finally, run migrations cd /home/project/yuniql/samples/basic-postgresql-sample yuniql run -a --platform postgresql Check the results pgcli helloyuniql SELECT * FROM regions migra migra is a schema comparison tool for PostgreSQL. Find differences in database schemas as easily as running a diff on two text files. Migra makes schema changes almost automatic. Management of database migration deployments becomes much easier, faster, and more reliable. For example, compare two local Postgres databases migra postgresql://abc:@localhost:5432/abc postgresql://abc:@localhost:5432/helloyuniql --unsafe Migra will produce a set of SQL commands to make database \"abc\" have the same DDL as databas \"helloyuniql\". Visualization tools dbdesigner dbdesigner a visual tool to create entity relationship diagrams and generate Postgres DDL code. Open dbdesigner from the Quickstart page. schemaspy schemaspy generates complete database documentation that looks great. schemaspy -h localhost -p 5432 -d usda -u abc -o /home/static-server/usda-schemaspy Open static file server and go to the folder 'usda-schemaspy' Postgres explain visualizer Pev helps to understand Postgres execution plans tbls tbls - a tool for document a database, written in Go. For example, generate Markdown docs tbls doc \"postgres://abc@localhost:5432/usda?sslmode=disable\" /home/static-server/usda-tbls Now you can push these docks push to Github. pg_flame pg_flame - a flamegraph generator for Postgres EXPLAIN ANALYZE output. psql usda -qAtc 'EXPLAIN (ANALYZE, FORMAT JSON) SELECT * FROM nut_data N JOIN food_des F ON F.ndb_no = N.ndb_no JOIN datsrcln D ON D.nutr_no = N.nutr_no LEFT JOIN data_src ON data_src.datasrc_id = D.datasrc_id WHERE N.nutr_val > 20 AND data_src.year > 1975 AND data_src.year < 1986;' \\ | pg_flame > /home/static-server/flamegraph.html Other useful tools pg_insights Convenient SQL for monitoring Postgres database health. Clone repo and use it to analyse Postgres databases git clone https://github.com/lob/pg_insights /home/pg_insights sqlfluff Sqlfluff is a moduler sql linter for humans. echo \"select * from t where id=1\" >/tmp/test.sql sqlfluff lint /tmp/test.sql Results in: == [/tmp/test.sql] FAIL L: 1 | P: 1 | L044 | Query produces an unknown number of result columns. L: 1 | P: 27 | L006 | Missing whitespace before = L: 1 | P: 27 | L006 | Missing whitespace after =","title":"Tutorial"},{"location":"postgres-workspace/tutorial/#postgres","text":"The workspace includes Postgres 14 up and running. You can use it to experiment and try things before trying on the application database. Connection parameters: host - localhost , user - abc (no password), database - abc . You can load sample database to check out the workspace features and applications. For example, load \"usda\" database git clone https://github.com/morenoh149/postgresDBSamples /home/pg_dbs cd /home/pg_dbs/usda-r18-1.0 createdb usda psql -f usda.sql usda #Explore data pgcli usda SELECT * FROM data_src load Chinook database cd /home/pg_dbs/chinook-1.4 createdb -E UTF8 chinook psql -f Chinook_PostgreSql_utf8.sql -d chinook # Explore data pgcli chinook SELECT * FROM \"Track\" load Adventureworks database cd /home/pg_dbs/adventureworks/ unzip data.zip # this will unzip all csv to the current directory psql -c \"CREATE DATABASE \\\"adventureworks\\\";\" psql -d adventureworks < install.sql # Explore data pgcli adventureworks SELECT * FROM sales.salesorderdetail Further examples will use this local Postgres instance, but can easily applied to another Postgres database.","title":"Postgres"},{"location":"postgres-workspace/tutorial/#vs-code-plugins","text":"Apache Theia , a browser-based version of the VS-code, is part of the workspace. It has extensions to query Postgres If you loaded sample databases to the local Postgres, connect & explore data directly in the IDE","title":"VS-code &amp; plugins"},{"location":"postgres-workspace/tutorial/#postgres-cli-tools","text":"Querying from command line can be awesome!","title":"Postgres CLI tools"},{"location":"postgres-workspace/tutorial/#psql","text":"psql is a PostgreSQL interactive terminal # connect to local Postgres psql # connect to database chinook (local Postgres) psql chinook # general psql connection pattern psql postgresql://dbmaster:5432/mydb # with password prompt psql -U postgres -W postgresql://my-postgres-host:5432/mydb $ without password prompt PGPASSWORD=xxxxxxxxxxx psql -U postgres postgresql://my-postgres-host:5432/mydb","title":"psql"},{"location":"postgres-workspace/tutorial/#pgcli","text":"pgcli - is a postgres client that does auto-completion and syntax highlighting. # pgcli to the local Postgres pgcli # connect to database usda (local Postgres) pgcli usda # pgcli to the remote Postgres pgcli -h my-postgres-host -p 5432 -U postgres -W -d mydb","title":"pgcli"},{"location":"postgres-workspace/tutorial/#pspg","text":"Pspg is a unix pager (with very rich functionality) designed for work with tables. Pspg is enabled by default, and will automatically applied to psql and pgcli if the result of SELECT statement is large. You can disable pspg by deleting the (last) line PAGER='pspg -s 4 --interactive --no-mouse' from file /home/abc/.zshrc. (or you can modify configuration).","title":"pspg"},{"location":"postgres-workspace/tutorial/#performance-and-load-testing","text":"Monitor live activity, collect and analyse metrics, perform load tests.","title":"Performance and load testing"},{"location":"postgres-workspace/tutorial/#pgmetrics","text":"pgmetrics is an open-source, zero-dependency, single-binary tool that can collect 350+ metrics from a running PostgreSQL server and display it in easy-to-read text format or export it as JSON and CSV for scripting. # Try on local Postgres database \"usda\" (upon password prompt just hit Enter) pgmetrics --no-pager -h localhost -U abc usda # Remote Postgres pgmetrics --no-pager -h my-postgres-host -p 5432 -U postgres mydb Hint: Output to txt file pgmetrics -h localhost -U abc usda >> /home/static-server/pgmetrics-usda.txt and view with static server. Alternatively, preview with file browser.","title":"pgmetrics"},{"location":"postgres-workspace/tutorial/#pgcenter","text":"pgCenter is a command-line admin tool for observing and troubleshooting Postgres. # for local Postgres pgcenter top -h localhost -p 5432 -U abc -d usda # for remote Postgres (password prompt) pgcenter top -h my-postgres-host -p 5432 -U postgres -d mydb # for remote Postgres (no password prompt) PGPASSWORD=xxxxxxxxxxx pgcenter top -h my-postgres-host -p 5432 -U postgres -d mydb","title":"pgCenter"},{"location":"postgres-workspace/tutorial/#pgbench","text":"pgbench is a simple program for running benchmark tests on PostgreSQL. # initialize pgbench -h localhost -U abc -i -s 50 abc # test pgbench -h localhost -U abc -c 10 -j 2 -t 10000 abc Example use on remote Postgres: # initialize pgbench -h my-postgres-host -U postgres -i -s 50 mydb # test pgbench -h my-postgres-host -U postgres -c 10 -j 2 -t 10000 mydb Learn more: - Pgbench tutorial .","title":"pgbench"},{"location":"postgres-workspace/tutorial/#backups-and-restore","text":"Create backup dumps with pg_dump or pg_dumpall , push to S3 with rclone and schedule with Cronicle .","title":"Backups and restore"},{"location":"postgres-workspace/tutorial/#pgdump","text":"The purpose of pg_dump is to generate a file with SQL commands that, when fed back to the server, will recreate the database in the same state as it was at the time of the dump. mkdir /home/dumps/ # Dump from local Postgres pg_dump -h localhost -p 5432 -U abc usda > /home/dumps/local_postgres_dump # Dump from remote Postgres PGPASSWORD=xxxxxxxxxxx pg_dump -h my-postgres-host -p 5432 -U postgres mydb > /home/dumps/remote_postgres_dump # Restore from dump to local Postgres psql dbname < /home/dumps/remote_postgres_dump Other versions of pg_dump The default pg_dump version is for Postgres 14. If you see error error: aborting because of server version mismatch , try pg_dump for your postgres version (currently Postgres versions 12, 13 and 14 are supported) pg_dump_12 -h localhost -p 5432 -U abc abc > /home/dumps/local_postgres_dump_12 pg_dump_13 -h localhost -p 5432 -U abc abc > /home/dumps/local_postgres_dump_13","title":"pgdump"},{"location":"postgres-workspace/tutorial/#pg_dumpall","text":"pg_dumpall is a utility for writing out (\"dumping\") all PostgreSQL databases of a cluster into one script file. pg_dumpall -h localhost -p 5432 -U abc > /home/dumps/local_postgres_dumpall # Dump from remote Postgres PGPASSWORD=xxxxxxxxxxx pg_dump -h my-postgres-host -p 5432 -U postgres mydb > /home/dumps/remote_postgres_dump # Restore from dump to local Postgres psql dbname < /home/dumps/remote_postgres_dump The default is pg_dumpall for Postgres 14, if you need earlier version, use pg_dumpall_12 , pg_dumpall_13 .","title":"pg_dumpall"},{"location":"postgres-workspace/tutorial/#rclone","text":"rclone is a command line program to manage files on cloud storage. It is a feature rich alternative to cloud vendors' web storage interfaces. Copy dump to S3: Create file ~/.config/rclone/rclone.conf with the following content [remote] type = s3 provider = AWS access_key_id = XXXXXXXXXXXXXXXXXXXXXX secret_access_key = XXxxXXxXXXxxxXXxXXXxxXXXxxxXXXxXXXX region = xx-xxxx-x Use Rclone to copy to S3 and delete from local rclone move /home/dumps/ remote:my-s3-bucket/dumps/ Restore from S3 to local: When there is a need to restore database from a dump, copy dupm from S3 or mount S3 bucket to local folder. In case of mounting, file will not be physically copied. Database restoration will happen directly from dump on S3. # copy & restore rclone copy remote:dwh-1/dumps/remote_postgres_dump /home/s3-dumps/ psql dbname < /home/s3-dumps/remote_postgres_dump # mount & restore rclone sync remote:dwh-1/dumps /home/s3-dumps psql dbname < /home/s3-dumps/remote_postgres_dump","title":"rclone"},{"location":"postgres-workspace/tutorial/#scheduling-backups","text":"Schedule with Cronicle : open Cronicle WEB UI from the workspace Quickstart page, use user/pass - admin/admin, and create scheduled event (plugin - shell script). Provide commands to be executed.","title":"scheduling backups"},{"location":"postgres-workspace/tutorial/#database-sample","text":"Who hasn't faced a need to make a sample database for testing/development purposes? :-) # Create sample from database \"usda\" in local Postgres pg_sample -h localhost -U abc --limit=500 --file=/home/project/sample_db.sql usda # create new database (local Postgres) createdb usda_sample # restore sample to the new database psql usda_sample < /home/project/sample_db.sql","title":"Database sample"},{"location":"postgres-workspace/tutorial/#data-importexport","text":"Import data from various files, export to csv, json, html. Download with Filebrowser , upload to S3 with Rclone , schedule with Cronicle .","title":"Data import/export"},{"location":"postgres-workspace/tutorial/#pgclimb","text":"pgclimb is a PostgreSQL utility to export data into different data formats with support for templates. # From local Postgres instance pgclimb --host localhost --port 5432 --dbname abc --username abc --query \"SELECT * FROM distributors\" -o /home/project/distributors.csv csv # From remote Postgres instance pgclimb --host my-postgres-host --port 5432 --dbname mydb --username postgres --password xxxxxxxxxxx --query \"SELECT * FROM phonebook\" -o /home/project/phone.csv csv pgclimb --host my-postgres-host --port 5432 --dbname mydb --username postgres --password xxxxxxxxxxx --query \"SELECT * FROM phonebook\" -o /home/static-server/phone.html html","title":"pgclimb"},{"location":"postgres-workspace/tutorial/#pgfutter","text":"pgfutter helps to import CSV and line delimited JSON into PostgreSQL the easy way. For example, create file /home/project/friends.json with the following content {\"name\": \"Jacob\", \"age\": 26, \"friends\": [\"Anthony\"]} {\"name\": \"Anthony\", \"age\": 25, \"friends\": []} {\"name\": \"Emma\", \"age\": 28, \"friends\": [\"Jacob\", \"Anthony\"]} Import data to Postgres # to the local Postgres pgfutter --host localhost --port 5432 --dbname abc --username abc --table friends json /home/project/friends.json # to the remote Postgres pgfutter --host my-postgres-host --port 5432 --dbname mydb --username postgres --pass admin --schema imports --table friends json /home/project/friends.json Another example: create file /home/project/friends2.csv with the following content name,age,friends Jacob,26,\"Anthony\" Anthony,25,\"\" Emma,28,\"Jacob,Anthony\" Import data to Postgres pgfutter --host my-postgres-host --port 5432 --dbname mydb --username postgres --pass admin --schema public --table friends2 csv /home/project/friends2.csv","title":"pgfutter"},{"location":"postgres-workspace/tutorial/#pgloader","text":"PGLoader - a very versatile data loading tool for PostgreSQL. Load data from files, or migrate entire databases to Postgres. Example of use: create test load file /home/project/test.csv with the following content Header, with a \u00a9 sign \"2.6.190.56\",\"2.6.190.63\",\"33996344\",\"33996351\",\"GB\",\"United Kingdom\" \"3.0.0.0\",\"4.17.135.31\",\"50331648\",\"68257567\",\"US\",\"United States\" \"4.17.135.32\",\"4.17.135.63\",\"68257568\",\"68257599\",\"CA\",\"Canada\" \"4.17.135.64\",\"4.17.142.255\",\"68257600\",\"68259583\",\"US\",\"United States\" \"4.17.143.0\",\"4.17.143.15\",\"68259584\",\"68259599\",\"CA\",\"Canada\" \"4.17.143.16\",\"4.18.32.71\",\"68259600\",\"68296775\",\"US\",\"United States\" Create PGLoader file /home/project/test.load with the following content (use your postgres connection arguments) LOAD CSV FROM '/home/project/test.csv' (x, y, a, b, c, d) -- local Postgres INTO postgresql://abc@localhost:5432/abc?csv (a, b, d, c) -- remote Postgres -- INTO postgresql://user:password@my-postgres-host:5432/mydb?csv (a, b, d, c) WITH truncate, skip header = 1, fields optionally enclosed by '\"', fields escaped by double-quote, fields terminated by ',' SET client_encoding to 'latin1', work_mem to '12MB', standard_conforming_strings to 'on' BEFORE LOAD DO $$ drop table if exists csv; $$, $$ create table csv ( a bigint, b bigint, c char(2), d text ); $$; Load data into new table called \"csv\" pgloader /home/project/test.load","title":"PGLoader"},{"location":"postgres-workspace/tutorial/#data-generatorsmocking","text":"Generate realistic data for existing tables, create new mock tables or databases.","title":"Data generators/mocking"},{"location":"postgres-workspace/tutorial/#synth","text":"Synth is a tool for generating realistic data using a declarative data model. Synth is database agnostic and can scale to millions of rows of data. Create dir mkdir /home/project/synth and create file /home/project/synth/companies.json with the following contents { \"type\": \"array\", \"length\": { \"type\": \"number\", \"constant\": 1 }, \"content\": { \"type\": \"object\", \"company_id\": { \"type\": \"number\", \"id\": {} }, \"company_name\": { \"type\": \"string\", \"faker\": { \"generator\": \"company_name\" } } } } Generate data (2 entries only) as json - synth generate /home/project/synth/ --size 2 | jq . Generate data into (local) Postgres # Create table users in local Postgres database echo \"CREATE TABLE companies(company_id SERIAL PRIMARY KEY, company_name VARCHAR(255) NOT NULL);\" | psql # generate random data to table users synth generate /home/project/synth/ --size 200 --to postgres://abc@localhost:5432/abc # check 10 records echo \"SELECT * FROM companies LIMIT 10\" | psql Learn more: - Synth docs - How to Create PostgreSQL Test Data","title":"Synth"},{"location":"postgres-workspace/tutorial/#mock-data","text":"The idea behind mock-data is to allow users to test database queries with sets of fake data in any pre-defined table. # Create table in (local) Postgres echo \"CREATE TABLE distributors (did integer, name varchar(40));\" | psql # Populate (local) Postgres with fake data (2000 rows) mock tables -t \"distributors\" -r 2000 --uri=\"postgres://abc@localhost:5432/abc?sslmode=disable\" Learn more: - Mock-data documentation","title":"mock-data"},{"location":"postgres-workspace/tutorial/#database-change-management","text":"Implement database change management in your team and drive higher software delivery and organizational performance.","title":"Database change management"},{"location":"postgres-workspace/tutorial/#sqitch","text":"sqitch - is a database change management application. To demonstrate (very briefly) how Sqitch works, create a new database called 'flipr_test' in the local Postgres: createdb flipr_test Initialize and configure a Sqitch project, called 'flipr' mkdir /home/project/flipr && cd /home/project/flipr sqitch init flipr --engine pg Open file /home/project/flipr/sqitch.conf and update configuration for the \"pg\" engine to conect to local Postgres as following [core] engine = pg [engine \"pg\"] target = db:pg: client = psql Create database and changes to be managed by Sqitch # create new Postgres database createdb flipr_test # create sqitch change sqitch add appschema -n 'Add schema for all flipr objects.' # add SQL command to change file echo \"CREATE SCHEMA flipr;\" >> /home/project/flipr/deploy/appschema.sql # add SQL command to change-revert file echo \"DROP SCHEMA flipr;\" >> /home/project/flipr/revert/appschema.sql Apply db change with Squitch sqitch deploy db:pg:flipr_test Connet to local Postgres, database 'flipr_test' and check schema 'flipr' was created pgcli flipr_test SELECT schema_name FROM information_schema.schemata Now you can open terminal back, and revert this change with the command sqitch revert db:pg:flipr_test . Learn more : - Sqitch Postgres tutorial","title":"sqitch"},{"location":"postgres-workspace/tutorial/#yuniql","text":"Yuniql is a data platform devops tool using migration-based and database-first delivery model. Migration-based as each changeset to the schema and seed data is a set of carefully prepared scripts controlled with a version number. To briefly demonstrate how Yuniql works, clone example Yuniql project git clone https://github.com/rdagumampan/yuniql.git /home/project/yuniql Create new database in the local Postgres instance createdb helloyuniql Apply to the local Postgres a Yuniql project in /home/project/yuniql/samples/basic-postgresql-sample export YUNIQL_PLATFORM=\"postgresql\" export YUNIQL_WORKSPACE=\"/home/project/yuniql/samples/basic-postgresql-sample\" export YUNIQL_CONNECTION_STRING=\"Host=localhost;Port=5432;Username=abc;Password=;Database=helloyuniql\" Finally, run migrations cd /home/project/yuniql/samples/basic-postgresql-sample yuniql run -a --platform postgresql Check the results pgcli helloyuniql SELECT * FROM regions","title":"yuniql"},{"location":"postgres-workspace/tutorial/#migra","text":"migra is a schema comparison tool for PostgreSQL. Find differences in database schemas as easily as running a diff on two text files. Migra makes schema changes almost automatic. Management of database migration deployments becomes much easier, faster, and more reliable. For example, compare two local Postgres databases migra postgresql://abc:@localhost:5432/abc postgresql://abc:@localhost:5432/helloyuniql --unsafe Migra will produce a set of SQL commands to make database \"abc\" have the same DDL as databas \"helloyuniql\".","title":"migra"},{"location":"postgres-workspace/tutorial/#visualization-tools","text":"","title":"Visualization tools"},{"location":"postgres-workspace/tutorial/#dbdesigner","text":"dbdesigner a visual tool to create entity relationship diagrams and generate Postgres DDL code. Open dbdesigner from the Quickstart page.","title":"dbdesigner"},{"location":"postgres-workspace/tutorial/#schemaspy","text":"schemaspy generates complete database documentation that looks great. schemaspy -h localhost -p 5432 -d usda -u abc -o /home/static-server/usda-schemaspy Open static file server and go to the folder 'usda-schemaspy'","title":"schemaspy"},{"location":"postgres-workspace/tutorial/#postgres-explain-visualizer","text":"Pev helps to understand Postgres execution plans","title":"Postgres explain visualizer"},{"location":"postgres-workspace/tutorial/#tbls","text":"tbls - a tool for document a database, written in Go. For example, generate Markdown docs tbls doc \"postgres://abc@localhost:5432/usda?sslmode=disable\" /home/static-server/usda-tbls Now you can push these docks push to Github.","title":"tbls"},{"location":"postgres-workspace/tutorial/#pg_flame","text":"pg_flame - a flamegraph generator for Postgres EXPLAIN ANALYZE output. psql usda -qAtc 'EXPLAIN (ANALYZE, FORMAT JSON) SELECT * FROM nut_data N JOIN food_des F ON F.ndb_no = N.ndb_no JOIN datsrcln D ON D.nutr_no = N.nutr_no LEFT JOIN data_src ON data_src.datasrc_id = D.datasrc_id WHERE N.nutr_val > 20 AND data_src.year > 1975 AND data_src.year < 1986;' \\ | pg_flame > /home/static-server/flamegraph.html","title":"pg_flame"},{"location":"postgres-workspace/tutorial/#other-useful-tools","text":"","title":"Other useful tools"},{"location":"postgres-workspace/tutorial/#pg_insights","text":"Convenient SQL for monitoring Postgres database health. Clone repo and use it to analyse Postgres databases git clone https://github.com/lob/pg_insights /home/pg_insights","title":"pg_insights"},{"location":"postgres-workspace/tutorial/#sqlfluff","text":"Sqlfluff is a moduler sql linter for humans. echo \"select * from t where id=1\" >/tmp/test.sql sqlfluff lint /tmp/test.sql Results in: == [/tmp/test.sql] FAIL L: 1 | P: 1 | L044 | Query produces an unknown number of result columns. L: 1 | P: 27 | L006 | Missing whitespace before = L: 1 | P: 27 | L006 | Missing whitespace after =","title":"sqlfluff"},{"location":"python-workspace/","text":"Python workspace Docker image for Python development environment. Includes Python, python tooling together with browser-based IDE, file browser, static server, job scheduler. Why this images If you need isolated dev environment where you can code and install packages and apps without affecting the base operating system. If you need self-hosted remote development environment. If you need to be just one command away from coding in Python. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/python-workspace and open localhost:8020 in browser. Features Python tools: IPython and Notebooks Pdoc3 Pytest-html-reporter SnakeViz Vprof Pyinstrument Flameprof Pylint-json2html Pre-commit Flake8 Poetry Black Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo: Python workspace","title":"About"},{"location":"python-workspace/#python-workspace","text":"Docker image for Python development environment. Includes Python, python tooling together with browser-based IDE, file browser, static server, job scheduler.","title":"Python workspace"},{"location":"python-workspace/#why-this-images","text":"If you need isolated dev environment where you can code and install packages and apps without affecting the base operating system. If you need self-hosted remote development environment. If you need to be just one command away from coding in Python.","title":"Why this images"},{"location":"python-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/python-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"python-workspace/#features","text":"Python tools: IPython and Notebooks Pdoc3 Pytest-html-reporter SnakeViz Vprof Pyinstrument Flameprof Pylint-json2html Pre-commit Flake8 Poetry Black Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Demo: Python workspace","title":"Features"},{"location":"python-workspace/pyenv-tutorial/","text":"Pyenv pyenv is used to isolate Python versions. For example, you may want to test your code against Python 2.7, 3.6, 3.7 and 3.8, so you'll need a way to switch between them. List python versions pyenv install --list Install additional python versions pyenv install 3.10.4 pyenv install 3.6.0 Check installed python versions pyenv versions Change global python version pyenv global 3.10.4 Check python version python --version Create folder with local python environment, check Python version mkdir test-pyenv cd test-pyenv pyenv local 3.6.0 python --version If you want to schedule script (with Cronicle) that uses pyenv, add eval \"$(pyenv init -)\" before the script. For example eval \"$(pyenv init -)\"; python /home/project/scripts/script.py","title":"Pyenv tutorial"},{"location":"python-workspace/pyenv-tutorial/#pyenv","text":"pyenv is used to isolate Python versions. For example, you may want to test your code against Python 2.7, 3.6, 3.7 and 3.8, so you'll need a way to switch between them. List python versions pyenv install --list Install additional python versions pyenv install 3.10.4 pyenv install 3.6.0 Check installed python versions pyenv versions Change global python version pyenv global 3.10.4 Check python version python --version Create folder with local python environment, check Python version mkdir test-pyenv cd test-pyenv pyenv local 3.6.0 python --version If you want to schedule script (with Cronicle) that uses pyenv, add eval \"$(pyenv init -)\" before the script. For example eval \"$(pyenv init -)\"; python /home/project/scripts/script.py","title":"Pyenv"},{"location":"python-workspace/tutorial/","text":"This doc contains example tutorials how to use Python tooling included in Python workspace. To start, open Quickstart page for quick access to VS-code and terminal. IPython and Notebooks IPython provides a rich toolkit to help you make the most of using Python interactively. One of its main components is a powerful interactive Python shell. IPython is very handy. For example, starting with IPython 7.0, and when using Python 3.6 and above, IPython offer the ability to run asynchronous code from the REPL. To start IPython kernel, open workspace terminal ad execute ipython . Below is an example of installing packages and evaluation of async code in IPython shell - something you cannot do in a standard python shell: NOTE: in order not to increase the Workspace size, by default Python Workspace can only render notebooks. Workspace does not have installed all the requirements to run notebooks. This is can be done easily. As soon as you try to run a cell in the note, you will see a pop-out winndow suggesting to install missing dependencies. You just need to accept. Demo: Install dependencies for notebooks Python environments Venv The venv virtualenv is a very popular tool that creates isolated Python environments for Python libraries. This module provides support for creating lightweight \u201cvirtual environments\u201d with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Create virtual environment called env-1 python3 -m venv env-1 Activate environnment source env-1/bin/activate Poetry Create our project poetry new poetry-demo cd poetry-demo Specify dependencies in pyproject.toml. By default, poetry creates a virtual environmen. There are several ways to run commands within this virtual environment. To run your script simply use poetry run poetry run python your_script.py The easiest way to activate the virtual environment is to create a new shell with poetry shell poetry shell Python tooling Python-report Python-report is a small utility that tryies to generate various reports and artefacts from your python project, such as linting report; run tests and make HTML report; make auto-documentation and profiling visualizations. Unit test statistics will be visualised with the browser-based dashboard. cd /home/examples/simple-script && python-report The resulting report will be produced to the folder /home/static-server/<NAME-OF-PYTHON-PROJECT-FOLDER>/<TIMESTAMP> . Demo: Python report (In addition, all pytests statistics will be collected, and available in foldder /home/static-server/<NAME-OF-PYTHON-PROJECT-FOLDER> ) . Python-report is a simple bash script /home/abc/utils/python-report.sh . You can also use separately any of the toos. Pytest-html-reporter Pytest-html-reporter generates a beautiful static html report based on pytest framework. These reports result in dashboard website, that shows all historical tests and statistics. To execute tests, and generate report with Pytest-html-reporter, cd to the python project tests folder, and execute pytest ./ --html-report=./pytest-report . The results will be produced to the sub-folder ./pytest-report . For instance, execute tests and generate report for the example python project execute cd /home/examples/simple-script && pytest ./ --html-report=/home/static-server/my-pytest-report the output will be in folder /home/static-server/my-pytest-report that is served with a Static-file server Demo: Pytest-html-reporter Pdoc3 Auto-generate API documentation for Python projects. Let's generate autodocumentation website for the example python project, with output into `` where it can be viewed with Static-file server cd /home/examples/simple-script && pdoc --html --output-dir /home/static-server/pdoc-html ./ Demo: Pdoc3 Vprof Vprof is a Python package providing rich and interactive visualizations for various Python program characteristics such as running time and memory usage. Vprof is a browser-based profiling tool. Here is an example of profiling scripts from the example python project: cd /home/examples/simple-script && vprof -H 0.0.0.0 -p 8031 -c cpmh fib.py cd /home/examples/simple-script && vprof -H 0.0.0.0 -p 8031 -c cpmh script.py Demo: Vprof SnakeViz SnakeViz is a browser based graphical viewer for the output of Python\u2019s cProfile module. Let's profile and visualize one of python modules in the example project: cd /home/examples/simple-script && python -m cProfile -o script.prof script.py snakeviz -s -p 8030 -H 0.0.0.0 script.prof You will see thae link appeared in the terminal, open it in browser Demo: SnakeViz Flameprof Flameprof is a Flamegraph generator for python's cProfile stats. Let's profile and visualize one of python modules in the example project: cd /home/examples/simple-script && python -m cProfile -o script.prof script.py flameprof script.prof > script.svg Demo: Flameprof Pyinstrument Pyinstrument is a Python profiler. A profiler is a tool to help you 'optimize' your code - make it faster. It sounds obvious, but to get the biggest speed increase you must focus on the slowest part of your program. Pyinstrument helps you find it! Profile and visualize one of python modules in the example project: mkdir -p /home/static-server/profiling/basic-python-script pyinstrument -t -r html -o /home/static-server/profiling/basic-python-script/p2 script.py Demo: Pyinstrument cProfile cProfile is recommended for most users; it's a C extension with reasonable overhead that makes it suitable for profiling long-running programs. Profile and visualize one of python modules in the example project: cd /home/examples/simple-script && python -m cProfile script.py >> /home/static-server/cprof.tx Pylint-json2html A pylint JSON report file to HTML: pylint is used to generate a JSON report, and this tool will transform this report into an HTML document: pylint script.py | pylint-json2html -f jsonextended -o script.html Demo: Pylint-json2html demo Pre-commit Git hook scripts are useful for identifying simple issues before submission to code review. We run our hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks. The example python project has a pre-commit configuration file: cd /home/examples/simple-script && pre-commit install pre-commit run --all-files Schedule python jobs Workspace inncludes Cronicle - a powerful scheduling tool, that has a browser-based UI with dashboards, allows to configure resource limits for jobs and much more! Python Workspace includes an example script that fetches today's exchange rates: cd /home/examples/exchange_rates python fetch-rates.py The script will fetch today's exchange rates from and output result to the folder /home/static-server/exchange-rates_<DATE>.json . This folder is served by the Static-file server Demo: Fetch exchange rates Fetching echange rates - is a typical problems for nearly every business, that is working on the international market. You can schedule execution of this script to fetch exchange rates daily Demo: Schedule exchange rates NOTE: Scheduling jobs is especially useful when the Workspace is running on a cloud server. Read here how to launch workspace in cloud .","title":"Tutorial"},{"location":"python-workspace/tutorial/#ipython-and-notebooks","text":"IPython provides a rich toolkit to help you make the most of using Python interactively. One of its main components is a powerful interactive Python shell. IPython is very handy. For example, starting with IPython 7.0, and when using Python 3.6 and above, IPython offer the ability to run asynchronous code from the REPL. To start IPython kernel, open workspace terminal ad execute ipython . Below is an example of installing packages and evaluation of async code in IPython shell - something you cannot do in a standard python shell: NOTE: in order not to increase the Workspace size, by default Python Workspace can only render notebooks. Workspace does not have installed all the requirements to run notebooks. This is can be done easily. As soon as you try to run a cell in the note, you will see a pop-out winndow suggesting to install missing dependencies. You just need to accept. Demo: Install dependencies for notebooks","title":"IPython and Notebooks"},{"location":"python-workspace/tutorial/#python-environments","text":"","title":"Python environments"},{"location":"python-workspace/tutorial/#venv","text":"The venv virtualenv is a very popular tool that creates isolated Python environments for Python libraries. This module provides support for creating lightweight \u201cvirtual environments\u201d with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Create virtual environment called env-1 python3 -m venv env-1 Activate environnment source env-1/bin/activate","title":"Venv"},{"location":"python-workspace/tutorial/#poetry","text":"Create our project poetry new poetry-demo cd poetry-demo Specify dependencies in pyproject.toml. By default, poetry creates a virtual environmen. There are several ways to run commands within this virtual environment. To run your script simply use poetry run poetry run python your_script.py The easiest way to activate the virtual environment is to create a new shell with poetry shell poetry shell","title":"Poetry"},{"location":"python-workspace/tutorial/#python-tooling","text":"","title":"Python tooling"},{"location":"python-workspace/tutorial/#python-report","text":"Python-report is a small utility that tryies to generate various reports and artefacts from your python project, such as linting report; run tests and make HTML report; make auto-documentation and profiling visualizations. Unit test statistics will be visualised with the browser-based dashboard. cd /home/examples/simple-script && python-report The resulting report will be produced to the folder /home/static-server/<NAME-OF-PYTHON-PROJECT-FOLDER>/<TIMESTAMP> . Demo: Python report (In addition, all pytests statistics will be collected, and available in foldder /home/static-server/<NAME-OF-PYTHON-PROJECT-FOLDER> ) . Python-report is a simple bash script /home/abc/utils/python-report.sh . You can also use separately any of the toos.","title":"Python-report"},{"location":"python-workspace/tutorial/#pytest-html-reporter","text":"Pytest-html-reporter generates a beautiful static html report based on pytest framework. These reports result in dashboard website, that shows all historical tests and statistics. To execute tests, and generate report with Pytest-html-reporter, cd to the python project tests folder, and execute pytest ./ --html-report=./pytest-report . The results will be produced to the sub-folder ./pytest-report . For instance, execute tests and generate report for the example python project execute cd /home/examples/simple-script && pytest ./ --html-report=/home/static-server/my-pytest-report the output will be in folder /home/static-server/my-pytest-report that is served with a Static-file server Demo: Pytest-html-reporter","title":"Pytest-html-reporter"},{"location":"python-workspace/tutorial/#pdoc3","text":"Auto-generate API documentation for Python projects. Let's generate autodocumentation website for the example python project, with output into `` where it can be viewed with Static-file server cd /home/examples/simple-script && pdoc --html --output-dir /home/static-server/pdoc-html ./ Demo: Pdoc3","title":"Pdoc3"},{"location":"python-workspace/tutorial/#vprof","text":"Vprof is a Python package providing rich and interactive visualizations for various Python program characteristics such as running time and memory usage. Vprof is a browser-based profiling tool. Here is an example of profiling scripts from the example python project: cd /home/examples/simple-script && vprof -H 0.0.0.0 -p 8031 -c cpmh fib.py cd /home/examples/simple-script && vprof -H 0.0.0.0 -p 8031 -c cpmh script.py Demo: Vprof","title":"Vprof"},{"location":"python-workspace/tutorial/#snakeviz","text":"SnakeViz is a browser based graphical viewer for the output of Python\u2019s cProfile module. Let's profile and visualize one of python modules in the example project: cd /home/examples/simple-script && python -m cProfile -o script.prof script.py snakeviz -s -p 8030 -H 0.0.0.0 script.prof You will see thae link appeared in the terminal, open it in browser Demo: SnakeViz","title":"SnakeViz"},{"location":"python-workspace/tutorial/#flameprof","text":"Flameprof is a Flamegraph generator for python's cProfile stats. Let's profile and visualize one of python modules in the example project: cd /home/examples/simple-script && python -m cProfile -o script.prof script.py flameprof script.prof > script.svg Demo: Flameprof","title":"Flameprof"},{"location":"python-workspace/tutorial/#pyinstrument","text":"Pyinstrument is a Python profiler. A profiler is a tool to help you 'optimize' your code - make it faster. It sounds obvious, but to get the biggest speed increase you must focus on the slowest part of your program. Pyinstrument helps you find it! Profile and visualize one of python modules in the example project: mkdir -p /home/static-server/profiling/basic-python-script pyinstrument -t -r html -o /home/static-server/profiling/basic-python-script/p2 script.py Demo: Pyinstrument","title":"Pyinstrument"},{"location":"python-workspace/tutorial/#cprofile","text":"cProfile is recommended for most users; it's a C extension with reasonable overhead that makes it suitable for profiling long-running programs. Profile and visualize one of python modules in the example project: cd /home/examples/simple-script && python -m cProfile script.py >> /home/static-server/cprof.tx","title":"cProfile"},{"location":"python-workspace/tutorial/#pylint-json2html","text":"A pylint JSON report file to HTML: pylint is used to generate a JSON report, and this tool will transform this report into an HTML document: pylint script.py | pylint-json2html -f jsonextended -o script.html Demo: Pylint-json2html demo","title":"Pylint-json2html"},{"location":"python-workspace/tutorial/#pre-commit","text":"Git hook scripts are useful for identifying simple issues before submission to code review. We run our hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks. The example python project has a pre-commit configuration file: cd /home/examples/simple-script && pre-commit install pre-commit run --all-files","title":"Pre-commit"},{"location":"python-workspace/tutorial/#schedule-python-jobs","text":"Workspace inncludes Cronicle - a powerful scheduling tool, that has a browser-based UI with dashboards, allows to configure resource limits for jobs and much more! Python Workspace includes an example script that fetches today's exchange rates: cd /home/examples/exchange_rates python fetch-rates.py The script will fetch today's exchange rates from and output result to the folder /home/static-server/exchange-rates_<DATE>.json . This folder is served by the Static-file server Demo: Fetch exchange rates Fetching echange rates - is a typical problems for nearly every business, that is working on the international market. You can schedule execution of this script to fetch exchange rates daily Demo: Schedule exchange rates NOTE: Scheduling jobs is especially useful when the Workspace is running on a cloud server. Read here how to launch workspace in cloud .","title":"Schedule python jobs"},{"location":"r-workspace/","text":"R workspace Docker image with R and browser-based RStudio version. Why this images Uou need self-hosted remote development environment. You want to be one 'docker run' command away from having everything needed to work with R. You need isolated environment where you can work with R without polluting main environment. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/r-workspace and open localhost:8020 in browser. Features R Dev tools: RStudio server - open source version of popular R IDE, browser-based. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"r-workspace/#r-workspace","text":"Docker image with R and browser-based RStudio version.","title":"R workspace"},{"location":"r-workspace/#why-this-images","text":"Uou need self-hosted remote development environment. You want to be one 'docker run' command away from having everything needed to work with R. You need isolated environment where you can work with R without polluting main environment.","title":"Why this images"},{"location":"r-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/r-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"r-workspace/#features","text":"R Dev tools: RStudio server - open source version of popular R IDE, browser-based. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"r-workspace/tutorial/","text":"This basic tutorial shows how to work with R in this workspace. Workspace includes RStudio Server, a browser-based IDE for R. RStudio Server - is the best way to use R for statistics, data analytics and visualisation. Workspace has full-size browser-base terminal. R Open terminal, and check R version R --version To start R shell, simply execute R in the terminal If you want to install packages from the terminal, execute Rscript -e 'install.packages(\"drat\", repos=\"https://cloud.r-project.org\")' Examples Open workspace terminal and clone example repository cd /home/abc git clone https://github.com/dmarcelinobr/r-code-examples.git Open Rstudio, and try some examples, i.e. 2d-density-plot.r . Evaluate code, agree to install dependencies","title":"Tutorial"},{"location":"r-workspace/tutorial/#r","text":"Open terminal, and check R version R --version To start R shell, simply execute R in the terminal If you want to install packages from the terminal, execute Rscript -e 'install.packages(\"drat\", repos=\"https://cloud.r-project.org\")'","title":"R"},{"location":"r-workspace/tutorial/#examples","text":"Open workspace terminal and clone example repository cd /home/abc git clone https://github.com/dmarcelinobr/r-code-examples.git Open Rstudio, and try some examples, i.e. 2d-density-plot.r . Evaluate code, agree to install dependencies","title":"Examples"},{"location":"redis-workspace/","text":"Redis workspace Docker image with Redis, Redis Commander and several CLI tools to interact with Redis. Why this images To directly interact with Redis inside your kubernetes cluster. A better Redis docker image for local dev environment. You get Redis together with the toolset to work with it. Start docker run --name rwid-1 -d -p 8020-8040:8020-8040 alnoda/redis-workspace and open localhost:8020 in browser. Features Redis Redis tools: Redis-commander - Redis web management tool. Iredis - CLI for Redis with AutoCompletion and Syntax Highlighting. Redis-dump - dump Redis keys to a file. Redis-Tui - Redis Text-based UI client in CLI. Redis extension for VS-Code Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"redis-workspace/#redis-workspace","text":"Docker image with Redis, Redis Commander and several CLI tools to interact with Redis.","title":"Redis workspace"},{"location":"redis-workspace/#why-this-images","text":"To directly interact with Redis inside your kubernetes cluster. A better Redis docker image for local dev environment. You get Redis together with the toolset to work with it.","title":"Why this images"},{"location":"redis-workspace/#start","text":"docker run --name rwid-1 -d -p 8020-8040:8020-8040 alnoda/redis-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"redis-workspace/#features","text":"Redis Redis tools: Redis-commander - Redis web management tool. Iredis - CLI for Redis with AutoCompletion and Syntax Highlighting. Redis-dump - dump Redis keys to a file. Redis-Tui - Redis Text-based UI client in CLI. Redis extension for VS-Code Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"redis-workspace/tutorial/","text":"This doc has examples how to use workspace to interact with Redis. In examples we use local Redis that is running inside the workspace, but the commands apply to any Redis instance. Open Quickstart for quick access to all the tools, including workspace terminal. Open workspace terminal, and load example Redis datasets git clone https://github.com/redis-developer/redis-datasets.git /home/project/redis-datasets cat /home/project/redis-datasets/movie-database/import_actors.redis | redis-cli cat /home/project/redis-datasets/movie-database/import_movies.redis | redis-cli Redis Commander Open Redis Commander and explore Redis databases Alternatively use browser-based VS-code with Redis extension CLI tools If you prefer working in the terminal, explore Redis with Redis-Tui . To connect to the internal Redis instance simply execute redis-tui To interact with internal Redis, inn the workspace terminal launch iredis CLI, get & set keys iredis Iredis has autocompletion and hints, which might be handy for administrative tasks iredis --newbie Create dump of the Redis database redis-dump-go -h localhost > /home/redis-movie-dump.resp Go to the file browser and download the database dump to your PC","title":"Tutorial"},{"location":"redis-workspace/tutorial/#redis-commander","text":"Open Redis Commander and explore Redis databases Alternatively use browser-based VS-code with Redis extension","title":"Redis Commander"},{"location":"redis-workspace/tutorial/#cli-tools","text":"If you prefer working in the terminal, explore Redis with Redis-Tui . To connect to the internal Redis instance simply execute redis-tui To interact with internal Redis, inn the workspace terminal launch iredis CLI, get & set keys iredis Iredis has autocompletion and hints, which might be handy for administrative tasks iredis --newbie Create dump of the Redis database redis-dump-go -h localhost > /home/redis-movie-dump.resp Go to the file browser and download the database dump to your PC","title":"CLI tools"},{"location":"ruby-workspace/","text":"Ruby workspace Docker image with Ruby and browser-based VS-Code version. Why this images If you need self-hosted remote development environment. If you want to be one terminal command away from coding in Ruby. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/ruby-workspace and open localhost:8020 in browser. Features Ruby Rbennv Bundler Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"ruby-workspace/#ruby-workspace","text":"Docker image with Ruby and browser-based VS-Code version.","title":"Ruby workspace"},{"location":"ruby-workspace/#why-this-images","text":"If you need self-hosted remote development environment. If you want to be one terminal command away from coding in Ruby.","title":"Why this images"},{"location":"ruby-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/ruby-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"ruby-workspace/#features","text":"Ruby Rbennv Bundler Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"ruby-workspace/tutorial/","text":"This basic workspace demonstrates how to create simple Hello-world application, install Rails, manage multiple Ruby versions with Rbenv and install dependencies with Bundler. Ruby Open terminal, and check Ruby version ruby -v Install Rails - a web application development framework written in the Ruby programming language. gem install rails Check version rails --version Hello world Open VS-code, and create file http_server.rb # http_server.rb require 'socket' server = TCPServer.new 8040 while session = server.accept request = session.gets puts request session.print \"HTTP/1.1 200\\r\\n\" # 1 session.print \"Content-Type: text/html\\r\\n\" # 2 session.print \"\\r\\n\" # 3 session.print \"Hello world! The time is #{Time.now}\" #4 session.close end Now open terminal, and execute to serve simple server ruby http_server.rb Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app Bundler Bundler provides a consistent environment for Ruby projects by tracking and installing the exact gems and versions that are needed. Create file Gemfile with the following content source 'https://rubygems.org' gem 'nokogiri' gem 'rack', '~> 2.0.1' gem 'rspec' Install all of the required gems bundle install Rbenv Use rbenv to pick a Ruby version for your application and guarantee that your development environment matches production. List available versions, and install another one rbenv install --list rbenv install 3.0.4 Lists all Ruby versions known to rbenv, and shows an asterisk next to the currently active version. rbenv versions global environment Change global Ruby (for all folders) rbenv global 3.0.4 local environment (specific folder) Chose local Ruby environment for this specific folder rbenv local 3.0.4","title":"Tutorial"},{"location":"ruby-workspace/tutorial/#ruby","text":"Open terminal, and check Ruby version ruby -v Install Rails - a web application development framework written in the Ruby programming language. gem install rails Check version rails --version","title":"Ruby"},{"location":"ruby-workspace/tutorial/#hello-world","text":"Open VS-code, and create file http_server.rb # http_server.rb require 'socket' server = TCPServer.new 8040 while session = server.accept request = session.gets puts request session.print \"HTTP/1.1 200\\r\\n\" # 1 session.print \"Content-Type: text/html\\r\\n\" # 2 session.print \"\\r\\n\" # 3 session.print \"Hello world! The time is #{Time.now}\" #4 session.close end Now open terminal, and execute to serve simple server ruby http_server.rb Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app","title":"Hello world"},{"location":"ruby-workspace/tutorial/#bundler","text":"Bundler provides a consistent environment for Ruby projects by tracking and installing the exact gems and versions that are needed. Create file Gemfile with the following content source 'https://rubygems.org' gem 'nokogiri' gem 'rack', '~> 2.0.1' gem 'rspec' Install all of the required gems bundle install","title":"Bundler"},{"location":"ruby-workspace/tutorial/#rbenv","text":"Use rbenv to pick a Ruby version for your application and guarantee that your development environment matches production. List available versions, and install another one rbenv install --list rbenv install 3.0.4 Lists all Ruby versions known to rbenv, and shows an asterisk next to the currently active version. rbenv versions global environment Change global Ruby (for all folders) rbenv global 3.0.4 local environment (specific folder) Chose local Ruby environment for this specific folder rbenv local 3.0.4","title":"Rbenv"},{"location":"rust-workspace/","text":"Rust workspace Docker image with Rust and browser-based VS-Code version. Why this images If you need self-hosted remote development environment. If you want to be one terminal command away from coding in Rust. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/rust-workspace and open localhost:8020 in browser. Features Rust Rustup Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"rust-workspace/#rust-workspace","text":"Docker image with Rust and browser-based VS-Code version.","title":"Rust workspace"},{"location":"rust-workspace/#why-this-images","text":"If you need self-hosted remote development environment. If you want to be one terminal command away from coding in Rust.","title":"Why this images"},{"location":"rust-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/rust-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"rust-workspace/#features","text":"Rust Rustup Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"rust-workspace/tutorial/","text":"This basic tutorial shows how to create Rust Hello-world application, manage multiple versions of Rust with Rustup and install dependencies with Cargo. Hello world Check Rust version rustc --version Create new project folder cargo new my_example cd my_example The new project is created together, with hello-world app in src sub-folder cat src/main.rs Compile it cargo build And then run it ./target/debug/my_example We can also compile and then run it, all in one step cargo run Rustup Rustup - is a toolchain is a specific version of the collection of programs needed to compile a Rust application. It includes, but is not limited to: The compiler, rustc The dependency manager and build tool, cargo The documentation generator, rustdoc Rustup provides ways to install, remove, update, select and otherwise manage these toolchains and their associated pieces. Install specific version of Rust toolchain rustup install 1.30.0 Show toolchains rustup show Change default toolchain rustup default 1.30.0 Project with dependencies Cargo is Rust package manager. It is a tool that allows Rust packages to declare their various dependencies and ensure that you\u2019ll always get a repeatable build. Clone example repo git clone https://github.com/rdesarz/rust-http-server.git cd rust-http-server Build and start the server cd example cargo run --package http-server --bin http-server 0.0.0.0:8040 You will see that before cargo builds the package, it installs all the dependencies from the file Cargo.toml . Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app","title":"Tutorial"},{"location":"rust-workspace/tutorial/#hello-world","text":"Check Rust version rustc --version Create new project folder cargo new my_example cd my_example The new project is created together, with hello-world app in src sub-folder cat src/main.rs Compile it cargo build And then run it ./target/debug/my_example We can also compile and then run it, all in one step cargo run","title":"Hello world"},{"location":"rust-workspace/tutorial/#rustup","text":"Rustup - is a toolchain is a specific version of the collection of programs needed to compile a Rust application. It includes, but is not limited to: The compiler, rustc The dependency manager and build tool, cargo The documentation generator, rustdoc Rustup provides ways to install, remove, update, select and otherwise manage these toolchains and their associated pieces. Install specific version of Rust toolchain rustup install 1.30.0 Show toolchains rustup show Change default toolchain rustup default 1.30.0","title":"Rustup"},{"location":"rust-workspace/tutorial/#project-with-dependencies","text":"Cargo is Rust package manager. It is a tool that allows Rust packages to declare their various dependencies and ensure that you\u2019ll always get a repeatable build. Clone example repo git clone https://github.com/rdesarz/rust-http-server.git cd rust-http-server Build and start the server cd example cargo run --package http-server --bin http-server 0.0.0.0:8040 You will see that before cargo builds the package, it installs all the dependencies from the file Cargo.toml . Open Quickstart page, go to \"My apps\" and use port 8040 shortcut to open your web app","title":"Project with dependencies"},{"location":"scala-workspace/","text":"Scala workspace Docker image with Scala and browser-based VS-Code version. Why this images If you need self-hosted remote development environment. If you want to be one command away from coding in Scala. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/scala-workspace and open localhost:8020 in browser. Features Scala Coursier Sbt Java Maven Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"scala-workspace/#scala-workspace","text":"Docker image with Scala and browser-based VS-Code version.","title":"Scala workspace"},{"location":"scala-workspace/#why-this-images","text":"If you need self-hosted remote development environment. If you want to be one command away from coding in Scala.","title":"Why this images"},{"location":"scala-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/scala-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"scala-workspace/#features","text":"Scala Coursier Sbt Java Maven Dev tools: Code-server - open source version of popular Visual Studio Code IDE. Codeserver has VS-Code extensions and works in browser. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"scala-workspace/tutorial/","text":"This basic tutorial shows how to create Scala Hello-world application, build and run project with Sbt. As well as manage Scala dependencies and versions with Coursier. Scala To check scala version, execute in terminal scala -version To open scala REPL simply execute scala Hello world Use Code editor to create folder helloworld with file Hello.scala in it. The file should have the following content object Hello { def main(args: Array[String]) = { println(\"Hello, world\") } } Open terminal, go into this folder and execute scala Hello.scala Hello world with sbt cd to the projects directory, and create new project with Sbt cd /home/project sbt new scala/scala3.g8 Upon prompt type any project name, i.e. \"hello-world\". Sbt will generate progje boilerplate with the required folder structure. Enter the project folder, compile and run the app sbt run Coursier Coursier is the Scala application and artifact manager. It can install Scala applications and setup your Scala development environment. It can also download and cache artifacts from the web. Coursier provides a number of services: - manage the installed Scala applications: install , list , update , uninstall , search - configure channels to install Scala applications from: channel - launchers for Scala applications: launch , bootstrap - manage the installed JVMs: java , java-home - directly manipulate Maven dependencies: fetch , resolve - perform setup again List all applications, installed by Coursier cs list The install command installs Scala applications, i.e. cs install scalafmt If you want to launch another scala version cs launch scala:2.12.15","title":"Tutorial"},{"location":"scala-workspace/tutorial/#scala","text":"To check scala version, execute in terminal scala -version To open scala REPL simply execute scala","title":"Scala"},{"location":"scala-workspace/tutorial/#hello-world","text":"Use Code editor to create folder helloworld with file Hello.scala in it. The file should have the following content object Hello { def main(args: Array[String]) = { println(\"Hello, world\") } } Open terminal, go into this folder and execute scala Hello.scala","title":"Hello world"},{"location":"scala-workspace/tutorial/#hello-world-with-sbt","text":"cd to the projects directory, and create new project with Sbt cd /home/project sbt new scala/scala3.g8 Upon prompt type any project name, i.e. \"hello-world\". Sbt will generate progje boilerplate with the required folder structure. Enter the project folder, compile and run the app sbt run","title":"Hello world with sbt"},{"location":"scala-workspace/tutorial/#coursier","text":"Coursier is the Scala application and artifact manager. It can install Scala applications and setup your Scala development environment. It can also download and cache artifacts from the web. Coursier provides a number of services: - manage the installed Scala applications: install , list , update , uninstall , search - configure channels to install Scala applications from: channel - launchers for Scala applications: launch , bootstrap - manage the installed JVMs: java , java-home - directly manipulate Maven dependencies: fetch , resolve - perform setup again List all applications, installed by Coursier cs list The install command installs Scala applications, i.e. cs install scalafmt If you want to launch another scala version cs launch scala:2.12.15","title":"Coursier"},{"location":"sqlite-workspace/","text":"SQLite Workspace Collection of tools to develop SQLite databases. Why this images If you need to develop embedded SQLite databases and fill them with data. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/sqlite-workspace and open localhost:8020 in browser. Features SQLite tools: nocodb - great UI to fill SQLite with data. NocoDB is an open source Airtable alternative. sqlite-web - web-based SQLite database browser. sqlean - the ultimate set of compiled SQLite extensions. litecli - CLI for SQLite. sqlite-viewer - view SQLite file online. Web-GUI-for-SQLite - alternative tool to view SQLite file online. DBdesigner - draw ERD diagrams and use it to generate DDL code. tbls - gnerate documentation (essentially data catalog) from the database SQLite extension: crypto - secure hashes fileio - read and write files fuzzy - fuzzy string matching and phonetics ipaddr : - IP address manipulation re - regular expressions stats - math statistics text - string functions unicode - Unicode support uuid - universally Unique IDentifiers Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"sqlite-workspace/#sqlite-workspace","text":"Collection of tools to develop SQLite databases.","title":"SQLite Workspace"},{"location":"sqlite-workspace/#why-this-images","text":"If you need to develop embedded SQLite databases and fill them with data.","title":"Why this images"},{"location":"sqlite-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/sqlite-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"sqlite-workspace/#features","text":"SQLite tools: nocodb - great UI to fill SQLite with data. NocoDB is an open source Airtable alternative. sqlite-web - web-based SQLite database browser. sqlean - the ultimate set of compiled SQLite extensions. litecli - CLI for SQLite. sqlite-viewer - view SQLite file online. Web-GUI-for-SQLite - alternative tool to view SQLite file online. DBdesigner - draw ERD diagrams and use it to generate DDL code. tbls - gnerate documentation (essentially data catalog) from the database SQLite extension: crypto - secure hashes fileio - read and write files fuzzy - fuzzy string matching and phonetics ipaddr : - IP address manipulation re - regular expressions stats - math statistics text - string functions unicode - Unicode support uuid - universally Unique IDentifiers Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"sqlite-workspace/tutorial/","text":"This doc has examples how to use workspace toolset to develop SQLite databases and fill them with data. As well explore, query and visualise. To start open Quickstart page for quick access to all the tools Use workspace terminal to execute CLI commands Workspace conntains example SQLite database, the file /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite NocoDB NocoDB . NocoDB is an open source Airtable alternative. Use it to create SQLite tables, develop databases without SQL. NocoDB can also be used to fill SQLite databases with data with the help of nice interactive UI. Launch the workspace, navigate to the Workspace UI and open NocoDB. Create new user (provide any email,pass). Create tables, enter data, import data files, create forms and much more. NocoDB with existing SQLite database Create NocoDB project with an empty external SQLite database file /home/project/sqlite-db/db-main.sqlite . ![noco-new-main](img/noco-new-main.png Create some data, and explore it with Sqlite-web You can also open existing SQLIte database, and edit it with NocoDB. Let's open example SQLite database /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite query NocoDB data NocoDB is backed by SQLite3, database file /home/nocodb/noco.db . You can query NocoDB databse directly with sqlite-web. Open it from Workspace UI Quickstart page. At any moment you can copy SQLite database from NocoDB with all the data, edit, modify and configure for the needs of your application. Copy of NocoDB database and continue developing with Sqlite-web mv -f /home/nocodb/noco.db /home/project/sqlite-db/db-main.sqlite Sqlite SQLite3 is installed. Open terminal and execute sqlite3 Note: check out a better CLI litecli Sqlite extensions The following compiled extensions are present in the folder /home/sqlite-extensions crypto : secure hashes fileio : read and write files fuzzy : fuzzy string matching and phonetics ipaddr : IP address manipulation re : regular expressions stats : math statistics text : string functions unicode : Unicode support uuid : Universally Unique IDentifiers Example start SQLite, and load extension: sqlite3 .load /home/sqlite-extensions/stats Upload & download SQLite databases No matter if workspace is used locally, in cloud or kubernetes you can upload and download database files using file browser Copy SQlite databases SQlite is just a file, jou can copy it and replace other databases. For example, if you want to copy NocoDB database into the \"Main\" database, which is served by Sqlite-web ('Main') simply execute in terminal mv -f /home/nocodb/noco.db /home/project/sqlite-db/db-main.sqlite You also can use file browser to move or copy files. Sqlite-web sqlite-web - is a web-based SQLite database browser. It can serve any SQLite database. There are 2 SQlite database served by default: NocoDB database. Sqlite-web 'Nocodb' 'Main' database. Empty SQlite3 database. Sqlite-web 'Main' 'Example/test' database. Filled with test \"Chinook\" database. Serve on-demand SQlite database One port 8034 is reserved for serving on-demand database with Sqlite-web. Serve it with the following command sqlite_web <path-to-your-sqlite-database-file> --host 0.0.0.0 --port 8034 Example, start sqlite-web on port 8034 for SQLite database file /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite sqlite_web /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite --host 0.0.0.0 --port 8034 and with extensions loaded: sqlite_web /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite --host 0.0.0.0 --port 8034 -e /home/sqlite-extensions/text.so -e /home/sqlite-extensions/unicode.so -e /home/sqlite-extensions/uuid.so You can open on-demand database served with Sqlite-web from the Quickstart page. Sqlite-viewer sqlite-viewer - explore and query SQLite databases. Upload SQLite database file (from your local machine) and explore. Web-GUI-for-SQLite Web-GUI-for-SQLite - explore and query SQLite databases. Upload SQLite database file (from your local machine) and explore. Litecli Litecli - is a command-line client for SQLite databases that has auto-completion and syntax highlighting. Useful if you prefer CLI. Open Terminal and try with example database: litecli /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite Load extensions .load /home/sqlite-extensions/stats Tbls Generate docs for the example database tbls doc sqlite:////home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite /home/static-server/sql-tbls and view SVG files with Static File Server DB designer With DB designer you can visually create ERD diagrams, and generate DDL SQL scripts Advanced SQLite Json Examples CREATE TABLE jsontest ( id INTEGER NOT NULL PRIMARY KEY, jss JSON ); INSERT INTO jsontest VALUES (1, json(' { \"key1\" : \"my key 1\", \"key2\": [ \"test\" ] } ')), (2, json(' { \"key1\" : \"my new key\", \"key2\": [ \"test\", \"test\" ] } ')), (3, json(' { \"key1\" : \"my yet another key\", \"key2\": [ \"test\", \"test\", \"test2\" ] } ')); SELECT * FROM jsontest; Links: - Complete SQlite JSON docs Rclone rclone is a command line program to manage files on cloud storage. It is a feature rich alternative to cloud vendors' web storage interfaces. Copy dump to S3: Create file ~/.config/rclone/rclone.conf with the following content [remote] type = s3 provider = AWS access_key_id = XXXXXXXXXXXXXXXXXXXXXX secret_access_key = XXxxXXxXXXxxxXXxXXXxxXXXxxxXXXxXXXX region = xx-xxxx-x Use Rclone to copy to S3 and delete from local rclone move /home/project/sqlite-db remote:my-s3-bucket/sqlite/ Restore from S3 to local: When there is a need to get database from S3 to local folder. # copy rclone copy remote:my-s3-bucket/sqlite/ /home/sqlite-db/ # mount S3 without copying rclone sync remote:my-s3-bucket/sqlite/ /home/sqlite-db/","title":"Tutorial"},{"location":"sqlite-workspace/tutorial/#nocodb","text":"NocoDB . NocoDB is an open source Airtable alternative. Use it to create SQLite tables, develop databases without SQL. NocoDB can also be used to fill SQLite databases with data with the help of nice interactive UI. Launch the workspace, navigate to the Workspace UI and open NocoDB. Create new user (provide any email,pass). Create tables, enter data, import data files, create forms and much more.","title":"NocoDB"},{"location":"sqlite-workspace/tutorial/#nocodb-with-existing-sqlite-database","text":"Create NocoDB project with an empty external SQLite database file /home/project/sqlite-db/db-main.sqlite . ![noco-new-main](img/noco-new-main.png Create some data, and explore it with Sqlite-web You can also open existing SQLIte database, and edit it with NocoDB. Let's open example SQLite database /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite","title":"NocoDB with existing SQLite database"},{"location":"sqlite-workspace/tutorial/#query-nocodb-data","text":"NocoDB is backed by SQLite3, database file /home/nocodb/noco.db . You can query NocoDB databse directly with sqlite-web. Open it from Workspace UI Quickstart page. At any moment you can copy SQLite database from NocoDB with all the data, edit, modify and configure for the needs of your application. Copy of NocoDB database and continue developing with Sqlite-web mv -f /home/nocodb/noco.db /home/project/sqlite-db/db-main.sqlite","title":"query NocoDB data"},{"location":"sqlite-workspace/tutorial/#sqlite","text":"SQLite3 is installed. Open terminal and execute sqlite3 Note: check out a better CLI litecli","title":"Sqlite"},{"location":"sqlite-workspace/tutorial/#sqlite-extensions","text":"The following compiled extensions are present in the folder /home/sqlite-extensions crypto : secure hashes fileio : read and write files fuzzy : fuzzy string matching and phonetics ipaddr : IP address manipulation re : regular expressions stats : math statistics text : string functions unicode : Unicode support uuid : Universally Unique IDentifiers Example start SQLite, and load extension: sqlite3 .load /home/sqlite-extensions/stats","title":"Sqlite extensions"},{"location":"sqlite-workspace/tutorial/#upload-download-sqlite-databases","text":"No matter if workspace is used locally, in cloud or kubernetes you can upload and download database files using file browser","title":"Upload &amp; download SQLite databases"},{"location":"sqlite-workspace/tutorial/#copy-sqlite-databases","text":"SQlite is just a file, jou can copy it and replace other databases. For example, if you want to copy NocoDB database into the \"Main\" database, which is served by Sqlite-web ('Main') simply execute in terminal mv -f /home/nocodb/noco.db /home/project/sqlite-db/db-main.sqlite You also can use file browser to move or copy files.","title":"Copy SQlite databases"},{"location":"sqlite-workspace/tutorial/#sqlite-web","text":"sqlite-web - is a web-based SQLite database browser. It can serve any SQLite database. There are 2 SQlite database served by default: NocoDB database. Sqlite-web 'Nocodb' 'Main' database. Empty SQlite3 database. Sqlite-web 'Main' 'Example/test' database. Filled with test \"Chinook\" database.","title":"Sqlite-web"},{"location":"sqlite-workspace/tutorial/#serve-on-demand-sqlite-database","text":"One port 8034 is reserved for serving on-demand database with Sqlite-web. Serve it with the following command sqlite_web <path-to-your-sqlite-database-file> --host 0.0.0.0 --port 8034 Example, start sqlite-web on port 8034 for SQLite database file /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite sqlite_web /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite --host 0.0.0.0 --port 8034 and with extensions loaded: sqlite_web /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite --host 0.0.0.0 --port 8034 -e /home/sqlite-extensions/text.so -e /home/sqlite-extensions/unicode.so -e /home/sqlite-extensions/uuid.so You can open on-demand database served with Sqlite-web from the Quickstart page.","title":"Serve on-demand SQlite database"},{"location":"sqlite-workspace/tutorial/#sqlite-viewer","text":"sqlite-viewer - explore and query SQLite databases. Upload SQLite database file (from your local machine) and explore.","title":"Sqlite-viewer"},{"location":"sqlite-workspace/tutorial/#web-gui-for-sqlite","text":"Web-GUI-for-SQLite - explore and query SQLite databases. Upload SQLite database file (from your local machine) and explore.","title":"Web-GUI-for-SQLite"},{"location":"sqlite-workspace/tutorial/#litecli","text":"Litecli - is a command-line client for SQLite databases that has auto-completion and syntax highlighting. Useful if you prefer CLI. Open Terminal and try with example database: litecli /home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite Load extensions .load /home/sqlite-extensions/stats","title":"Litecli"},{"location":"sqlite-workspace/tutorial/#tbls","text":"Generate docs for the example database tbls doc sqlite:////home/static-server/sqlite-viewer/examples/Chinook_Sqlite.sqlite /home/static-server/sql-tbls and view SVG files with Static File Server","title":"Tbls"},{"location":"sqlite-workspace/tutorial/#db-designer","text":"With DB designer you can visually create ERD diagrams, and generate DDL SQL scripts","title":"DB designer"},{"location":"sqlite-workspace/tutorial/#advanced","text":"","title":"Advanced"},{"location":"sqlite-workspace/tutorial/#sqlite-json","text":"Examples CREATE TABLE jsontest ( id INTEGER NOT NULL PRIMARY KEY, jss JSON ); INSERT INTO jsontest VALUES (1, json(' { \"key1\" : \"my key 1\", \"key2\": [ \"test\" ] } ')), (2, json(' { \"key1\" : \"my new key\", \"key2\": [ \"test\", \"test\" ] } ')), (3, json(' { \"key1\" : \"my yet another key\", \"key2\": [ \"test\", \"test\", \"test2\" ] } ')); SELECT * FROM jsontest; Links: - Complete SQlite JSON docs","title":"SQLite Json"},{"location":"sqlite-workspace/tutorial/#rclone","text":"rclone is a command line program to manage files on cloud storage. It is a feature rich alternative to cloud vendors' web storage interfaces. Copy dump to S3: Create file ~/.config/rclone/rclone.conf with the following content [remote] type = s3 provider = AWS access_key_id = XXXXXXXXXXXXXXXXXXXXXX secret_access_key = XXxxXXxXXXxxxXXxXXXxxXXXxxxXXXxXXXX region = xx-xxxx-x Use Rclone to copy to S3 and delete from local rclone move /home/project/sqlite-db remote:my-s3-bucket/sqlite/ Restore from S3 to local: When there is a need to get database from S3 to local folder. # copy rclone copy remote:my-s3-bucket/sqlite/ /home/sqlite-db/ # mount S3 without copying rclone sync remote:my-s3-bucket/sqlite/ /home/sqlite-db/","title":"Rclone"},{"location":"streamlit-workspace/","text":"Streamlit workspace Docker image for building Streamlit applications. Why this images If you need self-hosted environment for Streamlit app development. If you want to be one command away from coding Streamlit apps. Start docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/streamlit-workspace and open localhost:8020 in browser. Features Streamlit Python tools: IPython and Notebooks Pdoc3 Pytest-html-reporter SnakeViz Vprof Pyinstrument Flameprof Pylint-json2html Pre-commit Flake8 Poetry Black Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"About"},{"location":"streamlit-workspace/#streamlit-workspace","text":"Docker image for building Streamlit applications.","title":"Streamlit workspace"},{"location":"streamlit-workspace/#why-this-images","text":"If you need self-hosted environment for Streamlit app development. If you want to be one command away from coding Streamlit apps.","title":"Why this images"},{"location":"streamlit-workspace/#start","text":"docker run --name space-1 -d -p 8020-8040:8020-8040 alnoda/streamlit-workspace and open localhost:8020 in browser.","title":"Start"},{"location":"streamlit-workspace/#features","text":"Streamlit Python tools: IPython and Notebooks Pdoc3 Pytest-html-reporter SnakeViz Vprof Pyinstrument Flameprof Pylint-json2html Pre-commit Flake8 Poetry Black Dev tools: Eclipse Theia - open source version of popular Visual Studio Code IDE. Theia is trully open-source, has VS-Code extensions and works in browser. This means it can run inside a docker container on local machine or in cloud. A lot of beautiful color themes and many common plugins are already installed to save time. Terminal - secure browser-based terminal. FileBrowser - manage files and folders inside the workspace, and exchange data between local environment and the workspace Cronicle - task scheduler and runner, with a web based front-end UI. It handles both scheduled, repeating and on-demand jobs, targeting any number of worker servers, with real-time stats and live log viewer. Static File Server - view any static html sites as easy as if you do it on your local machine. Serve static websites easily. Ungit - rings user friendliness to git without sacrificing the versatility of it. MkDocs - create awesome documentation for your project with only markdown. Midnight Commander - Feature rich visual file manager with internal text viewer and editor. Process Monitor - Monitor running process and resource utilization. Quicklaunch UI with getting started tutorial Image is built from Ubuntu 20.4 with the additional CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"streamlit-workspace/tutorial/","text":"This ia simple example showing how to start a Streamlit app in the workspace. Streamlit Open terminal, and launch example streamlit application streamlit run /home/examples/uber.py Open running Streamlit app from the quickstart page You can luanch any Streamlit app, annd use quickstart page to quickly ope the app.","title":"Tutorial"},{"location":"streamlit-workspace/tutorial/#streamlit","text":"Open terminal, and launch example streamlit application streamlit run /home/examples/uber.py Open running Streamlit app from the quickstart page You can luanch any Streamlit app, annd use quickstart page to quickly ope the app.","title":"Streamlit"},{"location":"ubuntu-docker-workspace/","text":"Ubuntu docker workspace Containerized Linux terminal environment. Essentially Ubuntu 20.04 docker image extended with typical console apps, such as Git, file browsers and system monitors. Has docker in docker. Start docker run --name space-1 -d -v /var/run/docker.sock:/var/run/docker.sock --user=root alnoda/ubuntu-docker-workspace Enter workspace docker exec -it space-1 /bin/zsh Features Docker Docker Ctop - Top-like interface for container metrics. Lazydocker - A simple terminal UI for both docker and docker-compose, written in Go with the gocui library. Sen - A terminal user interface for containers. Dive - A tool for exploring a docker image, layer contents, and discovering ways to shrink the size of your Docker/OCI image. Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Why this image If you need to isolate some work without polluting main environment.","title":"About"},{"location":"ubuntu-docker-workspace/#ubuntu-docker-workspace","text":"Containerized Linux terminal environment. Essentially Ubuntu 20.04 docker image extended with typical console apps, such as Git, file browsers and system monitors. Has docker in docker. Start docker run --name space-1 -d -v /var/run/docker.sock:/var/run/docker.sock --user=root alnoda/ubuntu-docker-workspace Enter workspace docker exec -it space-1 /bin/zsh","title":"Ubuntu docker workspace"},{"location":"ubuntu-docker-workspace/#features","text":"Docker Docker Ctop - Top-like interface for container metrics. Lazydocker - A simple terminal UI for both docker and docker-compose, written in Go with the gocui library. Sen - A terminal user interface for containers. Dive - A tool for exploring a docker image, layer contents, and discovering ways to shrink the size of your Docker/OCI image. Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"ubuntu-docker-workspace/#why-this-image","text":"If you need to isolate some work without polluting main environment.","title":"Why this image"},{"location":"ubuntu-docker-workspace/tutorial/","text":"This a basic tutorial to demonstrate workspace docker toolset. Ctop Ctop - is a top-like interface for container metrics. ctop Lazydocker Lazydocker - a simple terminal UI for both docker and docker-compose, written in Go with the gocui library. lazydocker Sen Sen - is a terminal user interface for containers. sen Dive Dive - is a tool for exploring a docker image, layer contents, and discovering ways to shrink the size of your Docker/OCI image. docker pull postgres dive alpine/postgres Browser-based terminal This workspace has browser-based terminal up and running. Open localhost:8026 in browser. Start the workspace on Rasberri Pi and use it via Tablet. In order to use browser-based terminal, expose port 8026 docker run --name space-1 -d -p 8026:8026 -v /var/run/docker.sock:/var/run/docker.sock --user=root alnoda/ubuntu-docker-workspace Extra This workspace has all the features of the ubuntu-workspace","title":"Tutorial"},{"location":"ubuntu-docker-workspace/tutorial/#ctop","text":"Ctop - is a top-like interface for container metrics. ctop","title":"Ctop"},{"location":"ubuntu-docker-workspace/tutorial/#lazydocker","text":"Lazydocker - a simple terminal UI for both docker and docker-compose, written in Go with the gocui library. lazydocker","title":"Lazydocker"},{"location":"ubuntu-docker-workspace/tutorial/#sen","text":"Sen - is a terminal user interface for containers. sen","title":"Sen"},{"location":"ubuntu-docker-workspace/tutorial/#dive","text":"Dive - is a tool for exploring a docker image, layer contents, and discovering ways to shrink the size of your Docker/OCI image. docker pull postgres dive alpine/postgres","title":"Dive"},{"location":"ubuntu-docker-workspace/tutorial/#browser-based-terminal","text":"This workspace has browser-based terminal up and running. Open localhost:8026 in browser. Start the workspace on Rasberri Pi and use it via Tablet. In order to use browser-based terminal, expose port 8026 docker run --name space-1 -d -p 8026:8026 -v /var/run/docker.sock:/var/run/docker.sock --user=root alnoda/ubuntu-docker-workspace","title":"Browser-based terminal"},{"location":"ubuntu-docker-workspace/tutorial/#extra","text":"This workspace has all the features of the ubuntu-workspace","title":"Extra"},{"location":"ubuntu-workspace/","text":"Ubuntu workspace Containerized Linux terminal environment. Essentially Ubuntu 20.04 docker image extended with typical console apps, such as Git, file browsers and system monitors. Why this image If you need to isolate some work without polluting main environment. Start docker run --name space-1 -d alnoda/ubuntu-workspace Enter workspace docker exec -it --user=root space-1 /bin/zsh Features Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron Docs See our guides on getting started and advanced features . Demo: Ubuntu-workspace","title":"About"},{"location":"ubuntu-workspace/#ubuntu-workspace","text":"Containerized Linux terminal environment. Essentially Ubuntu 20.04 docker image extended with typical console apps, such as Git, file browsers and system monitors.","title":"Ubuntu workspace"},{"location":"ubuntu-workspace/#why-this-image","text":"If you need to isolate some work without polluting main environment.","title":"Why this image"},{"location":"ubuntu-workspace/#start","text":"docker run --name space-1 -d alnoda/ubuntu-workspace Enter workspace docker exec -it --user=root space-1 /bin/zsh","title":"Start"},{"location":"ubuntu-workspace/#features","text":"Ubuntu 20.4 with the following CLI apps Zsh , Oh my Zsh Python 3, Pip Node/nodeenv curl, wget, telnet, jq Git: git, git-flow, lazygit File browsers: mc, xplr Text editors: nano, vim, mcedit System monitors: ncdu, htop, glances, vizex Process Control: supervisord Job scheduler: cron","title":"Features"},{"location":"ubuntu-workspace/#docs","text":"See our guides on getting started and advanced features . Demo: Ubuntu-workspace","title":"Docs"},{"location":"ubuntu-workspace/remote-wid/","text":"This tutorial demonstrates how to securely launch ubuntu-workspace on remote server with self-signe https certificate and authentication. To add https and auth Traefik reverse proxy will be used. This is the docker-compose that can be used to spin-up Ubuntu-workspace on the remote server together with the reverse proxy, that adds authentication. version: \"3.3\" services: traefik: image: \"traefik:v2.4\" container_name: \"traefik\" command: - \"--providers.docker\" - \"--entrypoints.terminal.address=:8026\" ports: - 8026:8026 volumes: - \"/var/run/docker.sock:/var/run/docker.sock:ro\" workspace: image: alnoda/ubuntu-workspace labels: # To create user:password pair, execute in any workspace echo $(htpasswd -nB <userName>) | sed -e s/\\\\$/\\\\$\\\\$/g - \"traefik.http.middlewares.basic-auth.basicauth.users=admin:$$2y$$05$$eub6CV.CwUYCCQjNBvSf5uZnzdRmVwGZ/ncxecb9O7WxCR8aLuM3K\" - \"traefik.enable=true\" # terminal - \"traefik.http.services.terminal.loadbalancer.server.port=8026\" - \"traefik.http.routers.terminal.service=terminal\" - \"traefik.http.routers.terminal.rule=PathPrefix(`/`)\" - \"traefik.http.routers.terminal.entrypoints=terminal\" - \"traefik.http.routers.terminal.middlewares=basic-auth\" This configuration launches workspace with the default authentication user:pass is admin:admin . You might want to generate new credentials. The password for the traefik basic auth must be encrypted with the htpasswd . For connvenience, it is installed in every workspace-in-docker, and the easiest way is to generate the password is to launch workspace locally first, use its terminal to create a password, and then start workspace on remote server. To encrypt password open terminal of the local workspace and execute echo $(htpasswd -nB <userName>) | sed -e s/\\\\$/\\\\$\\\\$/g substitute <userName> with the new user name, and prowide password on prompt. After this htpasswd will output encrypted password. Don't forget to change this line in the docker-compose file with the new user:encpypted_pass - \"traefik.http.middlewares.basic-auth.basicauth.users=admin:$$2y$$05$$eub6CV.CwUYCCQjNBvSf5uZnzdRmVwGZ/ncxecb9O7WxCR8aLuM3K\" Create file remote-workspace-auth.yaml on the remote server, paste yaml from above (preferrably with new auth) and start workspace docker-compose -f remote-workspace-auth.yaml up -d NOTE: this set up adds authentication, but it is not secure, password and communication are unencrypted. Consider using workspace utility that generates configuration for more secure workspace deployment to the cloud server.","title":"Run remotely"},{"location":"ubuntu-workspace/tutorial/","text":"This is a demonstration of the CLI applications pre-installed in the workspace. Start docker run --name space-1 -d alnoda/ubuntu-workspace Now you can ssh into the running workspace container docker exec -it space-1 /bin/zsh Root user ssh into the running workspace container as root user docker exec -it --user=root space-1 /bin/zsh File browser Explore file system with Midnight Commander mc Or with Xplr xplr Text editor Text editors vim , nano and mcedit are available. For example, clone git repo and edit python file with nano git clone https://github.com/dimaba/sendmail.git cd sendmail nano sendmail.py mcedit sendmail.py System monitor Launch system-monitor, process-viewer and process-manager htop Explore processes and resources with Glances glances Explore file/folder sizes ncdu Git Clone Git repo and explore with Lazygit git clone https://github.com/peaceiris/mkdocs-material-boilerplate.git lazygit Browser terminal This workspace has browser-based terminal up and running. Open localhost:8026 in browser. Start the workspace on Rasberri Pi and use it via Tablet. In order to use browser-based terminal, expose port 8026 docker run --name space-1 -d -p 8026:8026 alnoda/ubuntu-workspace Docker in docker If you want to use docker, check out ubuntu-docker-workspace . It is the same, but with docker-in-docker.","title":"Tutorial"},{"location":"ubuntu-workspace/tutorial/#start","text":"docker run --name space-1 -d alnoda/ubuntu-workspace Now you can ssh into the running workspace container docker exec -it space-1 /bin/zsh","title":"Start"},{"location":"ubuntu-workspace/tutorial/#root-user","text":"ssh into the running workspace container as root user docker exec -it --user=root space-1 /bin/zsh","title":"Root user"},{"location":"ubuntu-workspace/tutorial/#file-browser","text":"Explore file system with Midnight Commander mc Or with Xplr xplr","title":"File browser"},{"location":"ubuntu-workspace/tutorial/#text-editor","text":"Text editors vim , nano and mcedit are available. For example, clone git repo and edit python file with nano git clone https://github.com/dimaba/sendmail.git cd sendmail nano sendmail.py mcedit sendmail.py","title":"Text editor"},{"location":"ubuntu-workspace/tutorial/#system-monitor","text":"Launch system-monitor, process-viewer and process-manager htop Explore processes and resources with Glances glances Explore file/folder sizes ncdu","title":"System monitor"},{"location":"ubuntu-workspace/tutorial/#git","text":"Clone Git repo and explore with Lazygit git clone https://github.com/peaceiris/mkdocs-material-boilerplate.git lazygit","title":"Git"},{"location":"ubuntu-workspace/tutorial/#browser-terminal","text":"This workspace has browser-based terminal up and running. Open localhost:8026 in browser. Start the workspace on Rasberri Pi and use it via Tablet. In order to use browser-based terminal, expose port 8026 docker run --name space-1 -d -p 8026:8026 alnoda/ubuntu-workspace","title":"Browser terminal"},{"location":"ubuntu-workspace/tutorial/#docker-in-docker","text":"If you want to use docker, check out ubuntu-docker-workspace . It is the same, but with docker-in-docker.","title":"Docker in docker"}]}